{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ea141c",
   "metadata": {},
   "source": [
    "# ConvNeXt-based v-CLR (VOC → Non-VOC) – Fast Training, AMP, Partial Fine-tuning\n",
    "\n",
    "This notebook implements a ConvNeXt-based v-CLR training pipeline with:\n",
    "\n",
    "- Precomputed `train_items` (no COCO object inside the Dataset) so DataLoader workers work on Windows.\n",
    "- Random selection of depth vs. stylized view per step (always with the natural view).\n",
    "- Mixed precision (AMP) for faster GPU training and lower memory.\n",
    "- ConvNeXt-tiny backbone initialized from ImageNet and partially fine-tuned (last few stages unfrozen).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e617485a",
   "metadata": {},
   "source": [
    "## v-CLR Paper Alignment Notes\n",
    "\n",
    "This implementation attempts to recreate the v-CLR (View-Consistent Learning) approach from the paper \n",
    "\"v-CLR: View-Consistent Learning for Open-World Instance Segmentation\" (arXiv:2504.01383) using a CNN \n",
    "backbone (ConvNeXt) instead of the original transformer-based MaskDINO architecture.\n",
    "\n",
    "### Key Differences from Original Paper:\n",
    "\n",
    "1. **Architecture**: Uses ConvNeXt-tiny backbone instead of MaskDINO/Swin Transformer\n",
    "2. **Detection Only**: Current implementation focuses on bounding box detection only \n",
    "   (masks are not yet implemented - set to `None`)\n",
    "3. **Dense Prediction**: Uses a dense prediction head instead of DETR-style object queries\n",
    "\n",
    "### Implementation Corrections Applied:\n",
    "\n",
    "1. **VOC Class Names**: Fixed to use the correct 20 PASCAL VOC categories in COCO naming:\n",
    "   - Removed incorrect classes: `truck`, `elephant`, `bear`, `zebra`, `giraffe`\n",
    "   - Added missing VOC classes: `bottle`, `chair`, `couch`, `potted plant`, `dining table`\n",
    "\n",
    "2. **GT Target Application**: Fixed training loop to apply ground-truth supervision only to the \n",
    "   natural view (as specified in v-CLR). Depth and stylized views are now trained via \n",
    "   view-consistency losses (L_obj, L_sim) only, not with GT annotations.\n",
    "\n",
    "### Known Limitations:\n",
    "\n",
    "1. **No Mask Prediction**: The original v-CLR paper includes instance segmentation masks. \n",
    "   This implementation only produces bounding boxes.\n",
    "\n",
    "2. **Dense vs Query-based**: The paper uses object queries (like DETR/MaskDINO), while this \n",
    "   implementation uses dense per-pixel predictions reshaped as queries.\n",
    "\n",
    "3. **Temperature Scaling**: The cosine similarity loss could benefit from temperature scaling \n",
    "   (common in contrastive learning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c6a5e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda NVIDIA GeForce RTX 4080\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torchvision.ops import box_iou, generalized_box_iou\n",
    "from torchvision.models import convnext_tiny\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from PIL import Image\n",
    "\n",
    "# Speed/predictability settings\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA is not available – this notebook expects a GPU.\")\n",
    "device = torch.device(\"cuda\")\n",
    "print(\"Using device:\", device, torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529bd9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "\n",
    "DATA_ROOT = Path(r\"C:\\workspace\\vclr\\datasets\")\n",
    "\n",
    "COCO_ROOT = DATA_ROOT\n",
    "COCO_TRAIN_JSON = COCO_ROOT / \"annotations\" / \"instances_train2017.json\"\n",
    "COCO_VAL_JSON   = COCO_ROOT / \"annotations\" / \"instances_val2017.json\"\n",
    "\n",
    "COCO_TRAIN_IMG_DIR = COCO_ROOT / \"train2017\"\n",
    "COCO_VAL_IMG_DIR   = COCO_ROOT / \"val2017\"\n",
    "\n",
    "# Extra views\n",
    "DEPTH_TRAIN_DIR = DATA_ROOT / \"train2017_depth_cmap\"    # depth as .png\n",
    "STYLE_TRAIN_DIR = DATA_ROOT / \"style_coco_train2017\"    # stylized view (if available)\n",
    "EDGE_TRAIN_DIR  = None                                  # optional\n",
    "\n",
    "# Non-VOC val + CutLER proposals\n",
    "NONVOC_VAL_JSON       = DATA_ROOT / \"uvo_nonvoc_val_rle.json\"\n",
    "CUTLER_PROPOSALS_JSON = DATA_ROOT / \"vCLR_coco_train2017_top5.json\"\n",
    "\n",
    "NONVOC_IMG_DIR = DATA_ROOT / \"uvo_videos_dense_frames\"\n",
    "\n",
    "# Training / eval hyperparameters\n",
    "NUM_EPOCHS         = 8\n",
    "BASE_LR            = 1e-4\n",
    "TRAIN_BATCH_SIZE   = 15\n",
    "VAL_BATCH_SIZE     = 15\n",
    "TRAIN_NUM_WORKERS  = 0   # safe after we remove COCO from Dataset\n",
    "VAL_NUM_WORKERS    = 0   # simpler eval\n",
    "IMG_SIZE           = 800\n",
    "\n",
    "# v-CLR loss weights\n",
    "LAMBDA_GT    = 1.0\n",
    "LAMBDA_OBJ   = 1.0\n",
    "LAMBDA_SIM   = 1.0\n",
    "LAMBDA_MATCH = 1.0\n",
    "\n",
    "# MaskDINO/DINO-style detection and segmentation loss weights (from official configs)\n",
    "# CLASS_WEIGHT, BOX_WEIGHT, GIOU_WEIGHT, MASK_WEIGHT, DICE_WEIGHT, NO_OBJECT_WEIGHT\n",
    "LOSS_CLASS_WEIGHT      = 4.0   # classification term\n",
    "LOSS_BOX_WEIGHT        = 5.0   # box L1 term\n",
    "LOSS_GIOU_WEIGHT       = 2.0   # (G)IoU term\n",
    "LOSS_MASK_BCE_WEIGHT   = 5.0   # mask BCE/focal term\n",
    "LOSS_MASK_DICE_WEIGHT  = 5.0   # mask dice term\n",
    "LOSS_NO_OBJECT_WEIGHT  = 0.1   # weight for \"no object\" queries\n",
    "\n",
    "# Number of object queries per image (DETR / MaskDINO style)\n",
    "NUM_QUERIES = 300\n",
    "\n",
    "# VOC 20 classes in COCO naming\n",
    "# Correct VOC 20 classes mapped to COCO naming conventions\n",
    "# These are the 20 PASCAL VOC categories as they appear in COCO\n",
    "VOC_CLASS_NAMES = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\",\n",
    "    \"train\", \"boat\",\n",
    "    \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
    "    \"bottle\", \"chair\", \"couch\", \"potted plant\", \"dining table\",\n",
    "    \"tv\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f168e6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=6.57s)\n",
      "creating index...\n",
      "index created!\n",
      "VOC category ids: [1, 2, 3, 4, 5, 6, 7, 8, 9, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 72]\n",
      "Non-VOC category ids (60 total): [10, 11, 13, 14, 15, 27, 28, 31, 32, 33] ...\n"
     ]
    }
   ],
   "source": [
    "# === COCO setup & VOC / Non-VOC split ===\n",
    "\n",
    "coco_train = COCO(str(COCO_TRAIN_JSON))\n",
    "\n",
    "name_to_id = {cat[\"name\"]: cat[\"id\"] for cat in coco_train.cats.values()}\n",
    "\n",
    "voc_cat_ids = []\n",
    "for name in VOC_CLASS_NAMES:\n",
    "    if name not in name_to_id:\n",
    "        raise ValueError(f\"VOC class '{name}' not found in COCO categories.\")\n",
    "    voc_cat_ids.append(name_to_id[name])\n",
    "\n",
    "voc_cat_ids = sorted(set(voc_cat_ids))\n",
    "all_cat_ids = sorted(coco_train.cats.keys())\n",
    "nonvoc_cat_ids = [cid for cid in all_cat_ids if cid not in voc_cat_ids]\n",
    "\n",
    "print(\"VOC category ids:\", voc_cat_ids)\n",
    "print(\"Non-VOC category ids ({} total):\".format(len(nonvoc_cat_ids)),\n",
    "      nonvoc_cat_ids[:10], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42fbd3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CutLER-style proposals (COCO JSON) for 118287 training images.\n",
      "Example proposals for image_id=558840: boxes=torch.Size([16, 4]), scores=torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# === Load CutLER proposals (COCO xywh -> xyxy for IoU/L1 with detector) ===\n",
    "\n",
    "with open(CUTLER_PROPOSALS_JSON, \"r\") as f:\n",
    "    cutler_data = json.load(f)\n",
    "\n",
    "proposal_dict: Dict[int, List[Dict[str, torch.Tensor]]] = {}\n",
    "\n",
    "def _xywh_to_xyxy(boxes_xywh: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Convert [x, y, w, h] -> [x1, y1, x2, y2] in the same coordinate system.\"\"\"\n",
    "    if boxes_xywh.ndim == 1:\n",
    "        boxes_xywh = boxes_xywh.unsqueeze(0)\n",
    "    boxes_xyxy = boxes_xywh.clone()\n",
    "    boxes_xyxy[:, 2] = boxes_xywh[:, 0] + boxes_xywh[:, 2]  # x2 = x + w\n",
    "    boxes_xyxy[:, 3] = boxes_xywh[:, 1] + boxes_xywh[:, 3]  # y2 = y + h\n",
    "    return boxes_xyxy\n",
    "\n",
    "if isinstance(cutler_data, dict) and \"annotations\" in cutler_data and \"images\" in cutler_data:\n",
    "    # Standard COCO-style proposals: each annotation has bbox=[x,y,w,h] in original pixels\n",
    "    boxes_by_image = defaultdict(list)\n",
    "    scores_by_image = defaultdict(list)\n",
    "\n",
    "    for ann in cutler_data[\"annotations\"]:\n",
    "        img_id = int(ann[\"image_id\"])\n",
    "        boxes_by_image[img_id].append(ann[\"bbox\"])               # COCO xywh\n",
    "        scores_by_image[img_id].append(ann.get(\"score\", 1.0))\n",
    "\n",
    "    for img_id, boxes_list in boxes_by_image.items():\n",
    "        boxes_xywh = torch.tensor(boxes_list, dtype=torch.float32)\n",
    "        boxes_xyxy = _xywh_to_xyxy(boxes_xywh)\n",
    "        scores = torch.tensor(scores_by_image[img_id], dtype=torch.float32)\n",
    "        proposal_dict[img_id] = [{\"boxes\": boxes_xyxy, \"scores\": scores}]\n",
    "\n",
    "    print(f\"Loaded CutLER-style proposals (COCO JSON) for {len(proposal_dict)} training images.\")\n",
    "\n",
    "elif isinstance(cutler_data, dict):\n",
    "    # Fallback: dictionary keyed directly by image_id (as string) -> boxes / {boxes,scores}\n",
    "    skipped_meta_keys: List[str] = []\n",
    "    for k, v in cutler_data.items():\n",
    "        try:\n",
    "            img_id = int(k)\n",
    "        except ValueError:\n",
    "            skipped_meta_keys.append(k)\n",
    "            continue\n",
    "\n",
    "        if isinstance(v, dict) and \"boxes\" in v:\n",
    "            boxes_xywh = torch.tensor(v[\"boxes\"], dtype=torch.float32)\n",
    "            scores = torch.tensor(\n",
    "                v.get(\"scores\", [1.0] * len(boxes_xywh)),\n",
    "                dtype=torch.float32,\n",
    "            )\n",
    "        else:\n",
    "            boxes_xywh = torch.tensor(v, dtype=torch.float32)\n",
    "            scores = torch.ones(len(boxes_xywh), dtype=torch.float32)\n",
    "\n",
    "        boxes_xyxy = _xywh_to_xyxy(boxes_xywh)\n",
    "        proposal_dict[img_id] = [{\"boxes\": boxes_xyxy, \"scores\": scores}]\n",
    "\n",
    "    print(\"Loaded dict-style proposals for\", len(proposal_dict), \"training images\")\n",
    "    if skipped_meta_keys:\n",
    "        print(\"Skipped non-image keys:\", skipped_meta_keys)\n",
    "else:\n",
    "    raise ValueError(\"Unrecognized CUTLER_PROPOSALS_JSON format for proposals.\")\n",
    "\n",
    "# Small sanity check: print one entry (id and shapes)\n",
    "_example_key = next(iter(proposal_dict.keys()))\n",
    "_example_entry = proposal_dict[_example_key][0]\n",
    "print(\n",
    "    f\"Example proposals for image_id={_example_key}: \"\n",
    "    f\"boxes={_example_entry['boxes'].shape}, scores={_example_entry['scores'].shape}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35aa7dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_items) = 118287\n"
     ]
    }
   ],
   "source": [
    "# === Precompute train_items for a Windows-safe Dataset ===\n",
    "\n",
    "train_items: List[Dict[str, Any]] = []\n",
    "\n",
    "for img_id in coco_train.getImgIds():\n",
    "    img_info = coco_train.loadImgs([img_id])[0]\n",
    "    ann_ids = coco_train.getAnnIds(imgIds=[img_id], iscrowd=None)\n",
    "    anns = coco_train.loadAnns(ann_ids)\n",
    "\n",
    "    boxes = []\n",
    "    iscrowd = []\n",
    "    segs = []\n",
    "\n",
    "    for a in anns:\n",
    "        if a[\"category_id\"] not in voc_cat_ids:\n",
    "            continue\n",
    "        if \"bbox\" not in a:\n",
    "            continue\n",
    "        boxes.append(a[\"bbox\"])              # [x, y, w, h]\n",
    "        iscrowd.append(a.get(\"iscrowd\", 0))\n",
    "        segs.append(a.get(\"segmentation\", None))\n",
    "\n",
    "    item = {\n",
    "        \"image_id\": img_id,\n",
    "        \"file_name\": img_info[\"file_name\"],\n",
    "        \"boxes\": boxes,\n",
    "        \"iscrowd\": iscrowd,\n",
    "        \"masks\": segs,\n",
    "        \"orig_size\": [img_info[\"height\"], img_info[\"width\"]],  # [H, W]\n",
    "    }\n",
    "    train_items.append(item)\n",
    "\n",
    "print(\"len(train_items) =\", len(train_items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "702a6cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vclr_dataset2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile vclr_dataset2.py\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import pycocotools.mask as mask_utils\n",
    "\n",
    "\n",
    "class VCLRTrainSubset(Dataset):\n",
    "    \"\"\"\n",
    "    Windows-safe v-CLR training subset.\n",
    "\n",
    "    Each entry in `items` is a Python dict:\n",
    "      - image_id: int\n",
    "      - file_name: str\n",
    "      - boxes: List[List[float]] in xywh (original pixels)\n",
    "      - iscrowd: List[int]\n",
    "      - masks: List[segmentation] (COCO polygons or RLE)\n",
    "      - orig_size: [H, W]\n",
    "\n",
    "    `proposals` is:\n",
    "      image_id -> list of { \"boxes\": Tensor[K,4] (xywh), \"scores\": Tensor[K] }.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        items: List[Dict[str, Any]],\n",
    "        img_dir: Path,\n",
    "        depth_dir: Optional[str] = None,\n",
    "        stylized_dir: Optional[str] = None,\n",
    "        edge_dir: Optional[str] = None,\n",
    "        proposals: Optional[Dict[int, List[Dict[str, torch.Tensor]]]] = None,\n",
    "        transform: Optional[Any] = None,\n",
    "    ):\n",
    "        self.items = items\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.depth_dir = Path(depth_dir) if depth_dir is not None else None\n",
    "        self.stylized_dir = Path(stylized_dir) if stylized_dir is not None else None\n",
    "        self.edge_dir = Path(edge_dir) if edge_dir is not None else None\n",
    "        self.proposals = proposals or {}\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)\n",
    "\n",
    "    def _load_optional_view(self, root: Optional[Path], file_name: str) -> Optional[Image.Image]:\n",
    "        if root is None:\n",
    "            return None\n",
    "        path = root / file_name\n",
    "        if not path.is_file():\n",
    "            alt = None\n",
    "            if path.suffix.lower() == \".png\":\n",
    "                alt = path.with_suffix(\".jpg\")\n",
    "            elif path.suffix.lower() == \".jpg\":\n",
    "                alt = path.with_suffix(\".png\")\n",
    "            if alt is None or not alt.is_file():\n",
    "                return None\n",
    "            path = alt\n",
    "        return Image.open(path).convert(\"RGB\")\n",
    "\n",
    "    def _decode_masks(self, segs, h: int, w: int) -> torch.Tensor:\n",
    "        if not segs:\n",
    "            # Legit: there are no masks for this image\n",
    "            return torch.zeros((0, h, w), dtype=torch.uint8)\n",
    "\n",
    "        decoded = []\n",
    "        failed = 0\n",
    "\n",
    "        for s in segs:\n",
    "            if s is None:\n",
    "                failed += 1\n",
    "                continue\n",
    "\n",
    "            rle = None\n",
    "            if isinstance(s, list):\n",
    "                rles = mask_utils.frPyObjects(s, h, w)\n",
    "                rle = mask_utils.merge(rles) if isinstance(rles, list) else rles\n",
    "\n",
    "            elif isinstance(s, dict) and \"counts\" in s:\n",
    "                if isinstance(s[\"counts\"], list):\n",
    "                    rle = mask_utils.frPyObjects(s, h, w)\n",
    "                    if isinstance(rle, list):\n",
    "                        rle = mask_utils.merge(rle)\n",
    "                else:\n",
    "                    rle = s\n",
    "\n",
    "            if rle is None:\n",
    "                failed += 1\n",
    "                continue\n",
    "\n",
    "            m = mask_utils.decode(rle).astype(\"uint8\")\n",
    "            decoded.append(m)\n",
    "\n",
    "        if not decoded:\n",
    "            # segs was non-empty but we failed to decode all of them\n",
    "            # --> better to raise or log, not silently pretend it's background\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to decode any masks for image; got {len(segs)} seg entries, all invalid.\"\n",
    "            )\n",
    "\n",
    "        return torch.from_numpy(np.stack(decoded, axis=0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        item = self.items[idx]\n",
    "        img_id = int(item[\"image_id\"])\n",
    "        file_name = item[\"file_name\"]\n",
    "\n",
    "        # Natural image\n",
    "        path = self.img_dir / file_name\n",
    "        if not path.is_file():\n",
    "            alt = None\n",
    "            if path.suffix.lower() == \".png\":\n",
    "                alt = path.with_suffix(\".jpg\")\n",
    "            elif path.suffix.lower() == \".jpg\":\n",
    "                alt = path.with_suffix(\".png\")\n",
    "            if alt is None or not alt.is_file():\n",
    "                raise FileNotFoundError(f\"Image not found: {path} (alt={alt})\")\n",
    "            path = alt\n",
    "        image_nat = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        h0, w0 = item[\"orig_size\"][0], item[\"orig_size\"][1]\n",
    "\n",
    "        # Extra views\n",
    "        image_depth = self._load_optional_view(self.depth_dir, file_name)\n",
    "        image_style = self._load_optional_view(self.stylized_dir, file_name)\n",
    "        image_edge  = self._load_optional_view(self.edge_dir, file_name)\n",
    "\n",
    "        boxes = torch.as_tensor(item.get(\"boxes\", []), dtype=torch.float32)\n",
    "        if boxes.numel() == 0:\n",
    "            boxes = boxes.reshape(0, 4)\n",
    "\n",
    "        iscrowd = torch.as_tensor(item.get(\"iscrowd\", []), dtype=torch.int64)\n",
    "        if iscrowd.numel() == 0:\n",
    "            iscrowd = iscrowd.reshape(0)\n",
    "\n",
    "        segs = item.get(\"masks\", [])\n",
    "        masks = self._decode_masks(segs, h0, w0)  # (N, H0, W0)\n",
    "        if masks.shape[0] != boxes.shape[0]:\n",
    "            masks = torch.zeros((boxes.shape[0], h0, w0), dtype=torch.uint8)\n",
    "\n",
    "        # Class-agnostic: all foreground\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target: Dict[str, Any] = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"iscrowd\": iscrowd,\n",
    "            \"masks\": masks,\n",
    "            \"image_id\": torch.tensor([img_id]),\n",
    "            \"orig_size\": torch.tensor([h0, w0], dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "        props = self.proposals.get(img_id, [])\n",
    "\n",
    "        sample: Dict[str, Any] = {\n",
    "            \"image_nat\": image_nat,\n",
    "            \"image_depth\": image_depth,\n",
    "            \"image_style\": image_style,\n",
    "            \"image_edge\": image_edge,\n",
    "            \"target\": target,\n",
    "            \"proposals\": props,\n",
    "        }\n",
    "\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eff4a295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train items: 106458 Val items (loss): 11829\n",
      "Train batches: 7098 Val (loss) batches: 789\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import vclr_dataset2\n",
    "importlib.reload(vclr_dataset2)\n",
    "from vclr_dataset2 import VCLRTrainSubset\n",
    "\n",
    "class VCLRTransform:\n",
    "    \"\"\"\n",
    "    - Resize images to (IMG_SIZE, IMG_SIZE)\n",
    "    - Convert to tensor and normalize\n",
    "    - Rescale boxes + proposals accordingly\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=IMG_SIZE):\n",
    "        self.img_size = img_size\n",
    "        self.resize_img = T.Resize((img_size, img_size))\n",
    "        self.to_tensor = T.ToTensor()\n",
    "        self.normalize = T.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        )\n",
    "\n",
    "    def _process_view(self, img):\n",
    "        if img is None:\n",
    "            return None\n",
    "        img = self.resize_img(img)\n",
    "        img = self.to_tensor(img)\n",
    "        img = self.normalize(img)\n",
    "        return img\n",
    "\n",
    "    def __call__(self, sample: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        img_nat = sample[\"image_nat\"]\n",
    "        h0, w0 = img_nat.size[1], img_nat.size[0]\n",
    "\n",
    "        sample[\"image_nat\"]   = self._process_view(sample[\"image_nat\"])\n",
    "        sample[\"image_depth\"] = self._process_view(sample[\"image_depth\"])\n",
    "        sample[\"image_style\"] = self._process_view(sample[\"image_style\"])\n",
    "        sample[\"image_edge\"]  = self._process_view(sample[\"image_edge\"])\n",
    "\n",
    "        sx = self.img_size / float(w0)\n",
    "        sy = self.img_size / float(h0)\n",
    "\n",
    "        target = sample[\"target\"]\n",
    "        boxes = target[\"boxes\"].clone()\n",
    "        if boxes.numel() > 0:\n",
    "            boxes[:, 0] = boxes[:, 0] * sx\n",
    "            boxes[:, 1] = boxes[:, 1] * sy\n",
    "            boxes[:, 2] = boxes[:, 2] * sx\n",
    "            boxes[:, 3] = boxes[:, 3] * sy\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"orig_size\"] = target[\"orig_size\"]\n",
    "        sample[\"target\"] = target\n",
    "\n",
    "        for entry in sample[\"proposals\"]:\n",
    "            b = entry[\"boxes\"].clone()\n",
    "            if b.numel() == 0:\n",
    "                continue\n",
    "            b[:, 0] = b[:, 0] * sx\n",
    "            b[:, 1] = b[:, 1] * sy\n",
    "            b[:, 2] = b[:, 2] * sx\n",
    "            b[:, 3] = b[:, 3] * sy\n",
    "            entry[\"boxes\"] = b\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "def vclr_collate(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    out = {\n",
    "        \"image_nat\": [],\n",
    "        \"image_depth\": [],\n",
    "        \"image_style\": [],\n",
    "        \"image_edge\": [],\n",
    "        \"targets\": [],\n",
    "        \"proposals\": [],\n",
    "    }\n",
    "    for b in batch:\n",
    "        out[\"image_nat\"].append(b[\"image_nat\"])\n",
    "        out[\"image_depth\"].append(b[\"image_depth\"])\n",
    "        out[\"image_style\"].append(b[\"image_style\"])\n",
    "        out[\"image_edge\"].append(b[\"image_edge\"])\n",
    "        out[\"targets\"].append(b[\"target\"])\n",
    "        out[\"proposals\"].append(b[\"proposals\"])\n",
    "\n",
    "    out[\"image_nat\"] = torch.stack(out[\"image_nat\"], dim=0)\n",
    "    return out\n",
    "\n",
    "\n",
    "train_transform = VCLRTransform(img_size=IMG_SIZE)\n",
    "\n",
    "# ---- Train/val split on train_items ----\n",
    "indices = list(range(len(train_items)))\n",
    "rng = random.Random(SEED)\n",
    "rng.shuffle(indices)\n",
    "\n",
    "val_frac = 0.1  # 10% for validation loss\n",
    "split = int((1.0 - val_frac) * len(indices))\n",
    "train_indices = indices[:split]\n",
    "val_indices   = indices[split:]\n",
    "\n",
    "train_items_train = [train_items[i] for i in train_indices]\n",
    "val_items_loss    = [train_items[i] for i in val_indices]\n",
    "\n",
    "print(\"Train items:\", len(train_items_train), \"Val items (loss):\", len(val_items_loss))\n",
    "\n",
    "train_dataset = VCLRTrainSubset(\n",
    "    items=train_items_train,\n",
    "    img_dir=COCO_TRAIN_IMG_DIR,\n",
    "    depth_dir=str(DEPTH_TRAIN_DIR),\n",
    "    stylized_dir=str(STYLE_TRAIN_DIR),\n",
    "    edge_dir=str(EDGE_TRAIN_DIR) if EDGE_TRAIN_DIR is not None else None,\n",
    "    proposals=proposal_dict,\n",
    "    transform=train_transform,\n",
    ")\n",
    "\n",
    "val_dataset_loss = VCLRTrainSubset(\n",
    "    items=val_items_loss,\n",
    "    img_dir=COCO_TRAIN_IMG_DIR,\n",
    "    depth_dir=str(DEPTH_TRAIN_DIR),\n",
    "    stylized_dir=str(STYLE_TRAIN_DIR),\n",
    "    edge_dir=str(EDGE_TRAIN_DIR) if EDGE_TRAIN_DIR is not None else None,\n",
    "    proposals=proposal_dict,\n",
    "    transform=train_transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=TRAIN_NUM_WORKERS,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    "    collate_fn=vclr_collate,\n",
    ")\n",
    "\n",
    "val_loader_loss = DataLoader(\n",
    "    val_dataset_loss,\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=VAL_NUM_WORKERS,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    "    collate_fn=vclr_collate,\n",
    ")\n",
    "\n",
    "print(\"Train batches:\", len(train_loader), \"Val (loss) batches:\", len(val_loader_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28f9ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Box utilities ===\n",
    "\n",
    "def box_xywh_to_cxcywh(boxes: torch.Tensor) -> torch.Tensor:\n",
    "    x, y, w, h = boxes.unbind(-1)\n",
    "    cx = x + 0.5 * w\n",
    "    cy = y + 0.5 * h\n",
    "    return torch.stack([cx, cy, w, h], dim=-1)\n",
    "\n",
    "def box_cxcywh_to_xyxy(boxes: torch.Tensor) -> torch.Tensor:\n",
    "    cx, cy, w, h = boxes.unbind(-1)\n",
    "    x1 = cx - 0.5 * w\n",
    "    y1 = cy - 0.5 * h\n",
    "    x2 = cx + 0.5 * w\n",
    "    y2 = cy + 0.5 * h\n",
    "    return torch.stack([x1, y1, x2, y2], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0af15af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable params: 27764997\n"
     ]
    }
   ],
   "source": [
    "# === ConvNeXt-based dense detector with partial fine-tuning + greedy GPU matching ===\n",
    "\n",
    "class ConvNeXtDetector(nn.Module):\n",
    "    def __init__(self, d_model: int = 256, unfreeze_stages: int = 2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        try:\n",
    "            from torchvision.models import ConvNeXt_Tiny_Weights\n",
    "            weights = ConvNeXt_Tiny_Weights.IMAGENET1K_V1\n",
    "            backbone = convnext_tiny(weights=weights)\n",
    "        except Exception:\n",
    "            backbone = convnext_tiny(pretrained=True)\n",
    "\n",
    "        for p in backbone.features.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        if unfreeze_stages > 0:\n",
    "            for layer in backbone.features[-unfreeze_stages:]:\n",
    "                for p in layer.parameters():\n",
    "                    p.requires_grad = True\n",
    "\n",
    "        self.backbone_features = backbone.features  # [B,C,Hf,Wf]\n",
    "\n",
    "        self.conv_proj: Optional[nn.Conv2d] = None\n",
    "\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Conv2d(d_model, d_model, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(d_model, 1, kernel_size=1),\n",
    "        )\n",
    "        self.box_head = nn.Sequential(\n",
    "            nn.Conv2d(d_model, d_model, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(d_model, 4, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        images: torch.Tensor,\n",
    "        targets: Optional[List[Dict[str, Any]]] = None,\n",
    "        proposals: Optional[List[Any]] = None,\n",
    "        view_name: str = \"nat\",\n",
    "    ):\n",
    "        B, _, H, W = images.shape\n",
    "        images = images.to(memory_format=torch.channels_last)\n",
    "\n",
    "        feat = self.backbone_features(images)  # [B,C,Hf,Wf]\n",
    "        if self.conv_proj is None:\n",
    "            self.conv_proj = nn.Conv2d(feat.shape[1], self.d_model, kernel_size=1).to(feat.device)\n",
    "        feat = self.conv_proj(feat)           # [B,d_model,Hf,Wf]\n",
    "\n",
    "        cls_map = self.cls_head(feat)         # [B,1,Hf,Wf]\n",
    "        box_map = self.box_head(feat).sigmoid()  # [B,4,Hf,Wf]\n",
    "\n",
    "        B, _, Hf, Wf = cls_map.shape\n",
    "        N = Hf * Wf\n",
    "\n",
    "        pred_logits = cls_map.view(B, 1, N).permute(0, 2, 1)   # [B,N,1]\n",
    "        pred_boxes  = box_map.view(B, 4, N).permute(0, 2, 1)   # [B,N,4]\n",
    "        query_feats = feat.view(B, self.d_model, N).permute(0, 2, 1)\n",
    "\n",
    "        outputs_raw = {\n",
    "            \"pred_logits\": pred_logits,\n",
    "            \"pred_boxes\": pred_boxes,\n",
    "            \"query_feats\": query_feats,\n",
    "        }\n",
    "\n",
    "        outputs_list: List[Dict[str, torch.Tensor]] = []\n",
    "        for b in range(B):\n",
    "            scores = pred_logits[b].sigmoid().squeeze(-1)      # [N]\n",
    "            boxes_norm_cxcywh = pred_boxes[b]                  # [N,4]\n",
    "            boxes_norm_xyxy = box_cxcywh_to_xyxy(boxes_norm_cxcywh)\n",
    "            scale = torch.tensor([W, H, W, H], device=boxes_norm_xyxy.device)\n",
    "            boxes_xyxy = boxes_norm_xyxy * scale\n",
    "            outputs_list.append({\n",
    "                \"boxes\": boxes_xyxy,\n",
    "                \"scores\": scores,\n",
    "                \"masks\": None,\n",
    "                \"query_feats\": query_feats[b],\n",
    "            })\n",
    "\n",
    "        if self.training and targets is not None:\n",
    "            loss_dict = self.compute_losses(outputs_raw, targets, (H, W))\n",
    "        else:\n",
    "            loss_dict = {}\n",
    "\n",
    "        return outputs_list, loss_dict\n",
    "\n",
    "    def compute_losses(\n",
    "        self,\n",
    "        outputs_raw: Dict[str, torch.Tensor],\n",
    "        targets: List[Dict[str, Any]],\n",
    "        img_size_hw: Tuple[int, int],\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        pred_logits = outputs_raw[\"pred_logits\"]  # [B,N,1]\n",
    "        pred_boxes  = outputs_raw[\"pred_boxes\"]   # [B,N,4]\n",
    "\n",
    "        B, N, _ = pred_boxes.shape\n",
    "        H, W = img_size_hw\n",
    "\n",
    "        loss_cls_list = []\n",
    "        loss_bbox_list = []\n",
    "        loss_giou_list = []\n",
    "\n",
    "        for b in range(B):\n",
    "            logits = pred_logits[b].squeeze(-1)  # [N]\n",
    "            pb = pred_boxes[b]                  # [N,4]\n",
    "\n",
    "            tgt = targets[b]\n",
    "            gt_boxes_xywh = tgt[\"boxes\"].to(pb.device)  # [Ng,4] in resized pixels\n",
    "\n",
    "            if gt_boxes_xywh.numel() == 0:\n",
    "                target_classes = torch.zeros(N, dtype=torch.float32, device=logits.device)\n",
    "                loss_cls_list.append(\n",
    "                    F.binary_cross_entropy_with_logits(logits, target_classes)\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            sizes = torch.tensor([W, H, W, H], device=pb.device)\n",
    "            gt_boxes_norm_xywh = gt_boxes_xywh / sizes\n",
    "            gt_boxes_norm_cxcywh = box_xywh_to_cxcywh(gt_boxes_norm_xywh)\n",
    "\n",
    "            pred_xyxy = box_cxcywh_to_xyxy(pb)\n",
    "            gt_xyxy   = box_cxcywh_to_xyxy(gt_boxes_norm_cxcywh)\n",
    "\n",
    "            ious = box_iou(pred_xyxy, gt_xyxy)  # [N_pred, N_gt]\n",
    "\n",
    "            best_pred_for_gt = ious.argmax(dim=0)         # [Ng]\n",
    "            matched_pred_idx = best_pred_for_gt.unique()  # [Nm]\n",
    "            matched_gt_idx = []\n",
    "            for j in matched_pred_idx:\n",
    "                gt_idxs = (best_pred_for_gt == j).nonzero(as_tuple=False).squeeze(-1)\n",
    "                _, k = ious[j, gt_idxs].max(dim=0)\n",
    "                matched_gt_idx.append(gt_idxs[k])\n",
    "            matched_gt_idx = torch.stack(matched_gt_idx, dim=0)\n",
    "\n",
    "            target_classes = torch.zeros(N, dtype=torch.float32, device=logits.device)\n",
    "            target_classes[matched_pred_idx] = 1.0\n",
    "\n",
    "            loss_cls_list.append(\n",
    "                F.binary_cross_entropy_with_logits(logits, target_classes)\n",
    "            )\n",
    "\n",
    "            pb_matched = pb[matched_pred_idx]\n",
    "            gt_matched = gt_boxes_norm_cxcywh[matched_gt_idx]\n",
    "\n",
    "            loss_bbox_list.append(F.l1_loss(pb_matched, gt_matched, reduction=\"mean\"))\n",
    "\n",
    "            giou = generalized_box_iou(\n",
    "                box_cxcywh_to_xyxy(pb_matched),\n",
    "                box_cxcywh_to_xyxy(gt_matched),\n",
    "            )\n",
    "            loss_giou_list.append(1.0 - giou.diag().mean())\n",
    "\n",
    "        loss_cls  = torch.stack(loss_cls_list).mean()\n",
    "        loss_bbox = torch.stack(loss_bbox_list).mean() if loss_bbox_list else torch.tensor(0.0, device=loss_cls.device)\n",
    "        loss_giou = torch.stack(loss_giou_list).mean() if loss_giou_list else torch.tensor(0.0, device=loss_cls.device)\n",
    "\n",
    "        return {\n",
    "            \"loss_cls\": loss_cls,\n",
    "            \"loss_bbox\": loss_bbox,\n",
    "            \"loss_giou\": loss_giou,\n",
    "        }\n",
    "\n",
    "\n",
    "model = ConvNeXtDetector(d_model=256, unfreeze_stages=4).to(device)\n",
    "model = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=BASE_LR, weight_decay=1e-4\n",
    ")\n",
    "\n",
    "def adjust_lr(optimizer, epoch: int):\n",
    "    factor = 0.1 if epoch >= 7 else 1.0\n",
    "    for pg in optimizer.param_groups:\n",
    "        pg[\"lr\"] = BASE_LR * factor\n",
    "\n",
    "print(\"Total trainable params:\",\n",
    "      sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2de91a17-1fb3-4df2-9b23-853fdd74650b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] trainable backbone params: 116\n",
      "[DEBUG] trainable head params:     8\n",
      "Total trainable params: 27764997\n"
     ]
    }
   ],
   "source": [
    "# === Teacher-Student wrapper around ConvNeXtDetector (v-CLR two-branch) ===\n",
    "\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "\n",
    "class TeacherStudentVCLR(nn.Module):\n",
    "    \"\"\"\n",
    "    EMA teacher–student wrapper around ConvNeXtDetector.\n",
    "    - teacher: natural-image branch (no gradients, EMA-updated)\n",
    "    - student: transformed-image branch (gets all supervision + gradients)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_detector: nn.Module, ema_momentum: float = 0.999):\n",
    "        super().__init__()\n",
    "        self.student = base_detector           # reuse your existing ConvNeXtDetector\n",
    "        self.teacher = copy.deepcopy(base_detector)\n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.ema_m = ema_momentum\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_teacher(self):\n",
    "        m = self.ema_m\n",
    "        for t_param, s_param in zip(self.teacher.parameters(), self.student.parameters()):\n",
    "            t_param.data.mul_(m).add_(s_param.data, alpha=1.0 - m)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        \"\"\"\n",
    "        Only the student is put in train() / eval().\n",
    "        Teacher is always kept in eval mode.\n",
    "        \"\"\"\n",
    "        self.student.train(mode)\n",
    "        self.teacher.eval()\n",
    "        self.training = mode\n",
    "        return self\n",
    "\n",
    "    def eval(self):  # type: ignore[override]\n",
    "        return self.train(False)\n",
    "\n",
    "\n",
    "# Wrap your existing single-branch ConvNeXtDetector as student\n",
    "# (assumes `model` already exists from the previous cell)\n",
    "base_detector = model\n",
    "model = TeacherStudentVCLR(base_detector, ema_momentum=0.999).to(device)\n",
    "\n",
    "# Use channels_last for both branches to speed up ConvNeXt\n",
    "model.student = model.student.to(memory_format=torch.channels_last)\n",
    "model.teacher = model.teacher.to(memory_format=torch.channels_last)\n",
    "\n",
    "# --- Optimizer + LR schedule (backbone vs head LRs) ---\n",
    "\n",
    "# BASE_LR should already be defined above (your \"main\" LR)\n",
    "backbone_lr = BASE_LR * 0.1   # smaller LR for pretrained ConvNeXt backbone\n",
    "head_lr     = BASE_LR         # original LR for detection/mask/v-CLR heads\n",
    "\n",
    "# Identify backbone parameters on the *student* branch\n",
    "backbone_param_ids = {id(p) for p in model.student.backbone_features.parameters()}\n",
    "\n",
    "backbone_params = []\n",
    "head_params = []\n",
    "\n",
    "for p in model.parameters():\n",
    "    if not p.requires_grad:\n",
    "        continue\n",
    "    if id(p) in backbone_param_ids:\n",
    "        backbone_params.append(p)\n",
    "    else:\n",
    "        head_params.append(p)\n",
    "\n",
    "print(f\"[DEBUG] trainable backbone params: {len(backbone_params)}\")\n",
    "print(f\"[DEBUG] trainable head params:     {len(head_params)}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": backbone_params, \"lr\": backbone_lr},\n",
    "        {\"params\": head_params,     \"lr\": head_lr},\n",
    "    ],\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "# Remember each group's initial LR so we keep the ratio when decaying\n",
    "for pg in optimizer.param_groups:\n",
    "    pg[\"initial_lr\"] = pg[\"lr\"]\n",
    "\n",
    "def adjust_lr(optimizer, epoch: int):\n",
    "    \"\"\"\n",
    "    Simple step schedule: drop all LRs by 10× after epoch 7,\n",
    "    while preserving the backbone/head LR ratio.\n",
    "    \"\"\"\n",
    "    factor = 0.1 if epoch >= 7 else 1.0\n",
    "    for pg in optimizer.param_groups:\n",
    "        pg[\"lr\"] = pg[\"initial_lr\"] * factor\n",
    "\n",
    "print(\n",
    "    \"Total trainable params:\",\n",
    "    sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7054dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === v-CLR auxiliary losses (L_obj, L_sim) – NaN-safe L_gt ===\n",
    "\n",
    "def cosine_sim_loss(q1: torch.Tensor, q2: torch.Tensor, temperature: float = 0.07) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity loss between query features with temperature scaling.\n",
    "    \n",
    "    Temperature scaling is standard in contrastive learning (e.g., SimCLR, CLIP).\n",
    "    Lower temperature makes the similarity more peaked (sharper), encouraging \n",
    "    better feature alignment. Default temperature of 0.07 is commonly used.\n",
    "    \n",
    "    The loss is computed as: 1 - cosine_similarity\n",
    "    Temperature is applied to scale the similarity before computing the loss,\n",
    "    making the optimization more sensitive to small differences.\n",
    "    \"\"\"\n",
    "    q1 = F.normalize(q1, dim=-1)\n",
    "    q2 = F.normalize(q2, dim=-1)\n",
    "    # Compute raw cosine similarity (range: [-1, 1])\n",
    "    similarity = (q1 * q2).sum(dim=-1)\n",
    "    # Clamp to valid cosine similarity range\n",
    "    similarity = similarity.clamp(-1, 1)\n",
    "    # Apply temperature: lower temp = sharper gradients for near-perfect matches\n",
    "    # The loss is (1 - similarity) / temperature, but we normalize by temperature\n",
    "    # to keep the loss scale reasonable\n",
    "    loss = (1.0 - similarity) / temperature\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def match_proposals_to_predictions(\n",
    "    proposal_boxes: torch.Tensor,\n",
    "    pred_boxes: torch.Tensor,\n",
    "    iou_thresh: float = 0.3,\n",
    "    max_pairs: int = 32,\n",
    "):\n",
    "    \"\"\"\n",
    "    Greedy IoU-based matching: for each proposal, pick the best prediction\n",
    "    above iou_thresh, then truncate to at most max_pairs matches.\n",
    "    Returns:\n",
    "      idx_p:    indices into proposal_boxes  [K]\n",
    "      idx_pred: indices into pred_boxes      [K]\n",
    "    \"\"\"\n",
    "    if proposal_boxes.numel() == 0 or pred_boxes.numel() == 0:\n",
    "        device = pred_boxes.device\n",
    "        return (\n",
    "            torch.empty(0, dtype=torch.long, device=device),\n",
    "            torch.empty(0, dtype=torch.long, device=device),\n",
    "        )\n",
    "\n",
    "    ious = box_iou(proposal_boxes, pred_boxes)  # [Np, Npred]\n",
    "    best_iou, best_idx = ious.max(dim=1)        # for each proposal: best pred\n",
    "\n",
    "    keep = best_iou > iou_thresh\n",
    "    idx_p = torch.nonzero(keep, as_tuple=False).squeeze(-1)\n",
    "    idx_pred = best_idx[keep]\n",
    "\n",
    "    if idx_p.numel() > max_pairs:\n",
    "        idx_p = idx_p[:max_pairs]\n",
    "        idx_pred = idx_pred[:max_pairs]\n",
    "\n",
    "    return idx_p, idx_pred\n",
    "\n",
    "\n",
    "def _intersect1d_with_indices(a: torch.Tensor, b: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Slow but simple 1D \"set intersection\" with index tracking.\n",
    "\n",
    "    Returns:\n",
    "      common:      values common to a and b          [K]\n",
    "      idx_a_local: indices in a for those values     [K]\n",
    "      idx_b_local: indices in b for those values     [K]\n",
    "    \"\"\"\n",
    "    assert a.dim() == 1 and b.dim() == 1\n",
    "    device = a.device\n",
    "    common_vals = []\n",
    "    idx_a = []\n",
    "    idx_b = []\n",
    "\n",
    "    for i in range(a.numel()):\n",
    "        v = a[i]\n",
    "        matches = (b == v).nonzero(as_tuple=False)\n",
    "        if matches.numel() > 0:\n",
    "            common_vals.append(v)\n",
    "            idx_a.append(i)\n",
    "            idx_b.append(matches[0].item())\n",
    "\n",
    "    if len(common_vals) == 0:\n",
    "        return (\n",
    "            torch.empty(0, dtype=a.dtype, device=device),\n",
    "            torch.empty(0, dtype=torch.long, device=device),\n",
    "            torch.empty(0, dtype=torch.long, device=device),\n",
    "        )\n",
    "\n",
    "    common = torch.stack(common_vals).to(device)\n",
    "    idx_a_local = torch.tensor(idx_a, dtype=torch.long, device=device)\n",
    "    idx_b_local = torch.tensor(idx_b, dtype=torch.long, device=device)\n",
    "    return common, idx_a_local, idx_b_local\n",
    "\n",
    "\n",
    "def compute_vclr_losses(\n",
    "    outputs_nat,   loss_nat,\n",
    "    outputs_depth, loss_depth,\n",
    "    outputs_style, loss_style,\n",
    "    proposals_batch,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Aggregate:\n",
    "      - L_gt   : detection loss (nat + optional depth/style)\n",
    "      - L_obj  : proposal–prediction L1 alignment across views\n",
    "      - L_sim  : query feature cosine similarity between nat and other views\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- device for scalar tensors ----\n",
    "    # outputs_nat is a list of per-image dicts; each has \"boxes\".\n",
    "    device = outputs_nat[0][\"boxes\"].device\n",
    "\n",
    "    def sum_loss_dict(ld: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sum detection losses in a dict, ignoring non-finite terms.\n",
    "        Always returns a 0-D tensor on `device`.\n",
    "        \"\"\"\n",
    "        if not ld:\n",
    "            return torch.tensor(0.0, device=device)\n",
    "\n",
    "        vals = []\n",
    "        for v in ld.values():\n",
    "            if not torch.is_tensor(v):\n",
    "                continue\n",
    "            v = v.to(device=device, dtype=torch.float32)\n",
    "            if not torch.isfinite(v):\n",
    "                # Rare; if this ever triggers, it's exactly the source of NaNs.\n",
    "                # Uncomment if you want explicit logging:\n",
    "                # print(f\"[WARN] non-finite detection loss term {v.item()} in sum_loss_dict, skipping.\")\n",
    "                continue\n",
    "            vals.append(v)\n",
    "\n",
    "        if not vals:\n",
    "            return torch.tensor(0.0, device=device)\n",
    "        # NOTE: no reweighting here; this keeps the same behaviour as before.\n",
    "        return torch.stack(vals).sum()\n",
    "\n",
    "    # ---- Ground-truth supervised term L_gt (nat + optional depth/style) ----\n",
    "    L_gt_nat   = sum_loss_dict(loss_nat)\n",
    "    L_gt_depth = sum_loss_dict(loss_depth)\n",
    "    L_gt_style = sum_loss_dict(loss_style)\n",
    "\n",
    "    # All of these are now guaranteed 0-D tensors on `device`, no Python floats.\n",
    "    L_gt = L_gt_nat + L_gt_depth + L_gt_style\n",
    "\n",
    "    # ---- Proposal-objectness term L_obj and query-similarity term L_sim ----\n",
    "    L_obj_list: List[torch.Tensor] = []\n",
    "    L_sim_list: List[torch.Tensor] = []\n",
    "\n",
    "    B = len(outputs_nat)\n",
    "\n",
    "    for b in range(B):\n",
    "        props = proposals_batch[b]\n",
    "        if not props:\n",
    "            continue\n",
    "\n",
    "        # proposals: list of dicts (we use the first entry)\n",
    "        prop_boxes = props[0][\"boxes\"].to(device)\n",
    "\n",
    "        # Per-view outputs for this image\n",
    "        nat_out   = outputs_nat[b]\n",
    "        depth_out = outputs_depth[b] if outputs_depth else None\n",
    "        style_out = outputs_style[b] if outputs_style else None\n",
    "\n",
    "        # 1) L_obj: L1 alignment between proposals and predicted boxes per view\n",
    "        #    Use normalized coordinates (divide by image size) so this lives on ~[0, 1]\n",
    "        for ov in [nat_out, depth_out, style_out]:\n",
    "            if ov is None:\n",
    "                continue\n",
    "            pred_boxes = ov[\"boxes\"]  # xyxy in pixels\n",
    "            if pred_boxes.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            idx_p, idx_pred = match_proposals_to_predictions(prop_boxes, pred_boxes)\n",
    "            if idx_p.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            sel_props = prop_boxes[idx_p]   # [K,4], xyxy in pixels\n",
    "            sel_preds = pred_boxes[idx_pred]\n",
    "\n",
    "            # Normalize coordinates by image size\n",
    "            scale_xyxy = torch.tensor(\n",
    "                [IMG_SIZE, IMG_SIZE, IMG_SIZE, IMG_SIZE],\n",
    "                device=sel_preds.device,\n",
    "                dtype=sel_preds.dtype,\n",
    "            )\n",
    "            sel_props_norm = sel_props / scale_xyxy\n",
    "            sel_preds_norm = sel_preds / scale_xyxy\n",
    "\n",
    "            L_obj_list.append(F.l1_loss(sel_preds_norm, sel_props_norm))\n",
    "\n",
    "\n",
    "        # 2) L_sim: cosine similarity between query features for nat vs each other view\n",
    "        idx_p_nat, idx_pred_nat = match_proposals_to_predictions(\n",
    "            prop_boxes, nat_out[\"boxes\"]\n",
    "        )\n",
    "        if idx_p_nat.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        def get_query_feats(out, idx_pred):\n",
    "            if out is None:\n",
    "                return None\n",
    "            q = out.get(\"query_feats\", None)\n",
    "            if q is None:\n",
    "                return None\n",
    "            if idx_pred.numel() == 0 or idx_pred.max().item() >= q.shape[0]:\n",
    "                return None\n",
    "            return q[idx_pred]\n",
    "\n",
    "        for ov in [depth_out, style_out]:\n",
    "            if ov is None:\n",
    "                continue\n",
    "\n",
    "            idx_p_view, idx_pred_view = match_proposals_to_predictions(\n",
    "                prop_boxes, ov[\"boxes\"]\n",
    "            )\n",
    "            if idx_p_view.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            common_prop, idx_nat_local, idx_view_local = _intersect1d_with_indices(\n",
    "                idx_p_nat, idx_p_view\n",
    "            )\n",
    "            if common_prop.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            q_nat  = get_query_feats(nat_out,  idx_pred_nat[idx_nat_local])\n",
    "            q_view = get_query_feats(ov,       idx_pred_view[idx_view_local])\n",
    "\n",
    "            if q_nat is None or q_view is None or q_nat.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            L_sim_list.append(cosine_sim_loss(q_nat, q_view))\n",
    "\n",
    "    # Safe defaults if we collected no matches\n",
    "    L_obj = (\n",
    "        torch.stack(L_obj_list).mean()\n",
    "        if L_obj_list\n",
    "        else torch.tensor(0.0, device=device)\n",
    "    )\n",
    "    L_sim = (\n",
    "        torch.stack(L_sim_list).mean()\n",
    "        if L_sim_list\n",
    "        else torch.tensor(0.0, device=device)\n",
    "    )\n",
    "\n",
    "    # v-CLR total loss (same structure as before)\n",
    "    L_total = (\n",
    "        LAMBDA_GT * L_gt +\n",
    "        LAMBDA_MATCH * (LAMBDA_OBJ * L_obj + LAMBDA_SIM * L_sim)\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"L_gt\":    L_gt,\n",
    "        \"L_obj\":   L_obj,\n",
    "        \"L_sim\":   L_sim,\n",
    "        \"L_total\": L_total,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efe378d8-2828-4702-bfd1-c4178dbe63db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# History for plotting\n",
    "train_history = {\n",
    "    \"epoch\": [],\n",
    "    \"L_total_train\": [],\n",
    "    \"L_gt_train\": [],\n",
    "    \"L_obj_train\": [],\n",
    "    \"L_sim_train\": [],\n",
    "    \"L_total_val\": [],\n",
    "    \"L_gt_val\": [],\n",
    "    \"L_obj_val\": [],\n",
    "    \"L_sim_val\": [],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4943d1b9-a194-4dfc-bb00-04affbb768a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Validation loss loop (v-CLR with teacher–student, robust view handling) ===\n",
    "\n",
    "def compute_epoch_loss_on_loader(model: nn.Module, loader: DataLoader) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    is_ts = hasattr(model, \"student\") and hasattr(model, \"teacher\")\n",
    "    student = model.student if is_ts else model\n",
    "    teacher = model.teacher if is_ts else model\n",
    "\n",
    "    total_L_total = 0.0\n",
    "    total_L_gt    = 0.0\n",
    "    total_L_obj   = 0.0\n",
    "    total_L_sim   = 0.0\n",
    "    batch_count   = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images_nat   = batch[\"image_nat\"].to(device, non_blocking=True)\n",
    "            images_depth = _normalize_view_batch(batch[\"image_depth\"], device)\n",
    "            images_style = _normalize_view_batch(batch[\"image_style\"], device)\n",
    "            targets      = batch[\"targets\"]\n",
    "            proposals    = batch[\"proposals\"]\n",
    "\n",
    "            has_depth = images_depth is not None\n",
    "            has_style = images_style is not None\n",
    "\n",
    "            if has_depth and has_style:\n",
    "                if random.random() < 0.5:\n",
    "                    use_depth, use_style = True, False\n",
    "                else:\n",
    "                    use_depth, use_style = False, True\n",
    "            else:\n",
    "                use_depth, use_style = has_depth, has_style\n",
    "\n",
    "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                if is_ts:\n",
    "                    # EMA teacher on natural images (no gradient)\n",
    "                    outputs_nat_teacher, _ = teacher(\n",
    "                        images_nat, targets=None, proposals=proposals, view_name=\"nat\"\n",
    "                    )\n",
    "                    # Student gets GT detection loss on nat\n",
    "                    outputs_nat_student, loss_nat = student(\n",
    "                        images_nat, targets=targets, proposals=proposals, view_name=\"nat\"\n",
    "                    )\n",
    "                    outputs_nat = outputs_nat_teacher\n",
    "                else:\n",
    "                    outputs_nat, loss_nat = student(\n",
    "                        images_nat, targets=targets, proposals=proposals, view_name=\"nat\"\n",
    "                    )\n",
    "\n",
    "                outputs_depth, loss_depth = outputs_nat, {}\n",
    "                outputs_style, loss_style = outputs_nat, {}\n",
    "\n",
    "                # NOTE: Depth/style views do NOT receive GT targets per v-CLR paper.\n",
    "                if use_depth and images_depth is not None:\n",
    "                    outputs_depth, loss_depth = student(\n",
    "                        images_depth, targets=None, proposals=proposals, view_name=\"depth\"\n",
    "                    )\n",
    "                if use_style and images_style is not None:\n",
    "                    outputs_style, loss_style = student(\n",
    "                        images_style, targets=None, proposals=proposals, view_name=\"style\"\n",
    "                    )\n",
    "\n",
    "                vclr_loss = compute_vclr_losses(\n",
    "                    outputs_nat,  loss_nat,\n",
    "                    outputs_depth, loss_depth,\n",
    "                    outputs_style, loss_style,\n",
    "                    proposals,\n",
    "                )\n",
    "                loss = vclr_loss[\"L_total\"]\n",
    "\n",
    "            total_L_total += float(loss)\n",
    "            total_L_gt    += float(vclr_loss[\"L_gt\"])\n",
    "            total_L_obj   += float(vclr_loss[\"L_obj\"])\n",
    "            total_L_sim   += float(vclr_loss[\"L_sim\"])\n",
    "            batch_count   += 1\n",
    "\n",
    "    if batch_count == 0:\n",
    "        return {\"L_total\": 0.0, \"L_gt\": 0.0, \"L_obj\": 0.0, \"L_sim\": 0.0}\n",
    "\n",
    "    return {\n",
    "        \"L_total\": total_L_total / batch_count,\n",
    "        \"L_gt\":    total_L_gt    / batch_count,\n",
    "        \"L_obj\":   total_L_obj   / batch_count,\n",
    "        \"L_sim\":   total_L_sim   / batch_count,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f44f2a3-9619-4b80-8f74-0d9cd25bdf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional, Union, List\n",
    "\n",
    "def _normalize_view_batch(\n",
    "    x: Union[None, torch.Tensor, List[Optional[torch.Tensor]]],\n",
    "    device: torch.device,\n",
    ") -> Optional[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Normalize depth/style batch from collate_fn:\n",
    "      - None -> None\n",
    "      - Tensor -> Tensor on device\n",
    "      - list[Tensor] -> stacked Tensor on device\n",
    "      - list[None] -> None\n",
    "    Raises if list contains a mix of None and Tensor, which indicates a real data issue.\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.to(device, non_blocking=True)\n",
    "\n",
    "    if isinstance(x, list):\n",
    "        if len(x) == 0:\n",
    "            return None\n",
    "        if all(v is None for v in x):\n",
    "            return None\n",
    "        if all(isinstance(v, torch.Tensor) for v in x):\n",
    "            return torch.stack(x, dim=0).to(device, non_blocking=True)\n",
    "        # Mixed None and Tensor -> this is a bug upstream\n",
    "        raise RuntimeError(\"image_depth/image_style list has mixed None and Tensors; please fix Dataset/Collate.\")\n",
    "\n",
    "    raise TypeError(f\"Unexpected type for view batch: {type(x)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad966d44-2d95-47c1-9aaf-8de80fc5c82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUG: single-batch inspection (nat view only) ===\n",
      "len(train_dataset) = 106458\n",
      "len(train_loader)  = 10646\n",
      "[DEBUG] type(images_nat): <class 'torch.Tensor'>\n",
      "[DEBUG] images_nat.shape: torch.Size([10, 3, 800, 800])\n",
      "[DEBUG] batch size (targets): 10\n",
      "[DEBUG] batch size (proposals): 10\n",
      "\n",
      "[DEBUG] sample 0: image_id=578350, num_gt_boxes=8\n",
      "  GT w/h min: [29.145540237426758, 80.4749984741211] max: [553.4647827148438, 481.75]\n",
      "  any NaN in GT boxes?  False\n",
      "  any Inf in GT boxes?   False\n",
      "\n",
      "[DEBUG] sample 1: image_id=45387, num_gt_boxes=0\n",
      "  -> no GT boxes\n",
      "\n",
      "[DEBUG] sample 2: image_id=174213, num_gt_boxes=3\n",
      "  GT w/h min: [23.125, 102.18267059326172] max: [122.61249542236328, 250.49180603027344]\n",
      "  any NaN in GT boxes?  False\n",
      "  any Inf in GT boxes?   False\n",
      "\n",
      "[DEBUG] sample 3: image_id=518415, num_gt_boxes=0\n",
      "  -> no GT boxes\n",
      "\n",
      "[DEBUG] sample 4: image_id=67443, num_gt_boxes=1\n",
      "  GT w/h min: [169.21249389648438, 410.2857360839844] max: [169.21249389648438, 410.2857360839844]\n",
      "  any NaN in GT boxes?  False\n",
      "  any Inf in GT boxes?   False\n",
      "\n",
      "[DEBUG] sample 5: image_id=472344, num_gt_boxes=9\n",
      "  GT w/h min: [20.784313201904297, 45.59477233886719] max: [352.82354736328125, 118.77124786376953]\n",
      "  any NaN in GT boxes?  False\n",
      "  any Inf in GT boxes?   False\n",
      "\n",
      "[DEBUG] sample 6: image_id=15794, num_gt_boxes=3\n",
      "  GT w/h min: [50.6875, 198.7833251953125] max: [109.2125015258789, 364.9499816894531]\n",
      "  any NaN in GT boxes?  False\n",
      "  any Inf in GT boxes?   False\n",
      "\n",
      "[DEBUG] sample 7: image_id=312316, num_gt_boxes=6\n",
      "  GT w/h min: [17.108434677124023, 20.91200065612793] max: [530.650634765625, 751.760009765625]\n",
      "  any NaN in GT boxes?  False\n",
      "  any Inf in GT boxes?   False\n",
      "\n",
      "[DEBUG] sample 8: image_id=488796, num_gt_boxes=1\n",
      "  GT w/h min: [402.0249938964844, 569.3552856445312] max: [402.0249938964844, 569.3552856445312]\n",
      "  any NaN in GT boxes?  False\n",
      "  any Inf in GT boxes?   False\n",
      "\n",
      "[DEBUG] sample 9: image_id=396054, num_gt_boxes=1\n",
      "  GT w/h min: [460.5749816894531, 591.925048828125] max: [460.5749816894531, 591.925048828125]\n",
      "  any NaN in GT boxes?  False\n",
      "  any Inf in GT boxes?   False\n",
      "\n",
      "[DEBUG] sample 0: proposals -> boxes torch.Size([15, 4]), scores torch.Size([15])\n",
      "  proposals min: 0.0 max: 800.0\n",
      "  any NaN in proposals?  False\n",
      "  any Inf in proposals?   False\n",
      "\n",
      "[DEBUG] sample 1: proposals -> boxes torch.Size([6, 4]), scores torch.Size([6])\n",
      "  proposals min: 117.56281280517578 max: 740.9398803710938\n",
      "  any NaN in proposals?  False\n",
      "  any Inf in proposals?   False\n",
      "\n",
      "[DEBUG] sample 2: proposals -> boxes torch.Size([8, 4]), scores torch.Size([8])\n",
      "  proposals min: 0.0 max: 800.0\n",
      "  any NaN in proposals?  False\n",
      "  any Inf in proposals?   False\n",
      "\n",
      "[DEBUG] sample 3: proposals -> boxes torch.Size([7, 4]), scores torch.Size([7])\n",
      "  proposals min: 0.0 max: 799.3956298828125\n",
      "  any NaN in proposals?  False\n",
      "  any Inf in proposals?   False\n",
      "\n",
      "[DEBUG] sample 4: proposals -> boxes torch.Size([7, 4]), scores torch.Size([7])\n",
      "  proposals min: 0.0 max: 800.0\n",
      "  any NaN in proposals?  False\n",
      "  any Inf in proposals?   False\n",
      "\n",
      "[DEBUG] sample 5: proposals -> boxes torch.Size([24, 4]), scores torch.Size([24])\n",
      "  proposals min: 0.0 max: 800.0\n",
      "  any NaN in proposals?  False\n",
      "  any Inf in proposals?   False\n",
      "\n",
      "[DEBUG] sample 6: proposals -> boxes torch.Size([30, 4]), scores torch.Size([30])\n",
      "  proposals min: 38.91666793823242 max: 800.0\n",
      "  any NaN in proposals?  False\n",
      "  any Inf in proposals?   False\n",
      "\n",
      "[DEBUG] sample 7: proposals -> boxes torch.Size([11, 4]), scores torch.Size([11])\n",
      "  proposals min: 0.0 max: 800.0\n",
      "  any NaN in proposals?  False\n",
      "  any Inf in proposals?   False\n",
      "\n",
      "[DEBUG] sample 8: proposals -> boxes torch.Size([6, 4]), scores torch.Size([6])\n",
      "  proposals min: 0.0 max: 800.0\n",
      "  any NaN in proposals?  False\n",
      "  any Inf in proposals?   False\n",
      "\n",
      "[DEBUG] sample 9: proposals -> boxes torch.Size([6, 4]), scores torch.Size([6])\n",
      "  proposals min: 6.309334754943848 max: 800.0\n",
      "  any NaN in proposals?  False\n",
      "  any Inf in proposals?   False\n",
      "\n",
      "[DEBUG] loss_nat raw: {}\n",
      "\n",
      "[DEBUG] teacher loss_nat raw: {}\n"
     ]
    }
   ],
   "source": [
    "# === DEBUG: inspect one batch (GT boxes, CutLER proposals, teacher/student losses) ===\n",
    "\n",
    "import math\n",
    "\n",
    "# Handle teacher–student or single model uniformly\n",
    "is_ts = hasattr(model, \"student\") and hasattr(model, \"teacher\")\n",
    "student = model.student if is_ts else model\n",
    "teacher = model.teacher if is_ts else None\n",
    "\n",
    "print(\"=== DEBUG: single-batch inspection (nat view only) ===\")\n",
    "print(f\"len(train_dataset) = {len(train_dataset)}\")\n",
    "print(f\"len(train_loader)  = {len(train_loader)}\")\n",
    "\n",
    "# Get a single batch from the train loader\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "images_nat = batch[\"image_nat\"]  # should be a tensor [B,3,H,W]\n",
    "print(\"[DEBUG] type(images_nat):\", type(images_nat))\n",
    "if isinstance(images_nat, torch.Tensor):\n",
    "    print(\"[DEBUG] images_nat.shape:\", images_nat.shape)\n",
    "else:\n",
    "    raise TypeError(f\"Expected images_nat to be Tensor, got {type(images_nat)}\")\n",
    "\n",
    "images_nat = images_nat.to(device, non_blocking=True)\n",
    "\n",
    "targets   = batch[\"targets\"]    # list of dict\n",
    "proposals = batch[\"proposals\"]  # list of list-of-dicts (CutLER proposals)\n",
    "\n",
    "print(f\"[DEBUG] batch size (targets): {len(targets)}\")\n",
    "print(f\"[DEBUG] batch size (proposals): {len(proposals)}\")\n",
    "\n",
    "# ---- 1) Inspect GT boxes for degenerate / weird values ----\n",
    "for i, tgt in enumerate(targets):\n",
    "    boxes = tgt[\"boxes\"]          # [Ng,4] in resized pixels (xywh in your pipeline)\n",
    "    img_id = int(tgt[\"image_id\"].item())\n",
    "    print(f\"\\n[DEBUG] sample {i}: image_id={img_id}, num_gt_boxes={boxes.shape[0]}\")\n",
    "\n",
    "    if boxes.numel() == 0:\n",
    "        print(\"  -> no GT boxes\")\n",
    "        continue\n",
    "\n",
    "    wh = boxes[:, 2:4]\n",
    "    print(\"  GT w/h min:\", wh.min(dim=0).values.cpu().tolist(),\n",
    "          \"max:\",        wh.max(dim=0).values.cpu().tolist())\n",
    "\n",
    "    print(\"  any NaN in GT boxes? \", torch.isnan(boxes).any().item())\n",
    "    print(\"  any Inf in GT boxes?  \",\n",
    "          (torch.isinf(boxes) & (boxes > 0)).any().item())\n",
    "\n",
    "# ---- 2) Inspect CutLER proposals for this batch ----\n",
    "for i, props in enumerate(proposals):\n",
    "    if not props:\n",
    "        print(f\"\\n[DEBUG] sample {i}: NO proposals\")\n",
    "        continue\n",
    "\n",
    "    entry = props[0]\n",
    "    boxes_p = entry[\"boxes\"]   # should now be xyxy after our earlier fix\n",
    "    scores_p = entry[\"scores\"]\n",
    "\n",
    "    print(f\"\\n[DEBUG] sample {i}: proposals -> boxes {boxes_p.shape}, scores {scores_p.shape}\")\n",
    "    if boxes_p.numel() > 0:\n",
    "        print(\"  proposals min:\", boxes_p.min().item(), \"max:\", boxes_p.max().item())\n",
    "    print(\"  any NaN in proposals? \", torch.isnan(boxes_p).any().item())\n",
    "    print(\"  any Inf in proposals?  \",\n",
    "          (torch.isinf(boxes_p) & (boxes_p > 0)).any().item())\n",
    "\n",
    "# ---- 3) Run student detection once and inspect loss_nat (source of L_gt_nat) ----\n",
    "student.eval()\n",
    "with torch.no_grad():\n",
    "    outputs_nat, loss_nat = student(\n",
    "        images_nat, targets=targets, proposals=proposals, view_name=\"nat\"\n",
    "    )\n",
    "\n",
    "print(\"\\n[DEBUG] loss_nat raw:\", loss_nat)\n",
    "for k, v in loss_nat.items():\n",
    "    if torch.is_tensor(v):\n",
    "        print(f\"[DEBUG] loss_nat[{k}]: {v.item():.6f}, finite={bool(torch.isfinite(v))}\")\n",
    "    else:\n",
    "        print(f\"[DEBUG] loss_nat[{k}]: non-tensor value -> {v}\")\n",
    "\n",
    "# Optional: also check teacher branch if you’re actually using it for GT loss\n",
    "if is_ts and teacher is not None:\n",
    "    teacher.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_nat_teacher, loss_nat_teacher = teacher(\n",
    "            images_nat, targets=targets, proposals=proposals, view_name=\"nat\"\n",
    "        )\n",
    "    print(\"\\n[DEBUG] teacher loss_nat raw:\", loss_nat_teacher)\n",
    "    for k, v in loss_nat_teacher.items():\n",
    "        if torch.is_tensor(v):\n",
    "            print(f\"[DEBUG] teacher loss_nat[{k}]: {v.item():.6f}, finite={bool(torch.isfinite(v))}\")\n",
    "        else:\n",
    "            print(f\"[DEBUG] teacher loss_nat[{k}]: non-tensor value -> {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db21facb-29f5-4953-b3bd-3ca214887d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUG v2: single-batch detection loss inspection (TRAIN mode) ===\n",
      "len(train_dataset) = 106458\n",
      "len(train_loader)  = 10646\n",
      "[DEBUG] images_nat.shape: torch.Size([10, 3, 800, 800])\n",
      "[DEBUG] batch size: 10\n",
      "\n",
      "[DEBUG] loss_nat keys: ['loss_cls', 'loss_bbox', 'loss_giou']\n",
      "  loss_nat[loss_cls] = 0.723318, finite=True\n",
      "  loss_nat[loss_bbox] = 0.187690, finite=True\n",
      "  loss_nat[loss_giou] = 0.827295, finite=True\n",
      "\n",
      "[DEBUG] running depth view forward for detection loss...\n",
      "  loss_depth[loss_cls] = 0.726135, finite=True\n",
      "  loss_depth[loss_bbox] = 0.196456, finite=True\n",
      "  loss_depth[loss_giou] = 0.842603, finite=True\n",
      "\n",
      "[DEBUG] running style view forward for detection loss...\n",
      "  loss_style[loss_cls] = 0.726785, finite=True\n",
      "  loss_style[loss_bbox] = 0.187293, finite=True\n",
      "  loss_style[loss_giou] = 0.829546, finite=True\n"
     ]
    }
   ],
   "source": [
    "# === DEBUG v2: inspect detection losses in TRAIN mode (single batch) ===\n",
    "\n",
    "import math\n",
    "\n",
    "# Make sure we use the student network (if teacher–student is wrapped)\n",
    "is_ts = hasattr(model, \"student\") and hasattr(model, \"teacher\")\n",
    "student = model.student if is_ts else model\n",
    "\n",
    "print(\"=== DEBUG v2: single-batch detection loss inspection (TRAIN mode) ===\")\n",
    "print(f\"len(train_dataset) = {len(train_dataset)}\")\n",
    "print(f\"len(train_loader)  = {len(train_loader)}\")\n",
    "\n",
    "# Switch student to train mode so `compute_losses` actually runs\n",
    "student.train()\n",
    "\n",
    "# Get one batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "images_nat   = batch[\"image_nat\"].to(device, non_blocking=True)\n",
    "images_depth = _normalize_view_batch(batch[\"image_depth\"], device)\n",
    "images_style = _normalize_view_batch(batch[\"image_style\"], device)\n",
    "targets      = batch[\"targets\"]\n",
    "proposals    = batch[\"proposals\"]\n",
    "\n",
    "print(\"[DEBUG] images_nat.shape:\", images_nat.shape)\n",
    "print(\"[DEBUG] batch size:\", len(targets))\n",
    "\n",
    "# ---- 1) One forward pass on nat view, with GT + proposals ----\n",
    "# No need for autocast/GradScaler here; we just want the raw losses.\n",
    "outputs_nat, loss_nat = student(\n",
    "    images_nat, targets=targets, proposals=proposals, view_name=\"nat\"\n",
    ")\n",
    "\n",
    "print(\"\\n[DEBUG] loss_nat keys:\", list(loss_nat.keys()))\n",
    "for k, v in loss_nat.items():\n",
    "    if torch.is_tensor(v):\n",
    "        print(f\"  loss_nat[{k}] = {v.item():.6f}, finite={bool(torch.isfinite(v))}\")\n",
    "    else:\n",
    "        print(f\"  loss_nat[{k}] = {v} (non-tensor)\")\n",
    "\n",
    "# ---- 2) If depth/style are present, you can optionally inspect them too ----\n",
    "if images_depth is not None:\n",
    "    print(\"\\n[DEBUG] running depth view forward for detection loss...\")\n",
    "    outputs_depth, loss_depth = student(\n",
    "        images_depth, targets=targets, proposals=proposals, view_name=\"depth\"\n",
    "    )\n",
    "    for k, v in loss_depth.items():\n",
    "        if torch.is_tensor(v):\n",
    "            print(f\"  loss_depth[{k}] = {v.item():.6f}, finite={bool(torch.isfinite(v))}\")\n",
    "        else:\n",
    "            print(f\"  loss_depth[{k}] = {v} (non-tensor)\")\n",
    "\n",
    "if images_style is not None:\n",
    "    print(\"\\n[DEBUG] running style view forward for detection loss...\")\n",
    "    outputs_style, loss_style = student(\n",
    "        images_style, targets=targets, proposals=proposals, view_name=\"style\"\n",
    "    )\n",
    "    for k, v in loss_style.items():\n",
    "        if torch.is_tensor(v):\n",
    "            print(f\"  loss_style[{k}] = {v.item():.6f}, finite={bool(torch.isfinite(v))}\")\n",
    "        else:\n",
    "            print(f\"  loss_style[{k}] = {v} (non-tensor)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70512d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Starting training.\n",
      "[DEBUG] len(train_dataset) = 106458\n",
      "[DEBUG] len(train_loader)  = 7098\n",
      "[DEBUG] TRAIN_BATCH_SIZE = 15, TRAIN_NUM_WORKERS = 0\n",
      "\n",
      "[Epoch 1] -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pshl7\\AppData\\Local\\Temp\\ipykernel_25484\\1296381611.py:50: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | step 50/7098] L_total=2.3412  L_gt=1.7174  L_obj=0.1247  L_sim=0.4991\n",
      "[Epoch 1 | step 100/7098] L_total=1.8388  L_gt=1.2556  L_obj=0.1039  L_sim=0.4793\n",
      "[Epoch 1 | step 150/7098] L_total=2.0708  L_gt=1.4860  L_obj=0.1166  L_sim=0.4682\n",
      "[Epoch 1 | step 200/7098] L_total=1.9727  L_gt=1.3801  L_obj=0.1204  L_sim=0.4722\n",
      "[Epoch 1 | step 250/7098] L_total=1.8291  L_gt=1.2487  L_obj=0.1156  L_sim=0.4648\n",
      "[Epoch 1 | step 300/7098] L_total=1.7003  L_gt=1.1425  L_obj=0.1066  L_sim=0.4512\n",
      "[Epoch 1 | step 350/7098] L_total=1.7608  L_gt=1.2183  L_obj=0.1173  L_sim=0.4252\n",
      "[Epoch 1 | step 400/7098] L_total=1.7618  L_gt=1.2268  L_obj=0.1121  L_sim=0.4229\n",
      "[Epoch 1 | step 450/7098] L_total=1.9242  L_gt=1.3817  L_obj=0.1134  L_sim=0.4291\n",
      "[Epoch 1 | step 500/7098] L_total=1.4072  L_gt=0.9233  L_obj=0.1055  L_sim=0.3784\n",
      "[Epoch 1 | step 550/7098] L_total=1.5720  L_gt=1.0906  L_obj=0.1041  L_sim=0.3774\n",
      "[Epoch 1 | step 600/7098] L_total=1.6215  L_gt=1.1309  L_obj=0.1063  L_sim=0.3843\n",
      "[Epoch 1 | step 650/7098] L_total=1.6504  L_gt=1.1728  L_obj=0.1053  L_sim=0.3723\n",
      "[Epoch 1 | step 700/7098] L_total=2.0167  L_gt=1.5439  L_obj=0.0949  L_sim=0.3779\n",
      "[Epoch 1 | step 750/7098] L_total=1.5132  L_gt=1.0696  L_obj=0.0977  L_sim=0.3459\n",
      "[Epoch 1 | step 800/7098] L_total=1.6371  L_gt=1.2050  L_obj=0.0922  L_sim=0.3399\n",
      "[Epoch 1 | step 850/7098] L_total=1.6904  L_gt=1.2665  L_obj=0.0876  L_sim=0.3363\n",
      "[Epoch 1 | step 900/7098] L_total=1.6607  L_gt=1.2145  L_obj=0.0902  L_sim=0.3560\n",
      "[Epoch 1 | step 950/7098] L_total=1.8969  L_gt=1.4932  L_obj=0.0906  L_sim=0.3132\n",
      "[Epoch 1 | step 1000/7098] L_total=1.8599  L_gt=1.4481  L_obj=0.0826  L_sim=0.3292\n",
      "[Epoch 1 | step 1050/7098] L_total=1.5748  L_gt=1.1889  L_obj=0.0809  L_sim=0.3050\n",
      "[Epoch 1 | step 1100/7098] L_total=1.5831  L_gt=1.1828  L_obj=0.0815  L_sim=0.3189\n",
      "[Epoch 1 | step 1150/7098] L_total=1.5225  L_gt=1.1664  L_obj=0.0789  L_sim=0.2772\n",
      "[Epoch 1 | step 1200/7098] L_total=1.5825  L_gt=1.2236  L_obj=0.0871  L_sim=0.2718\n",
      "[Epoch 1 | step 1250/7098] L_total=1.6508  L_gt=1.2787  L_obj=0.0736  L_sim=0.2985\n",
      "[Epoch 1 | step 1300/7098] L_total=1.3707  L_gt=1.0082  L_obj=0.0651  L_sim=0.2974\n",
      "[Epoch 1 | step 1350/7098] L_total=1.5290  L_gt=1.1674  L_obj=0.0737  L_sim=0.2880\n",
      "[Epoch 1 | step 1400/7098] L_total=1.3989  L_gt=1.0629  L_obj=0.0673  L_sim=0.2686\n",
      "[Epoch 1 | step 1450/7098] L_total=1.4803  L_gt=1.1573  L_obj=0.0707  L_sim=0.2523\n",
      "[Epoch 1 | step 1500/7098] L_total=1.3743  L_gt=1.0538  L_obj=0.0679  L_sim=0.2526\n",
      "[Epoch 1 | step 1550/7098] L_total=1.6722  L_gt=1.3243  L_obj=0.0724  L_sim=0.2755\n",
      "[Epoch 1 | step 1600/7098] L_total=1.4227  L_gt=1.1122  L_obj=0.0610  L_sim=0.2494\n",
      "[Epoch 1 | step 1650/7098] L_total=1.4588  L_gt=1.1262  L_obj=0.0590  L_sim=0.2737\n",
      "[Epoch 1 | step 1700/7098] L_total=1.3810  L_gt=1.0989  L_obj=0.0608  L_sim=0.2213\n",
      "[Epoch 1 | step 1750/7098] L_total=1.3667  L_gt=1.0831  L_obj=0.0569  L_sim=0.2267\n",
      "[Epoch 1 | step 1800/7098] L_total=1.4640  L_gt=1.1443  L_obj=0.0597  L_sim=0.2601\n",
      "[Epoch 1 | step 1850/7098] L_total=1.5490  L_gt=1.2624  L_obj=0.0622  L_sim=0.2243\n",
      "[Epoch 1 | step 1900/7098] L_total=1.5597  L_gt=1.2518  L_obj=0.0534  L_sim=0.2545\n",
      "[Epoch 1 | step 1950/7098] L_total=1.4379  L_gt=1.1382  L_obj=0.0560  L_sim=0.2437\n",
      "[Epoch 1 | step 2000/7098] L_total=1.5846  L_gt=1.3213  L_obj=0.0523  L_sim=0.2110\n",
      "[Epoch 1 | step 2050/7098] L_total=1.4887  L_gt=1.2017  L_obj=0.0553  L_sim=0.2317\n",
      "[Epoch 1 | step 2100/7098] L_total=1.2794  L_gt=0.9885  L_obj=0.0539  L_sim=0.2370\n",
      "[Epoch 1 | step 2150/7098] L_total=1.4061  L_gt=1.1186  L_obj=0.0561  L_sim=0.2315\n",
      "[Epoch 1 | step 2200/7098] L_total=1.3442  L_gt=1.0868  L_obj=0.0566  L_sim=0.2007\n",
      "[Epoch 1 | step 2250/7098] L_total=1.3176  L_gt=1.0417  L_obj=0.0514  L_sim=0.2245\n",
      "[Epoch 1 | step 2300/7098] L_total=1.7690  L_gt=1.4790  L_obj=0.0578  L_sim=0.2322\n",
      "[Epoch 1 | step 2350/7098] L_total=1.3847  L_gt=1.1348  L_obj=0.0523  L_sim=0.1977\n",
      "[Epoch 1 | step 2400/7098] L_total=1.6779  L_gt=1.4319  L_obj=0.0500  L_sim=0.1960\n",
      "[Epoch 1 | step 2450/7098] L_total=1.5131  L_gt=1.2354  L_obj=0.0520  L_sim=0.2257\n",
      "[Epoch 1 | step 2500/7098] L_total=1.4900  L_gt=1.2216  L_obj=0.0468  L_sim=0.2216\n",
      "[Epoch 1 | step 2550/7098] L_total=1.4672  L_gt=1.2111  L_obj=0.0446  L_sim=0.2116\n",
      "[Epoch 1 | step 2600/7098] L_total=1.2529  L_gt=0.9955  L_obj=0.0458  L_sim=0.2116\n",
      "[Epoch 1 | step 2650/7098] L_total=1.5777  L_gt=1.3029  L_obj=0.0471  L_sim=0.2277\n",
      "[Epoch 1 | step 2700/7098] L_total=1.3417  L_gt=1.0996  L_obj=0.0461  L_sim=0.1960\n",
      "[Epoch 1 | step 2750/7098] L_total=1.6962  L_gt=1.4551  L_obj=0.0514  L_sim=0.1897\n",
      "[Epoch 1 | step 2800/7098] L_total=1.3197  L_gt=1.0713  L_obj=0.0466  L_sim=0.2017\n",
      "[Epoch 1 | step 2850/7098] L_total=1.4868  L_gt=1.2322  L_obj=0.0524  L_sim=0.2022\n",
      "[Epoch 1 | step 2900/7098] L_total=1.4376  L_gt=1.1875  L_obj=0.0494  L_sim=0.2007\n",
      "[Epoch 1 | step 2950/7098] L_total=1.3276  L_gt=1.0833  L_obj=0.0416  L_sim=0.2028\n",
      "[Epoch 1 | step 3000/7098] L_total=1.3877  L_gt=1.1642  L_obj=0.0515  L_sim=0.1719\n",
      "[Epoch 1 | step 3050/7098] L_total=1.5632  L_gt=1.3421  L_obj=0.0511  L_sim=0.1700\n",
      "[Epoch 1 | step 3100/7098] L_total=1.4595  L_gt=1.2261  L_obj=0.0396  L_sim=0.1937\n",
      "[Epoch 1 | step 3150/7098] L_total=1.3227  L_gt=1.1155  L_obj=0.0466  L_sim=0.1606\n",
      "[Epoch 1 | step 3200/7098] L_total=1.2313  L_gt=1.0000  L_obj=0.0452  L_sim=0.1862\n",
      "[Epoch 1 | step 3250/7098] L_total=1.2418  L_gt=1.0158  L_obj=0.0481  L_sim=0.1780\n",
      "[Epoch 1 | step 3300/7098] L_total=1.4696  L_gt=1.2484  L_obj=0.0511  L_sim=0.1701\n",
      "[Epoch 1 | step 3350/7098] L_total=1.3154  L_gt=1.0668  L_obj=0.0542  L_sim=0.1944\n",
      "[Epoch 1 | step 3400/7098] L_total=1.7248  L_gt=1.5130  L_obj=0.0443  L_sim=0.1676\n",
      "[Epoch 1 | step 3450/7098] L_total=1.3301  L_gt=1.1064  L_obj=0.0433  L_sim=0.1805\n",
      "[Epoch 1 | step 3500/7098] L_total=1.2269  L_gt=1.0080  L_obj=0.0415  L_sim=0.1775\n",
      "[Epoch 1 | step 3550/7098] L_total=1.4351  L_gt=1.2305  L_obj=0.0468  L_sim=0.1579\n",
      "[Epoch 1 | step 3600/7098] L_total=1.4465  L_gt=1.2049  L_obj=0.0447  L_sim=0.1969\n",
      "[Epoch 1 | step 3650/7098] L_total=1.1156  L_gt=0.8837  L_obj=0.0436  L_sim=0.1882\n",
      "[Epoch 1 | step 3700/7098] L_total=1.3963  L_gt=1.1792  L_obj=0.0516  L_sim=0.1655\n",
      "[Epoch 1 | step 3750/7098] L_total=1.2659  L_gt=1.0666  L_obj=0.0423  L_sim=0.1570\n",
      "[Epoch 1 | step 3800/7098] L_total=1.2844  L_gt=1.0579  L_obj=0.0523  L_sim=0.1742\n",
      "[Epoch 1 | step 3850/7098] L_total=1.2244  L_gt=1.0104  L_obj=0.0498  L_sim=0.1642\n",
      "[Epoch 1 | step 3900/7098] L_total=1.4903  L_gt=1.2821  L_obj=0.0473  L_sim=0.1609\n",
      "[Epoch 1 | step 3950/7098] L_total=1.0371  L_gt=0.8349  L_obj=0.0443  L_sim=0.1578\n",
      "[Epoch 1 | step 4000/7098] L_total=1.5412  L_gt=1.3352  L_obj=0.0462  L_sim=0.1598\n",
      "[Epoch 1 | step 4050/7098] L_total=1.2279  L_gt=1.0415  L_obj=0.0462  L_sim=0.1402\n",
      "[Epoch 1 | step 4100/7098] L_total=1.2312  L_gt=1.0424  L_obj=0.0484  L_sim=0.1404\n",
      "[Epoch 1 | step 4150/7098] L_total=1.0299  L_gt=0.8440  L_obj=0.0504  L_sim=0.1354\n",
      "[Epoch 1 | step 4200/7098] L_total=1.1934  L_gt=0.9884  L_obj=0.0486  L_sim=0.1564\n",
      "[Epoch 1 | step 4250/7098] L_total=1.1196  L_gt=0.9163  L_obj=0.0437  L_sim=0.1595\n",
      "[Epoch 1 | step 4300/7098] L_total=1.5796  L_gt=1.3788  L_obj=0.0503  L_sim=0.1505\n",
      "[Epoch 1 | step 4350/7098] L_total=1.3974  L_gt=1.1718  L_obj=0.0473  L_sim=0.1782\n",
      "[Epoch 1 | step 4400/7098] L_total=1.4713  L_gt=1.2806  L_obj=0.0454  L_sim=0.1452\n",
      "[Epoch 1 | step 4450/7098] L_total=1.5139  L_gt=1.3150  L_obj=0.0496  L_sim=0.1494\n",
      "[Epoch 1 | step 4500/7098] L_total=1.3550  L_gt=1.1581  L_obj=0.0442  L_sim=0.1527\n",
      "[Epoch 1 | step 4550/7098] L_total=1.2375  L_gt=1.0569  L_obj=0.0455  L_sim=0.1351\n",
      "[Epoch 1 | step 4600/7098] L_total=1.2650  L_gt=1.0617  L_obj=0.0429  L_sim=0.1604\n",
      "[Epoch 1 | step 4650/7098] L_total=0.9759  L_gt=0.7684  L_obj=0.0381  L_sim=0.1694\n",
      "[Epoch 1 | step 4700/7098] L_total=1.4956  L_gt=1.2775  L_obj=0.0452  L_sim=0.1729\n",
      "[Epoch 1 | step 4750/7098] L_total=1.0905  L_gt=0.8856  L_obj=0.0443  L_sim=0.1605\n",
      "[Epoch 1 | step 4800/7098] L_total=1.5271  L_gt=1.3398  L_obj=0.0468  L_sim=0.1406\n",
      "[Epoch 1 | step 4850/7098] L_total=1.3791  L_gt=1.1872  L_obj=0.0435  L_sim=0.1484\n",
      "[Epoch 1 | step 4900/7098] L_total=1.2131  L_gt=1.0466  L_obj=0.0399  L_sim=0.1265\n",
      "[Epoch 1 | step 4950/7098] L_total=1.1875  L_gt=0.9821  L_obj=0.0458  L_sim=0.1595\n",
      "[Epoch 1 | step 5000/7098] L_total=1.3886  L_gt=1.2063  L_obj=0.0428  L_sim=0.1394\n",
      "[Epoch 1 | step 5050/7098] L_total=1.1088  L_gt=0.9309  L_obj=0.0414  L_sim=0.1364\n",
      "[Epoch 1 | step 5100/7098] L_total=1.2533  L_gt=1.0664  L_obj=0.0438  L_sim=0.1431\n",
      "[Epoch 1 | step 5150/7098] L_total=1.2903  L_gt=1.1204  L_obj=0.0379  L_sim=0.1320\n",
      "[Epoch 1 | step 5200/7098] L_total=1.2191  L_gt=1.0113  L_obj=0.0425  L_sim=0.1654\n",
      "[Epoch 1 | step 5250/7098] L_total=1.3740  L_gt=1.1984  L_obj=0.0419  L_sim=0.1337\n",
      "[Epoch 1 | step 5300/7098] L_total=1.2946  L_gt=1.1160  L_obj=0.0485  L_sim=0.1301\n",
      "[Epoch 1 | step 5350/7098] L_total=1.2872  L_gt=1.0926  L_obj=0.0504  L_sim=0.1442\n",
      "[Epoch 1 | step 5400/7098] L_total=1.3628  L_gt=1.1642  L_obj=0.0470  L_sim=0.1516\n",
      "[Epoch 1 | step 5450/7098] L_total=1.2447  L_gt=1.0626  L_obj=0.0437  L_sim=0.1384\n",
      "[Epoch 1 | step 5500/7098] L_total=1.0859  L_gt=0.8996  L_obj=0.0461  L_sim=0.1403\n",
      "[Epoch 1 | step 5550/7098] L_total=1.0876  L_gt=0.8978  L_obj=0.0463  L_sim=0.1435\n",
      "[Epoch 1 | step 5600/7098] L_total=1.4269  L_gt=1.2501  L_obj=0.0446  L_sim=0.1322\n",
      "[Epoch 1 | step 5650/7098] L_total=1.4257  L_gt=1.2446  L_obj=0.0420  L_sim=0.1391\n",
      "[Epoch 1 | step 5700/7098] L_total=1.1274  L_gt=0.9447  L_obj=0.0422  L_sim=0.1405\n",
      "[Epoch 1 | step 5750/7098] L_total=1.4541  L_gt=1.2676  L_obj=0.0408  L_sim=0.1458\n",
      "[Epoch 1 | step 5800/7098] L_total=1.3691  L_gt=1.1858  L_obj=0.0430  L_sim=0.1402\n",
      "[Epoch 1 | step 5850/7098] L_total=1.1594  L_gt=0.9703  L_obj=0.0428  L_sim=0.1462\n",
      "[Epoch 1 | step 5900/7098] L_total=1.4666  L_gt=1.2831  L_obj=0.0426  L_sim=0.1409\n",
      "[Epoch 1 | step 5950/7098] L_total=1.3591  L_gt=1.1925  L_obj=0.0414  L_sim=0.1251\n",
      "[Epoch 1 | step 6000/7098] L_total=1.2437  L_gt=1.0817  L_obj=0.0457  L_sim=0.1163\n",
      "[Epoch 1 | step 6050/7098] L_total=1.4038  L_gt=1.2115  L_obj=0.0415  L_sim=0.1508\n",
      "[Epoch 1 | step 6100/7098] L_total=1.4897  L_gt=1.3098  L_obj=0.0461  L_sim=0.1338\n",
      "[Epoch 1 | step 6150/7098] L_total=1.0780  L_gt=0.9120  L_obj=0.0422  L_sim=0.1239\n",
      "[Epoch 1 | step 6200/7098] L_total=1.3024  L_gt=1.1385  L_obj=0.0404  L_sim=0.1235\n",
      "[Epoch 1 | step 6250/7098] L_total=1.2182  L_gt=1.0556  L_obj=0.0420  L_sim=0.1206\n",
      "[Epoch 1 | step 6300/7098] L_total=1.1577  L_gt=0.9850  L_obj=0.0464  L_sim=0.1263\n",
      "[Epoch 1 | step 6350/7098] L_total=1.3727  L_gt=1.1820  L_obj=0.0411  L_sim=0.1496\n",
      "[Epoch 1 | step 6400/7098] L_total=1.3464  L_gt=1.1540  L_obj=0.0476  L_sim=0.1448\n",
      "[Epoch 1 | step 6450/7098] L_total=1.1853  L_gt=1.0089  L_obj=0.0433  L_sim=0.1331\n",
      "[Epoch 1 | step 6500/7098] L_total=1.3522  L_gt=1.1660  L_obj=0.0410  L_sim=0.1452\n",
      "[Epoch 1 | step 6550/7098] L_total=1.0975  L_gt=0.9305  L_obj=0.0432  L_sim=0.1237\n",
      "[Epoch 1 | step 6600/7098] L_total=1.3181  L_gt=1.1566  L_obj=0.0434  L_sim=0.1181\n",
      "[Epoch 1 | step 6650/7098] L_total=1.3095  L_gt=1.1275  L_obj=0.0421  L_sim=0.1399\n",
      "[Epoch 1 | step 6700/7098] L_total=1.3821  L_gt=1.1891  L_obj=0.0429  L_sim=0.1501\n",
      "[Epoch 1 | step 6750/7098] L_total=1.3243  L_gt=1.1492  L_obj=0.0446  L_sim=0.1305\n",
      "[Epoch 1 | step 6800/7098] L_total=1.3994  L_gt=1.2283  L_obj=0.0414  L_sim=0.1298\n",
      "[Epoch 1 | step 6850/7098] L_total=1.1963  L_gt=1.0371  L_obj=0.0408  L_sim=0.1184\n",
      "[Epoch 1 | step 6900/7098] L_total=1.2673  L_gt=1.0875  L_obj=0.0426  L_sim=0.1373\n",
      "[Epoch 1 | step 6950/7098] L_total=0.9502  L_gt=0.7932  L_obj=0.0469  L_sim=0.1101\n",
      "[Epoch 1 | step 7000/7098] L_total=1.2193  L_gt=1.0519  L_obj=0.0411  L_sim=0.1263\n",
      "[Epoch 1 | step 7050/7098] L_total=1.4296  L_gt=1.2503  L_obj=0.0422  L_sim=0.1371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pshl7\\AppData\\Local\\Temp\\ipykernel_25484\\1469034792.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] TRAIN: L_total=1.4314  L_gt=1.1637  L_obj=0.0572  L_sim=0.2105\n",
      "[Epoch 1] VAL  : L_total=0.1572  L_gt=0.0000  L_obj=0.0416  L_sim=0.1156\n",
      "[Epoch 1] New best val L_total=0.1572\n",
      "  -> Saved checkpoint: vclr_convnext_teacher_student_best.pth\n",
      "\n",
      "[Epoch 2] -----------------------------\n",
      "[Epoch 2 | step 50/7098] L_total=1.5085  L_gt=1.2859  L_obj=0.1097  L_sim=0.1129\n",
      "[Epoch 2 | step 100/7098] L_total=1.4478  L_gt=1.1831  L_obj=0.1458  L_sim=0.1189\n",
      "[Epoch 2 | step 150/7098] L_total=1.4446  L_gt=1.2108  L_obj=0.1233  L_sim=0.1104\n",
      "[Epoch 2 | step 200/7098] L_total=1.3065  L_gt=1.0652  L_obj=0.1380  L_sim=0.1033\n",
      "[Epoch 2 | step 250/7098] L_total=1.1765  L_gt=0.9513  L_obj=0.1139  L_sim=0.1113\n",
      "[Epoch 2 | step 300/7098] L_total=1.2283  L_gt=0.9636  L_obj=0.1547  L_sim=0.1100\n",
      "[Epoch 2 | step 350/7098] L_total=1.3042  L_gt=1.0576  L_obj=0.1331  L_sim=0.1135\n",
      "[Epoch 2 | step 400/7098] L_total=1.1759  L_gt=0.9506  L_obj=0.1232  L_sim=0.1021\n",
      "[Epoch 2 | step 450/7098] L_total=1.3706  L_gt=1.1516  L_obj=0.1066  L_sim=0.1124\n",
      "[Epoch 2 | step 500/7098] L_total=1.3905  L_gt=1.1440  L_obj=0.1246  L_sim=0.1219\n",
      "[Epoch 2 | step 550/7098] L_total=1.1076  L_gt=0.8792  L_obj=0.1239  L_sim=0.1044\n",
      "[Epoch 2 | step 600/7098] L_total=1.5311  L_gt=1.3209  L_obj=0.1132  L_sim=0.0971\n",
      "[Epoch 2 | step 650/7098] L_total=1.4043  L_gt=1.1642  L_obj=0.1210  L_sim=0.1191\n",
      "[Epoch 2 | step 700/7098] L_total=1.3735  L_gt=1.1439  L_obj=0.1315  L_sim=0.0981\n",
      "[Epoch 2 | step 750/7098] L_total=1.2420  L_gt=1.0174  L_obj=0.1140  L_sim=0.1106\n",
      "[Epoch 2 | step 800/7098] L_total=1.6562  L_gt=1.4157  L_obj=0.1579  L_sim=0.0826\n",
      "[Epoch 2 | step 850/7098] L_total=1.1375  L_gt=0.8893  L_obj=0.1470  L_sim=0.1012\n",
      "[Epoch 2 | step 900/7098] L_total=1.1836  L_gt=0.9277  L_obj=0.1448  L_sim=0.1112\n",
      "[Epoch 2 | step 950/7098] L_total=1.4953  L_gt=1.2728  L_obj=0.1182  L_sim=0.1043\n",
      "[Epoch 2 | step 1000/7098] L_total=1.4200  L_gt=1.2087  L_obj=0.0989  L_sim=0.1124\n",
      "[Epoch 2 | step 1050/7098] L_total=1.6801  L_gt=1.4452  L_obj=0.1231  L_sim=0.1117\n",
      "[Epoch 2 | step 1100/7098] L_total=1.1762  L_gt=0.9337  L_obj=0.1463  L_sim=0.0962\n",
      "[Epoch 2 | step 1150/7098] L_total=1.0934  L_gt=0.8795  L_obj=0.1013  L_sim=0.1126\n",
      "[Epoch 2 | step 1200/7098] L_total=1.3188  L_gt=1.1035  L_obj=0.1213  L_sim=0.0940\n",
      "[Epoch 2 | step 1250/7098] L_total=1.1363  L_gt=0.9234  L_obj=0.1211  L_sim=0.0918\n",
      "[Epoch 2 | step 1300/7098] L_total=1.2796  L_gt=1.0408  L_obj=0.1257  L_sim=0.1130\n",
      "[Epoch 2 | step 1350/7098] L_total=1.2633  L_gt=1.0502  L_obj=0.1095  L_sim=0.1036\n",
      "[Epoch 2 | step 1400/7098] L_total=1.3510  L_gt=1.1177  L_obj=0.1208  L_sim=0.1125\n",
      "[Epoch 2 | step 1450/7098] L_total=1.2814  L_gt=1.0634  L_obj=0.1358  L_sim=0.0822\n",
      "[Epoch 2 | step 1500/7098] L_total=1.2150  L_gt=0.9927  L_obj=0.1316  L_sim=0.0907\n",
      "[Epoch 2 | step 1550/7098] L_total=1.3531  L_gt=1.1194  L_obj=0.1359  L_sim=0.0978\n",
      "[Epoch 2 | step 1600/7098] L_total=1.4006  L_gt=1.1501  L_obj=0.1377  L_sim=0.1129\n",
      "[Epoch 2 | step 1650/7098] L_total=1.2714  L_gt=1.0256  L_obj=0.1508  L_sim=0.0951\n",
      "[Epoch 2 | step 1700/7098] L_total=1.0919  L_gt=0.8601  L_obj=0.1291  L_sim=0.1028\n",
      "[Epoch 2 | step 1750/7098] L_total=1.0537  L_gt=0.8457  L_obj=0.1253  L_sim=0.0827\n",
      "[Epoch 2 | step 1800/7098] L_total=1.2901  L_gt=1.0518  L_obj=0.1443  L_sim=0.0941\n",
      "[Epoch 2 | step 1850/7098] L_total=1.1991  L_gt=0.9516  L_obj=0.1426  L_sim=0.1049\n",
      "[Epoch 2 | step 1900/7098] L_total=1.3578  L_gt=1.1242  L_obj=0.1399  L_sim=0.0938\n",
      "[Epoch 2 | step 1950/7098] L_total=1.3627  L_gt=1.1343  L_obj=0.1338  L_sim=0.0946\n",
      "[Epoch 2 | step 2000/7098] L_total=1.1352  L_gt=0.8999  L_obj=0.1313  L_sim=0.1041\n",
      "[Epoch 2 | step 2050/7098] L_total=1.1558  L_gt=0.9317  L_obj=0.1318  L_sim=0.0923\n",
      "[Epoch 2 | step 2100/7098] L_total=1.1666  L_gt=0.9388  L_obj=0.1207  L_sim=0.1071\n",
      "[Epoch 2 | step 2150/7098] L_total=1.0931  L_gt=0.8994  L_obj=0.1130  L_sim=0.0806\n",
      "[Epoch 2 | step 2200/7098] L_total=1.6104  L_gt=1.4000  L_obj=0.1120  L_sim=0.0984\n",
      "[Epoch 2 | step 2250/7098] L_total=1.3601  L_gt=1.1356  L_obj=0.1384  L_sim=0.0861\n",
      "[Epoch 2 | step 2300/7098] L_total=1.3177  L_gt=1.0738  L_obj=0.1595  L_sim=0.0844\n",
      "[Epoch 2 | step 2350/7098] L_total=1.2894  L_gt=1.0427  L_obj=0.1428  L_sim=0.1039\n",
      "[Epoch 2 | step 2400/7098] L_total=1.5700  L_gt=1.3704  L_obj=0.1174  L_sim=0.0822\n",
      "[Epoch 2 | step 2450/7098] L_total=1.1970  L_gt=0.9731  L_obj=0.1188  L_sim=0.1051\n",
      "[Epoch 2 | step 2500/7098] L_total=1.1454  L_gt=0.8981  L_obj=0.1405  L_sim=0.1069\n",
      "[Epoch 2 | step 2550/7098] L_total=1.3416  L_gt=1.1486  L_obj=0.1127  L_sim=0.0803\n",
      "[Epoch 2 | step 2600/7098] L_total=1.4728  L_gt=1.2747  L_obj=0.0972  L_sim=0.1010\n",
      "[Epoch 2 | step 2650/7098] L_total=0.8071  L_gt=0.5760  L_obj=0.1502  L_sim=0.0809\n",
      "[Epoch 2 | step 2700/7098] L_total=1.3280  L_gt=1.1203  L_obj=0.1238  L_sim=0.0839\n",
      "[Epoch 2 | step 2750/7098] L_total=1.2205  L_gt=1.0003  L_obj=0.1296  L_sim=0.0906\n",
      "[Epoch 2 | step 2800/7098] L_total=1.2742  L_gt=1.0529  L_obj=0.1146  L_sim=0.1066\n",
      "[Epoch 2 | step 2850/7098] L_total=1.2538  L_gt=1.0122  L_obj=0.1437  L_sim=0.0980\n",
      "[Epoch 2 | step 2900/7098] L_total=1.2820  L_gt=1.0521  L_obj=0.1281  L_sim=0.1019\n",
      "[Epoch 2 | step 2950/7098] L_total=1.0057  L_gt=0.7811  L_obj=0.1464  L_sim=0.0782\n",
      "[Epoch 2 | step 3000/7098] L_total=1.6528  L_gt=1.4350  L_obj=0.1115  L_sim=0.1064\n",
      "[Epoch 2 | step 3050/7098] L_total=1.1805  L_gt=0.9546  L_obj=0.1225  L_sim=0.1034\n",
      "[Epoch 2 | step 3100/7098] L_total=1.2545  L_gt=1.0405  L_obj=0.1234  L_sim=0.0905\n",
      "[Epoch 2 | step 3150/7098] L_total=1.2532  L_gt=1.0366  L_obj=0.1132  L_sim=0.1034\n",
      "[Epoch 2 | step 3200/7098] L_total=1.0482  L_gt=0.8149  L_obj=0.1405  L_sim=0.0928\n",
      "[Epoch 2 | step 3250/7098] L_total=1.2835  L_gt=1.0747  L_obj=0.1192  L_sim=0.0897\n",
      "[Epoch 2 | step 3300/7098] L_total=1.3225  L_gt=1.1352  L_obj=0.1079  L_sim=0.0794\n",
      "[Epoch 2 | step 3350/7098] L_total=1.0835  L_gt=0.8536  L_obj=0.1351  L_sim=0.0947\n",
      "[Epoch 2 | step 3400/7098] L_total=1.2637  L_gt=1.0619  L_obj=0.1082  L_sim=0.0936\n",
      "[Epoch 2 | step 3450/7098] L_total=1.0648  L_gt=0.8514  L_obj=0.1336  L_sim=0.0799\n",
      "[Epoch 2 | step 3500/7098] L_total=1.4944  L_gt=1.2715  L_obj=0.1229  L_sim=0.1000\n",
      "[Epoch 2 | step 3550/7098] L_total=1.1054  L_gt=0.9009  L_obj=0.1340  L_sim=0.0705\n",
      "[Epoch 2 | step 3600/7098] L_total=1.2360  L_gt=0.9924  L_obj=0.1392  L_sim=0.1043\n",
      "[Epoch 2 | step 3650/7098] L_total=1.3341  L_gt=1.1208  L_obj=0.1148  L_sim=0.0985\n",
      "[Epoch 2 | step 3700/7098] L_total=1.2881  L_gt=1.0485  L_obj=0.1636  L_sim=0.0759\n",
      "[Epoch 2 | step 3750/7098] L_total=1.0397  L_gt=0.8049  L_obj=0.1715  L_sim=0.0632\n",
      "[Epoch 2 | step 3800/7098] L_total=1.3079  L_gt=1.0859  L_obj=0.1439  L_sim=0.0781\n",
      "[Epoch 2 | step 3850/7098] L_total=1.3565  L_gt=1.1511  L_obj=0.0996  L_sim=0.1058\n",
      "[Epoch 2 | step 3900/7098] L_total=1.4635  L_gt=1.2519  L_obj=0.1305  L_sim=0.0810\n",
      "[Epoch 2 | step 3950/7098] L_total=1.3790  L_gt=1.1716  L_obj=0.1119  L_sim=0.0955\n",
      "[Epoch 2 | step 4000/7098] L_total=0.9978  L_gt=0.7633  L_obj=0.1395  L_sim=0.0950\n",
      "[Epoch 2 | step 4050/7098] L_total=1.3801  L_gt=1.1939  L_obj=0.1158  L_sim=0.0704\n",
      "[Epoch 2 | step 4100/7098] L_total=1.1421  L_gt=0.9283  L_obj=0.1296  L_sim=0.0841\n",
      "[Epoch 2 | step 4150/7098] L_total=1.3690  L_gt=1.1395  L_obj=0.1378  L_sim=0.0918\n",
      "[Epoch 2 | step 4200/7098] L_total=0.9716  L_gt=0.7588  L_obj=0.1224  L_sim=0.0904\n",
      "[Epoch 2 | step 4250/7098] L_total=1.2311  L_gt=1.0074  L_obj=0.1383  L_sim=0.0854\n",
      "[Epoch 2 | step 4300/7098] L_total=1.1384  L_gt=0.9115  L_obj=0.1349  L_sim=0.0920\n",
      "[Epoch 2 | step 4350/7098] L_total=1.4209  L_gt=1.2080  L_obj=0.1270  L_sim=0.0859\n",
      "[Epoch 2 | step 4400/7098] L_total=1.3011  L_gt=1.0887  L_obj=0.1206  L_sim=0.0918\n",
      "[Epoch 2 | step 4450/7098] L_total=1.2352  L_gt=1.0238  L_obj=0.1294  L_sim=0.0820\n",
      "[Epoch 2 | step 4500/7098] L_total=1.6100  L_gt=1.4021  L_obj=0.1082  L_sim=0.0996\n",
      "[Epoch 2 | step 4550/7098] L_total=1.1619  L_gt=0.9606  L_obj=0.1185  L_sim=0.0828\n",
      "[Epoch 2 | step 4600/7098] L_total=1.2319  L_gt=1.0245  L_obj=0.1200  L_sim=0.0874\n",
      "[Epoch 2 | step 4650/7098] L_total=1.6270  L_gt=1.4207  L_obj=0.1238  L_sim=0.0825\n",
      "[Epoch 2 | step 4700/7098] L_total=1.1694  L_gt=0.9561  L_obj=0.1197  L_sim=0.0937\n",
      "[Epoch 2 | step 4750/7098] L_total=1.2935  L_gt=1.0644  L_obj=0.1401  L_sim=0.0889\n",
      "[Epoch 2 | step 4800/7098] L_total=1.1853  L_gt=0.9598  L_obj=0.1356  L_sim=0.0899\n",
      "[Epoch 2 | step 4850/7098] L_total=1.3051  L_gt=1.0973  L_obj=0.1379  L_sim=0.0699\n",
      "[Epoch 2 | step 4900/7098] L_total=1.5375  L_gt=1.3185  L_obj=0.1249  L_sim=0.0941\n",
      "[Epoch 2 | step 4950/7098] L_total=1.0091  L_gt=0.8150  L_obj=0.1252  L_sim=0.0690\n",
      "[Epoch 2 | step 5000/7098] L_total=1.5727  L_gt=1.3850  L_obj=0.0961  L_sim=0.0916\n",
      "[Epoch 2 | step 5050/7098] L_total=1.3488  L_gt=1.1392  L_obj=0.1291  L_sim=0.0806\n",
      "[Epoch 2 | step 5100/7098] L_total=1.2962  L_gt=1.0977  L_obj=0.1061  L_sim=0.0924\n",
      "[Epoch 2 | step 5150/7098] L_total=1.3256  L_gt=1.1138  L_obj=0.1187  L_sim=0.0932\n",
      "[Epoch 2 | step 5200/7098] L_total=1.3031  L_gt=1.0976  L_obj=0.1202  L_sim=0.0853\n",
      "[Epoch 2 | step 5250/7098] L_total=1.3985  L_gt=1.1961  L_obj=0.1265  L_sim=0.0759\n",
      "[Epoch 2 | step 5300/7098] L_total=1.3364  L_gt=1.1300  L_obj=0.1266  L_sim=0.0798\n",
      "[Epoch 2 | step 5350/7098] L_total=1.3853  L_gt=1.1816  L_obj=0.1203  L_sim=0.0834\n",
      "[Epoch 2 | step 5400/7098] L_total=0.9121  L_gt=0.7075  L_obj=0.1275  L_sim=0.0771\n",
      "[Epoch 2 | step 5450/7098] L_total=1.4227  L_gt=1.2208  L_obj=0.1242  L_sim=0.0777\n",
      "[Epoch 2 | step 5500/7098] L_total=1.0544  L_gt=0.8480  L_obj=0.1180  L_sim=0.0884\n",
      "[Epoch 2 | step 5550/7098] L_total=1.2523  L_gt=1.0383  L_obj=0.1279  L_sim=0.0860\n",
      "[Epoch 2 | step 5600/7098] L_total=1.4146  L_gt=1.2149  L_obj=0.1146  L_sim=0.0851\n",
      "[Epoch 2 | step 5650/7098] L_total=1.4603  L_gt=1.2260  L_obj=0.1286  L_sim=0.1057\n",
      "[Epoch 2 | step 5700/7098] L_total=1.1778  L_gt=0.9665  L_obj=0.1167  L_sim=0.0946\n",
      "[Epoch 2 | step 5750/7098] L_total=1.3491  L_gt=1.1598  L_obj=0.1019  L_sim=0.0875\n",
      "[Epoch 2 | step 5800/7098] L_total=1.1856  L_gt=0.9849  L_obj=0.1180  L_sim=0.0827\n",
      "[Epoch 2 | step 5850/7098] L_total=1.1159  L_gt=0.9182  L_obj=0.1410  L_sim=0.0568\n",
      "[Epoch 2 | step 5900/7098] L_total=1.2791  L_gt=1.0905  L_obj=0.1004  L_sim=0.0881\n",
      "[Epoch 2 | step 5950/7098] L_total=1.2160  L_gt=1.0042  L_obj=0.1281  L_sim=0.0837\n",
      "[Epoch 2 | step 6000/7098] L_total=1.2377  L_gt=1.0358  L_obj=0.1110  L_sim=0.0909\n",
      "[Epoch 2 | step 6050/7098] L_total=1.3775  L_gt=1.1684  L_obj=0.1195  L_sim=0.0896\n",
      "[Epoch 2 | step 6100/7098] L_total=1.1202  L_gt=0.9137  L_obj=0.1354  L_sim=0.0711\n",
      "[Epoch 2 | step 6150/7098] L_total=1.4446  L_gt=1.2450  L_obj=0.1189  L_sim=0.0808\n",
      "[Epoch 2 | step 6200/7098] L_total=1.3971  L_gt=1.1949  L_obj=0.1205  L_sim=0.0816\n",
      "[Epoch 2 | step 6250/7098] L_total=1.1879  L_gt=0.9917  L_obj=0.0955  L_sim=0.1006\n",
      "[Epoch 2 | step 6300/7098] L_total=1.2420  L_gt=1.0243  L_obj=0.1488  L_sim=0.0689\n",
      "[Epoch 2 | step 6350/7098] L_total=1.3145  L_gt=1.1086  L_obj=0.1318  L_sim=0.0741\n",
      "[Epoch 2 | step 6400/7098] L_total=1.2804  L_gt=1.0751  L_obj=0.1363  L_sim=0.0691\n",
      "[Epoch 2 | step 6450/7098] L_total=1.2716  L_gt=1.0527  L_obj=0.1332  L_sim=0.0857\n",
      "[Epoch 2 | step 6500/7098] L_total=1.5354  L_gt=1.3266  L_obj=0.1368  L_sim=0.0719\n",
      "[Epoch 2 | step 6550/7098] L_total=0.9566  L_gt=0.7389  L_obj=0.1375  L_sim=0.0802\n",
      "[Epoch 2 | step 6600/7098] L_total=0.8912  L_gt=0.6802  L_obj=0.1258  L_sim=0.0851\n",
      "[Epoch 2 | step 6650/7098] L_total=1.1502  L_gt=0.9665  L_obj=0.1116  L_sim=0.0721\n",
      "[Epoch 2 | step 6700/7098] L_total=1.2104  L_gt=1.0233  L_obj=0.1108  L_sim=0.0764\n",
      "[Epoch 2 | step 6750/7098] L_total=1.4334  L_gt=1.2350  L_obj=0.1067  L_sim=0.0918\n",
      "[Epoch 2 | step 6800/7098] L_total=1.2769  L_gt=1.0570  L_obj=0.1410  L_sim=0.0789\n",
      "[Epoch 2 | step 6850/7098] L_total=1.3462  L_gt=1.1530  L_obj=0.1310  L_sim=0.0622\n",
      "[Epoch 2 | step 6900/7098] L_total=1.2523  L_gt=1.0426  L_obj=0.1366  L_sim=0.0731\n",
      "[Epoch 2 | step 6950/7098] L_total=1.3151  L_gt=1.1551  L_obj=0.0838  L_sim=0.0763\n",
      "[Epoch 2 | step 7000/7098] L_total=1.3604  L_gt=1.1461  L_obj=0.1456  L_sim=0.0687\n",
      "[Epoch 2 | step 7050/7098] L_total=1.0987  L_gt=0.9079  L_obj=0.1189  L_sim=0.0719\n",
      "[Epoch 2] TRAIN: L_total=1.2721  L_gt=1.0535  L_obj=0.1264  L_sim=0.0922\n",
      "[Epoch 2] VAL  : L_total=0.2000  L_gt=0.0000  L_obj=0.1242  L_sim=0.0758\n",
      "\n",
      "[Epoch 3] -----------------------------\n",
      "[Epoch 3 | step 50/7098] L_total=1.0580  L_gt=0.7972  L_obj=0.1732  L_sim=0.0875\n",
      "[Epoch 3 | step 100/7098] L_total=1.1707  L_gt=0.9356  L_obj=0.1548  L_sim=0.0804\n",
      "[Epoch 3 | step 150/7098] L_total=1.5355  L_gt=1.2982  L_obj=0.1737  L_sim=0.0637\n",
      "[Epoch 3 | step 200/7098] L_total=1.3834  L_gt=1.1267  L_obj=0.1855  L_sim=0.0712\n",
      "[Epoch 3 | step 250/7098] L_total=1.4510  L_gt=1.1869  L_obj=0.1950  L_sim=0.0691\n",
      "[Epoch 3 | step 300/7098] L_total=1.2234  L_gt=0.9988  L_obj=0.1475  L_sim=0.0771\n",
      "[Epoch 3 | step 350/7098] L_total=1.3413  L_gt=1.0873  L_obj=0.1897  L_sim=0.0643\n",
      "[Epoch 3 | step 400/7098] L_total=1.2809  L_gt=0.9719  L_obj=0.2301  L_sim=0.0789\n",
      "[Epoch 3 | step 450/7098] L_total=1.0991  L_gt=0.8479  L_obj=0.1887  L_sim=0.0625\n",
      "[Epoch 3 | step 500/7098] L_total=1.1183  L_gt=0.8182  L_obj=0.2277  L_sim=0.0725\n",
      "[Epoch 3 | step 550/7098] L_total=1.5110  L_gt=1.2324  L_obj=0.1892  L_sim=0.0894\n",
      "[Epoch 3 | step 600/7098] L_total=1.2811  L_gt=1.0111  L_obj=0.1946  L_sim=0.0754\n",
      "[Epoch 3 | step 650/7098] L_total=1.4547  L_gt=1.2381  L_obj=0.1479  L_sim=0.0688\n",
      "[Epoch 3 | step 700/7098] L_total=1.1777  L_gt=0.9409  L_obj=0.1570  L_sim=0.0798\n",
      "[Epoch 3 | step 750/7098] L_total=1.5346  L_gt=1.3118  L_obj=0.1650  L_sim=0.0577\n",
      "[Epoch 3 | step 800/7098] L_total=1.3900  L_gt=1.1706  L_obj=0.1622  L_sim=0.0572\n",
      "[Epoch 3 | step 850/7098] L_total=1.5258  L_gt=1.2970  L_obj=0.1410  L_sim=0.0879\n",
      "[Epoch 3 | step 900/7098] L_total=1.4109  L_gt=1.1236  L_obj=0.2301  L_sim=0.0573\n",
      "[Epoch 3 | step 950/7098] L_total=1.3503  L_gt=1.1122  L_obj=0.1706  L_sim=0.0675\n",
      "[Epoch 3 | step 1000/7098] L_total=1.1871  L_gt=0.8967  L_obj=0.2472  L_sim=0.0431\n",
      "[Epoch 3 | step 1050/7098] L_total=1.0548  L_gt=0.8444  L_obj=0.1374  L_sim=0.0731\n",
      "[Epoch 3 | step 1100/7098] L_total=1.0639  L_gt=0.8123  L_obj=0.1927  L_sim=0.0589\n",
      "[Epoch 3 | step 1150/7098] L_total=1.4372  L_gt=1.1691  L_obj=0.2232  L_sim=0.0448\n",
      "[Epoch 3 | step 1200/7098] L_total=1.1189  L_gt=0.8452  L_obj=0.2060  L_sim=0.0677\n",
      "[Epoch 3 | step 1250/7098] L_total=1.3098  L_gt=1.0592  L_obj=0.2037  L_sim=0.0470\n",
      "[Epoch 3 | step 1300/7098] L_total=1.2671  L_gt=1.0229  L_obj=0.1787  L_sim=0.0655\n",
      "[Epoch 3 | step 1350/7098] L_total=1.4039  L_gt=1.1437  L_obj=0.1969  L_sim=0.0632\n",
      "[Epoch 3 | step 1400/7098] L_total=1.1564  L_gt=0.8869  L_obj=0.1961  L_sim=0.0735\n",
      "[Epoch 3 | step 1450/7098] L_total=1.0787  L_gt=0.7851  L_obj=0.2388  L_sim=0.0548\n",
      "[Epoch 3 | step 1500/7098] L_total=1.2767  L_gt=0.9826  L_obj=0.2374  L_sim=0.0567\n",
      "[Epoch 3 | step 1550/7098] L_total=1.3571  L_gt=1.0912  L_obj=0.2055  L_sim=0.0605\n",
      "[Epoch 3 | step 1600/7098] L_total=1.2184  L_gt=0.9295  L_obj=0.2185  L_sim=0.0704\n",
      "[Epoch 3 | step 1650/7098] L_total=1.0570  L_gt=0.8545  L_obj=0.1239  L_sim=0.0786\n",
      "[Epoch 3 | step 1700/7098] L_total=1.2202  L_gt=0.9259  L_obj=0.2142  L_sim=0.0800\n",
      "[Epoch 3 | step 1750/7098] L_total=1.4362  L_gt=1.1502  L_obj=0.2051  L_sim=0.0809\n",
      "[Epoch 3 | step 1800/7098] L_total=1.5406  L_gt=1.2932  L_obj=0.1693  L_sim=0.0781\n",
      "[Epoch 3 | step 1850/7098] L_total=1.1972  L_gt=0.9493  L_obj=0.1920  L_sim=0.0559\n",
      "[Epoch 3 | step 1900/7098] L_total=1.2482  L_gt=1.0115  L_obj=0.1641  L_sim=0.0726\n",
      "[Epoch 3 | step 1950/7098] L_total=1.2299  L_gt=0.9764  L_obj=0.1787  L_sim=0.0747\n",
      "[Epoch 3 | step 2000/7098] L_total=1.3789  L_gt=1.1005  L_obj=0.2222  L_sim=0.0562\n",
      "[Epoch 3 | step 2050/7098] L_total=1.6345  L_gt=1.4252  L_obj=0.1396  L_sim=0.0696\n",
      "[Epoch 3 | step 2100/7098] L_total=1.2551  L_gt=0.9567  L_obj=0.2302  L_sim=0.0682\n",
      "[Epoch 3 | step 2150/7098] L_total=1.3318  L_gt=1.0915  L_obj=0.1756  L_sim=0.0648\n",
      "[Epoch 3 | step 2200/7098] L_total=1.0731  L_gt=0.8064  L_obj=0.2109  L_sim=0.0558\n",
      "[Epoch 3 | step 2250/7098] L_total=1.1846  L_gt=0.9762  L_obj=0.1450  L_sim=0.0634\n",
      "[Epoch 3 | step 2300/7098] L_total=1.1489  L_gt=0.8963  L_obj=0.1990  L_sim=0.0536\n",
      "[Epoch 3 | step 2350/7098] L_total=1.1816  L_gt=0.9487  L_obj=0.1625  L_sim=0.0703\n",
      "[Epoch 3 | step 2400/7098] L_total=1.2052  L_gt=0.9926  L_obj=0.1453  L_sim=0.0673\n",
      "[Epoch 3 | step 2450/7098] L_total=1.2744  L_gt=1.0138  L_obj=0.2127  L_sim=0.0479\n",
      "[Epoch 3 | step 2500/7098] L_total=1.4168  L_gt=1.1971  L_obj=0.1518  L_sim=0.0680\n",
      "[Epoch 3 | step 2550/7098] L_total=1.4653  L_gt=1.2183  L_obj=0.1757  L_sim=0.0712\n",
      "[Epoch 3 | step 2600/7098] L_total=1.1960  L_gt=0.9458  L_obj=0.1869  L_sim=0.0633\n",
      "[Epoch 3 | step 2650/7098] L_total=1.2985  L_gt=1.0764  L_obj=0.1425  L_sim=0.0796\n",
      "[Epoch 3 | step 2700/7098] L_total=1.3887  L_gt=1.1176  L_obj=0.2056  L_sim=0.0655\n",
      "[Epoch 3 | step 2750/7098] L_total=1.3483  L_gt=1.0810  L_obj=0.2004  L_sim=0.0669\n",
      "[Epoch 3 | step 2800/7098] L_total=1.2059  L_gt=0.9262  L_obj=0.2218  L_sim=0.0579\n",
      "[Epoch 3 | step 2850/7098] L_total=1.3321  L_gt=1.0398  L_obj=0.2387  L_sim=0.0535\n",
      "[Epoch 3 | step 2900/7098] L_total=1.1321  L_gt=0.9012  L_obj=0.1670  L_sim=0.0638\n",
      "[Epoch 3 | step 2950/7098] L_total=1.3175  L_gt=1.0641  L_obj=0.2156  L_sim=0.0378\n",
      "[Epoch 3 | step 3000/7098] L_total=1.3785  L_gt=1.1513  L_obj=0.1545  L_sim=0.0727\n",
      "[Epoch 3 | step 3050/7098] L_total=1.0933  L_gt=0.8173  L_obj=0.2070  L_sim=0.0691\n",
      "[Epoch 3 | step 3100/7098] L_total=1.1766  L_gt=0.9416  L_obj=0.1735  L_sim=0.0615\n",
      "[Epoch 3 | step 3150/7098] L_total=1.2896  L_gt=1.0387  L_obj=0.1903  L_sim=0.0606\n",
      "[Epoch 3 | step 3200/7098] L_total=1.1695  L_gt=0.8918  L_obj=0.1910  L_sim=0.0866\n",
      "[Epoch 3 | step 3250/7098] L_total=1.3388  L_gt=1.0321  L_obj=0.2476  L_sim=0.0591\n",
      "[Epoch 3 | step 3300/7098] L_total=1.2380  L_gt=1.0040  L_obj=0.1640  L_sim=0.0701\n",
      "[Epoch 3 | step 3350/7098] L_total=1.3836  L_gt=1.1381  L_obj=0.1783  L_sim=0.0672\n",
      "[Epoch 3 | step 3400/7098] L_total=1.0612  L_gt=0.8293  L_obj=0.1793  L_sim=0.0526\n",
      "[Epoch 3 | step 3450/7098] L_total=1.3594  L_gt=1.1048  L_obj=0.1872  L_sim=0.0675\n",
      "[Epoch 3 | step 3500/7098] L_total=1.1238  L_gt=0.8137  L_obj=0.2248  L_sim=0.0853\n",
      "[Epoch 3 | step 3550/7098] L_total=1.1034  L_gt=0.8599  L_obj=0.1850  L_sim=0.0584\n",
      "[Epoch 3 | step 3600/7098] L_total=1.4509  L_gt=1.2419  L_obj=0.1506  L_sim=0.0584\n",
      "[Epoch 3 | step 3650/7098] L_total=1.1451  L_gt=0.8935  L_obj=0.1931  L_sim=0.0585\n",
      "[Epoch 3 | step 3700/7098] L_total=1.0774  L_gt=0.8633  L_obj=0.1361  L_sim=0.0780\n",
      "[Epoch 3 | step 3750/7098] L_total=1.2554  L_gt=0.9834  L_obj=0.2205  L_sim=0.0514\n",
      "[Epoch 3 | step 3800/7098] L_total=1.3684  L_gt=1.0954  L_obj=0.2225  L_sim=0.0506\n",
      "[Epoch 3 | step 3850/7098] L_total=1.1482  L_gt=0.8968  L_obj=0.1726  L_sim=0.0788\n",
      "[Epoch 3 | step 3900/7098] L_total=1.2058  L_gt=0.9342  L_obj=0.2422  L_sim=0.0294\n",
      "[Epoch 3 | step 3950/7098] L_total=1.5557  L_gt=1.3037  L_obj=0.1751  L_sim=0.0768\n",
      "[Epoch 3 | step 4000/7098] L_total=1.0478  L_gt=0.7678  L_obj=0.2277  L_sim=0.0523\n",
      "[Epoch 3 | step 4050/7098] L_total=1.3038  L_gt=1.0868  L_obj=0.1488  L_sim=0.0682\n",
      "[Epoch 3 | step 4100/7098] L_total=1.1994  L_gt=0.8832  L_obj=0.2678  L_sim=0.0484\n",
      "[Epoch 3 | step 4150/7098] L_total=1.2374  L_gt=0.9821  L_obj=0.1854  L_sim=0.0699\n",
      "[Epoch 3 | step 4200/7098] L_total=1.0855  L_gt=0.8919  L_obj=0.1058  L_sim=0.0877\n",
      "[Epoch 3 | step 4250/7098] L_total=1.3672  L_gt=1.1207  L_obj=0.1794  L_sim=0.0671\n",
      "[Epoch 3 | step 4300/7098] L_total=1.3132  L_gt=1.0429  L_obj=0.1778  L_sim=0.0924\n",
      "[Epoch 3 | step 4350/7098] L_total=1.5970  L_gt=1.3694  L_obj=0.1516  L_sim=0.0761\n",
      "[Epoch 3 | step 4400/7098] L_total=1.1972  L_gt=0.9387  L_obj=0.1962  L_sim=0.0623\n",
      "[Epoch 3 | step 4450/7098] L_total=1.3110  L_gt=1.0897  L_obj=0.1421  L_sim=0.0792\n",
      "[Epoch 3 | step 4500/7098] L_total=1.2116  L_gt=0.9625  L_obj=0.1697  L_sim=0.0794\n",
      "[Epoch 3 | step 4550/7098] L_total=1.0852  L_gt=0.8116  L_obj=0.2188  L_sim=0.0548\n",
      "[Epoch 3 | step 4600/7098] L_total=1.5894  L_gt=1.3114  L_obj=0.2179  L_sim=0.0601\n",
      "[Epoch 3 | step 4650/7098] L_total=1.1788  L_gt=0.8855  L_obj=0.2396  L_sim=0.0537\n",
      "[Epoch 3 | step 4700/7098] L_total=1.3493  L_gt=1.0599  L_obj=0.2080  L_sim=0.0813\n",
      "[Epoch 3 | step 4750/7098] L_total=1.0829  L_gt=0.8636  L_obj=0.1597  L_sim=0.0597\n",
      "[Epoch 3 | step 4800/7098] L_total=1.2070  L_gt=0.9363  L_obj=0.1806  L_sim=0.0901\n",
      "[Epoch 3 | step 4850/7098] L_total=1.2961  L_gt=1.0187  L_obj=0.2169  L_sim=0.0606\n",
      "[Epoch 3 | step 4900/7098] L_total=1.2109  L_gt=0.9315  L_obj=0.2046  L_sim=0.0748\n",
      "[Epoch 3 | step 4950/7098] L_total=1.0070  L_gt=0.7802  L_obj=0.1800  L_sim=0.0468\n",
      "[Epoch 3 | step 5000/7098] L_total=1.1246  L_gt=0.8451  L_obj=0.1889  L_sim=0.0906\n",
      "[Epoch 3 | step 5050/7098] L_total=1.2232  L_gt=0.9979  L_obj=0.1618  L_sim=0.0635\n",
      "[Epoch 3 | step 5100/7098] L_total=1.2954  L_gt=1.0731  L_obj=0.1340  L_sim=0.0883\n",
      "[Epoch 3 | step 5150/7098] L_total=1.2074  L_gt=0.9857  L_obj=0.1585  L_sim=0.0632\n",
      "[Epoch 3 | step 5200/7098] L_total=1.3101  L_gt=1.0662  L_obj=0.1815  L_sim=0.0624\n",
      "[Epoch 3 | step 5250/7098] L_total=1.2579  L_gt=0.9958  L_obj=0.1974  L_sim=0.0648\n",
      "[Epoch 3 | step 5300/7098] L_total=1.1542  L_gt=0.8904  L_obj=0.2196  L_sim=0.0443\n",
      "[Epoch 3 | step 5350/7098] L_total=1.3235  L_gt=1.1044  L_obj=0.1461  L_sim=0.0730\n",
      "[Epoch 3 | step 5400/7098] L_total=1.3603  L_gt=1.1336  L_obj=0.1634  L_sim=0.0634\n",
      "[Epoch 3 | step 5450/7098] L_total=1.4471  L_gt=1.1790  L_obj=0.2180  L_sim=0.0501\n",
      "[Epoch 3 | step 5500/7098] L_total=1.2265  L_gt=0.9508  L_obj=0.2001  L_sim=0.0756\n",
      "[Epoch 3 | step 5550/7098] L_total=1.2037  L_gt=0.9611  L_obj=0.1689  L_sim=0.0737\n",
      "[Epoch 3 | step 5600/7098] L_total=1.3344  L_gt=1.0522  L_obj=0.2228  L_sim=0.0595\n",
      "[Epoch 3 | step 5650/7098] L_total=1.3257  L_gt=1.0929  L_obj=0.1661  L_sim=0.0667\n",
      "[Epoch 3 | step 5700/7098] L_total=1.4449  L_gt=1.1778  L_obj=0.2170  L_sim=0.0502\n",
      "[Epoch 3 | step 5750/7098] L_total=1.2916  L_gt=1.0652  L_obj=0.1580  L_sim=0.0684\n",
      "[Epoch 3 | step 5800/7098] L_total=1.1439  L_gt=0.8439  L_obj=0.2279  L_sim=0.0720\n",
      "[Epoch 3 | step 5850/7098] L_total=1.1113  L_gt=0.8358  L_obj=0.1998  L_sim=0.0758\n",
      "[Epoch 3 | step 5900/7098] L_total=1.4901  L_gt=1.2103  L_obj=0.2041  L_sim=0.0757\n",
      "[Epoch 3 | step 5950/7098] L_total=1.3762  L_gt=1.1538  L_obj=0.1695  L_sim=0.0528\n",
      "[Epoch 3 | step 6000/7098] L_total=1.2362  L_gt=0.9526  L_obj=0.2258  L_sim=0.0578\n",
      "[Epoch 3 | step 6050/7098] L_total=1.2861  L_gt=1.1126  L_obj=0.1376  L_sim=0.0359\n",
      "[Epoch 3 | step 6100/7098] L_total=1.2887  L_gt=1.0153  L_obj=0.2178  L_sim=0.0555\n",
      "[Epoch 3 | step 6150/7098] L_total=1.4266  L_gt=1.2181  L_obj=0.1372  L_sim=0.0712\n",
      "[Epoch 3 | step 6200/7098] L_total=1.0761  L_gt=0.8379  L_obj=0.1966  L_sim=0.0416\n",
      "[Epoch 3 | step 6250/7098] L_total=1.2889  L_gt=1.0338  L_obj=0.1746  L_sim=0.0804\n",
      "[Epoch 3 | step 6300/7098] L_total=1.4690  L_gt=1.2094  L_obj=0.2027  L_sim=0.0569\n",
      "[Epoch 3 | step 6350/7098] L_total=1.2939  L_gt=1.0413  L_obj=0.1754  L_sim=0.0772\n",
      "[Epoch 3 | step 6400/7098] L_total=1.2132  L_gt=0.9598  L_obj=0.1938  L_sim=0.0596\n",
      "[Epoch 3 | step 6450/7098] L_total=1.1717  L_gt=0.9771  L_obj=0.1233  L_sim=0.0714\n",
      "[Epoch 3 | step 6500/7098] L_total=1.2585  L_gt=1.0475  L_obj=0.1481  L_sim=0.0628\n",
      "[Epoch 3 | step 6550/7098] L_total=1.2249  L_gt=0.9346  L_obj=0.2196  L_sim=0.0707\n",
      "[Epoch 3 | step 6600/7098] L_total=1.4167  L_gt=1.1766  L_obj=0.1661  L_sim=0.0740\n",
      "[Epoch 3 | step 6650/7098] L_total=1.3302  L_gt=1.0211  L_obj=0.2578  L_sim=0.0513\n",
      "[Epoch 3 | step 6700/7098] L_total=1.3847  L_gt=1.1246  L_obj=0.1698  L_sim=0.0903\n",
      "[Epoch 3 | step 6750/7098] L_total=1.3153  L_gt=1.1103  L_obj=0.1431  L_sim=0.0619\n",
      "[Epoch 3 | step 6800/7098] L_total=1.3495  L_gt=0.9940  L_obj=0.2895  L_sim=0.0661\n",
      "[Epoch 3 | step 6850/7098] L_total=1.3220  L_gt=1.1049  L_obj=0.1683  L_sim=0.0488\n",
      "[Epoch 3 | step 6900/7098] L_total=1.4540  L_gt=1.1800  L_obj=0.1952  L_sim=0.0788\n",
      "[Epoch 3 | step 6950/7098] L_total=1.2474  L_gt=0.9916  L_obj=0.1963  L_sim=0.0595\n",
      "[Epoch 3 | step 7000/7098] L_total=1.4168  L_gt=1.1301  L_obj=0.2249  L_sim=0.0619\n",
      "[Epoch 3 | step 7050/7098] L_total=1.4704  L_gt=1.2789  L_obj=0.1268  L_sim=0.0648\n",
      "[Epoch 3] TRAIN: L_total=1.2642  L_gt=1.0138  L_obj=0.1845  L_sim=0.0658\n",
      "[Epoch 3] VAL  : L_total=0.2451  L_gt=0.0000  L_obj=0.1892  L_sim=0.0559\n",
      "\n",
      "[Epoch 4] -----------------------------\n",
      "[Epoch 4 | step 50/7098] L_total=1.2901  L_gt=1.0575  L_obj=0.1904  L_sim=0.0422\n",
      "[Epoch 4 | step 100/7098] L_total=1.2846  L_gt=1.0575  L_obj=0.1680  L_sim=0.0590\n",
      "[Epoch 4 | step 150/7098] L_total=1.3848  L_gt=1.1609  L_obj=0.1668  L_sim=0.0571\n",
      "[Epoch 4 | step 200/7098] L_total=1.4073  L_gt=1.1427  L_obj=0.2132  L_sim=0.0515\n",
      "[Epoch 4 | step 250/7098] L_total=1.4265  L_gt=1.2057  L_obj=0.1300  L_sim=0.0908\n",
      "[Epoch 4 | step 300/7098] L_total=1.2170  L_gt=0.8824  L_obj=0.2802  L_sim=0.0544\n",
      "[Epoch 4 | step 350/7098] L_total=1.3148  L_gt=1.1035  L_obj=0.1238  L_sim=0.0875\n",
      "[Epoch 4 | step 400/7098] L_total=1.2881  L_gt=1.0480  L_obj=0.1893  L_sim=0.0508\n",
      "[Epoch 4 | step 450/7098] L_total=1.2809  L_gt=0.9996  L_obj=0.2007  L_sim=0.0806\n",
      "[Epoch 4 | step 500/7098] L_total=1.5275  L_gt=1.2504  L_obj=0.2194  L_sim=0.0577\n",
      "[Epoch 4 | step 550/7098] L_total=1.0537  L_gt=0.8257  L_obj=0.1688  L_sim=0.0592\n",
      "[Epoch 4 | step 600/7098] L_total=1.0109  L_gt=0.7961  L_obj=0.1586  L_sim=0.0562\n",
      "[Epoch 4 | step 650/7098] L_total=1.3988  L_gt=1.1877  L_obj=0.1526  L_sim=0.0586\n",
      "[Epoch 4 | step 700/7098] L_total=1.0367  L_gt=0.8221  L_obj=0.1544  L_sim=0.0602\n",
      "[Epoch 4 | step 750/7098] L_total=1.3391  L_gt=1.0164  L_obj=0.2823  L_sim=0.0404\n",
      "[Epoch 4 | step 800/7098] L_total=1.0198  L_gt=0.8055  L_obj=0.0703  L_sim=0.1441\n",
      "[Epoch 4 | step 850/7098] L_total=1.7381  L_gt=1.2986  L_obj=0.4030  L_sim=0.0366\n",
      "[Epoch 4 | step 900/7098] L_total=1.1632  L_gt=0.9240  L_obj=0.1867  L_sim=0.0526\n",
      "[Epoch 4 | step 950/7098] L_total=1.5209  L_gt=0.9780  L_obj=0.4273  L_sim=0.1155\n",
      "[Epoch 4 | step 1000/7098] L_total=1.3485  L_gt=1.0789  L_obj=0.1686  L_sim=0.1011\n",
      "[Epoch 4 | step 1050/7098] L_total=1.1724  L_gt=0.9776  L_obj=0.1533  L_sim=0.0414\n",
      "[Epoch 4 | step 1100/7098] L_total=1.1280  L_gt=0.8936  L_obj=0.1989  L_sim=0.0355\n",
      "[Epoch 4 | step 1150/7098] L_total=1.4658  L_gt=1.1890  L_obj=0.2072  L_sim=0.0696\n",
      "[Epoch 4 | step 1200/7098] L_total=1.3571  L_gt=1.0576  L_obj=0.2604  L_sim=0.0390\n",
      "[Epoch 4 | step 1250/7098] L_total=1.2091  L_gt=0.9133  L_obj=0.2107  L_sim=0.0851\n",
      "[Epoch 4 | step 1300/7098] L_total=1.1728  L_gt=0.9958  L_obj=0.1116  L_sim=0.0653\n",
      "[Epoch 4 | step 1350/7098] L_total=1.2447  L_gt=1.0223  L_obj=0.1736  L_sim=0.0488\n",
      "[Epoch 4 | step 1400/7098] L_total=1.2745  L_gt=1.0305  L_obj=0.2440  L_sim=0.0000\n",
      "[Epoch 4 | step 1450/7098] L_total=1.2989  L_gt=0.9695  L_obj=0.2809  L_sim=0.0484\n",
      "[Epoch 4 | step 1500/7098] L_total=1.0195  L_gt=0.7896  L_obj=0.1789  L_sim=0.0510\n",
      "[Epoch 4 | step 1550/7098] L_total=1.1069  L_gt=0.8832  L_obj=0.1680  L_sim=0.0557\n",
      "[Epoch 4 | step 1600/7098] L_total=1.2824  L_gt=0.8919  L_obj=0.3133  L_sim=0.0772\n",
      "[Epoch 4 | step 1650/7098] L_total=1.0780  L_gt=0.8364  L_obj=0.1612  L_sim=0.0804\n",
      "[Epoch 4 | step 1700/7098] L_total=1.1462  L_gt=0.8954  L_obj=0.1653  L_sim=0.0855\n",
      "[Epoch 4 | step 1750/7098] L_total=0.9586  L_gt=0.8086  L_obj=0.1203  L_sim=0.0297\n",
      "[Epoch 4 | step 1800/7098] L_total=1.1687  L_gt=0.9650  L_obj=0.1312  L_sim=0.0725\n",
      "[Epoch 4 | step 1850/7098] L_total=1.3704  L_gt=1.1546  L_obj=0.1642  L_sim=0.0516\n",
      "[Epoch 4 | step 1900/7098] L_total=1.1591  L_gt=0.9572  L_obj=0.1537  L_sim=0.0482\n",
      "[Epoch 4 | step 1950/7098] L_total=1.0752  L_gt=0.8413  L_obj=0.1871  L_sim=0.0467\n",
      "[Epoch 4 | step 2000/7098] L_total=1.2461  L_gt=0.9825  L_obj=0.2240  L_sim=0.0396\n",
      "[Epoch 4 | step 2050/7098] L_total=1.2105  L_gt=0.8855  L_obj=0.1989  L_sim=0.1261\n",
      "[Epoch 4 | step 2100/7098] L_total=1.3566  L_gt=1.1416  L_obj=0.1668  L_sim=0.0482\n",
      "[Epoch 4 | step 2150/7098] L_total=1.3805  L_gt=1.0364  L_obj=0.3232  L_sim=0.0208\n",
      "[Epoch 4 | step 2200/7098] L_total=1.4034  L_gt=1.1276  L_obj=0.2558  L_sim=0.0200\n",
      "[Epoch 4 | step 2250/7098] L_total=1.2318  L_gt=1.0018  L_obj=0.1570  L_sim=0.0730\n",
      "[Epoch 4 | step 2300/7098] L_total=1.2375  L_gt=0.9558  L_obj=0.2512  L_sim=0.0305\n",
      "[Epoch 4 | step 2350/7098] L_total=1.4182  L_gt=1.1251  L_obj=0.2512  L_sim=0.0419\n",
      "[Epoch 4 | step 2400/7098] L_total=1.2679  L_gt=1.0279  L_obj=0.1901  L_sim=0.0498\n",
      "[Epoch 4 | step 2450/7098] L_total=1.3162  L_gt=0.9823  L_obj=0.2875  L_sim=0.0465\n",
      "[Epoch 4 | step 2500/7098] L_total=1.2460  L_gt=0.8913  L_obj=0.2959  L_sim=0.0588\n",
      "[Epoch 4 | step 2550/7098] L_total=1.2581  L_gt=0.9829  L_obj=0.1267  L_sim=0.1485\n",
      "[Epoch 4 | step 2600/7098] L_total=1.1524  L_gt=0.9395  L_obj=0.1479  L_sim=0.0650\n",
      "[Epoch 4 | step 2650/7098] L_total=1.5127  L_gt=1.1436  L_obj=0.3402  L_sim=0.0289\n",
      "[Epoch 4 | step 2700/7098] L_total=1.2768  L_gt=0.9274  L_obj=0.2925  L_sim=0.0569\n",
      "[Epoch 4 | step 2750/7098] L_total=1.1939  L_gt=0.9661  L_obj=0.1779  L_sim=0.0499\n",
      "[Epoch 4 | step 2800/7098] L_total=1.4256  L_gt=1.2058  L_obj=0.1652  L_sim=0.0545\n",
      "[Epoch 4 | step 2850/7098] L_total=1.3066  L_gt=1.0554  L_obj=0.2249  L_sim=0.0263\n",
      "[Epoch 4 | step 2900/7098] L_total=1.1893  L_gt=1.0558  L_obj=0.0548  L_sim=0.0787\n",
      "[Epoch 4 | step 2950/7098] L_total=1.2008  L_gt=0.9276  L_obj=0.2493  L_sim=0.0239\n",
      "[Epoch 4 | step 3000/7098] L_total=1.1039  L_gt=0.8300  L_obj=0.2156  L_sim=0.0583\n",
      "[Epoch 4 | step 3050/7098] L_total=1.0111  L_gt=0.6807  L_obj=0.2920  L_sim=0.0385\n",
      "[Epoch 4 | step 3100/7098] L_total=1.0143  L_gt=0.7172  L_obj=0.2476  L_sim=0.0495\n",
      "[Epoch 4 | step 3150/7098] L_total=1.3002  L_gt=1.0877  L_obj=0.1576  L_sim=0.0549\n",
      "[Epoch 4 | step 3200/7098] L_total=1.2502  L_gt=0.9995  L_obj=0.1933  L_sim=0.0573\n",
      "[Epoch 4 | step 3250/7098] L_total=1.2752  L_gt=1.0577  L_obj=0.1516  L_sim=0.0659\n",
      "[Epoch 4 | step 3300/7098] L_total=1.2892  L_gt=1.0619  L_obj=0.1537  L_sim=0.0736\n",
      "[Epoch 4 | step 3350/7098] L_total=1.1296  L_gt=0.8783  L_obj=0.1940  L_sim=0.0573\n",
      "[Epoch 4 | step 3400/7098] L_total=1.3467  L_gt=0.9537  L_obj=0.3304  L_sim=0.0626\n",
      "[Epoch 4 | step 3450/7098] L_total=1.3061  L_gt=1.0058  L_obj=0.2321  L_sim=0.0682\n",
      "[Epoch 4 | step 3500/7098] L_total=1.1499  L_gt=0.9153  L_obj=0.1907  L_sim=0.0439\n",
      "[Epoch 4 | step 3550/7098] L_total=0.8853  L_gt=0.7007  L_obj=0.1400  L_sim=0.0446\n",
      "[Epoch 4 | step 3600/7098] L_total=1.2714  L_gt=1.0049  L_obj=0.2066  L_sim=0.0599\n",
      "[Epoch 4 | step 3650/7098] L_total=1.2963  L_gt=1.0633  L_obj=0.1626  L_sim=0.0704\n",
      "[Epoch 4 | step 3700/7098] L_total=1.1160  L_gt=0.9636  L_obj=0.0959  L_sim=0.0565\n",
      "[Epoch 4 | step 3750/7098] L_total=1.2114  L_gt=1.0181  L_obj=0.1526  L_sim=0.0407\n",
      "[Epoch 4 | step 3800/7098] L_total=0.9425  L_gt=0.6690  L_obj=0.2198  L_sim=0.0536\n",
      "[Epoch 4 | step 3850/7098] L_total=1.2911  L_gt=0.9662  L_obj=0.2617  L_sim=0.0633\n",
      "[Epoch 4 | step 3900/7098] L_total=1.2689  L_gt=0.9779  L_obj=0.2440  L_sim=0.0470\n",
      "[Epoch 4 | step 3950/7098] L_total=1.1594  L_gt=0.9149  L_obj=0.2005  L_sim=0.0440\n",
      "[Epoch 4 | step 4000/7098] L_total=1.4663  L_gt=1.2068  L_obj=0.1962  L_sim=0.0633\n",
      "[Epoch 4 | step 4050/7098] L_total=1.3051  L_gt=1.0175  L_obj=0.2297  L_sim=0.0579\n",
      "[Epoch 4 | step 4100/7098] L_total=1.0914  L_gt=0.7962  L_obj=0.2448  L_sim=0.0504\n",
      "[Epoch 4 | step 4150/7098] L_total=1.1664  L_gt=0.9532  L_obj=0.1553  L_sim=0.0579\n",
      "[Epoch 4 | step 4200/7098] L_total=0.9127  L_gt=0.7349  L_obj=0.0926  L_sim=0.0852\n",
      "[Epoch 4 | step 4250/7098] L_total=1.1882  L_gt=0.8950  L_obj=0.2463  L_sim=0.0469\n",
      "[Epoch 4 | step 4300/7098] L_total=1.3197  L_gt=1.1037  L_obj=0.1273  L_sim=0.0887\n",
      "[Epoch 4 | step 4350/7098] L_total=1.1740  L_gt=0.9210  L_obj=0.1928  L_sim=0.0602\n",
      "[Epoch 4 | step 4400/7098] L_total=1.5789  L_gt=1.3235  L_obj=0.2077  L_sim=0.0478\n",
      "[Epoch 4 | step 4450/7098] L_total=1.3850  L_gt=1.1497  L_obj=0.1976  L_sim=0.0377\n",
      "[Epoch 4 | step 4500/7098] L_total=1.3768  L_gt=1.1121  L_obj=0.1826  L_sim=0.0821\n",
      "[Epoch 4 | step 4550/7098] L_total=1.0703  L_gt=0.7801  L_obj=0.2142  L_sim=0.0759\n",
      "[Epoch 4 | step 4600/7098] L_total=1.5085  L_gt=1.2030  L_obj=0.2571  L_sim=0.0484\n",
      "[Epoch 4 | step 4650/7098] L_total=1.3497  L_gt=1.1107  L_obj=0.1905  L_sim=0.0485\n",
      "[Epoch 4 | step 4700/7098] L_total=1.3440  L_gt=1.1142  L_obj=0.1984  L_sim=0.0315\n",
      "[Epoch 4 | step 4750/7098] L_total=1.2250  L_gt=0.9195  L_obj=0.2605  L_sim=0.0450\n",
      "[Epoch 4 | step 4800/7098] L_total=1.1036  L_gt=0.7602  L_obj=0.3227  L_sim=0.0207\n",
      "[Epoch 4 | step 4850/7098] L_total=1.3598  L_gt=1.0871  L_obj=0.2055  L_sim=0.0672\n",
      "[Epoch 4 | step 4900/7098] L_total=1.0844  L_gt=0.8478  L_obj=0.1793  L_sim=0.0573\n",
      "[Epoch 4 | step 4950/7098] L_total=0.9718  L_gt=0.7479  L_obj=0.1353  L_sim=0.0886\n",
      "[Epoch 4 | step 5000/7098] L_total=1.0342  L_gt=0.8073  L_obj=0.1881  L_sim=0.0388\n",
      "[Epoch 4 | step 5050/7098] L_total=1.2690  L_gt=1.0112  L_obj=0.2016  L_sim=0.0563\n",
      "[Epoch 4 | step 5100/7098] L_total=1.1658  L_gt=0.9079  L_obj=0.2166  L_sim=0.0414\n",
      "[Epoch 4 | step 5150/7098] L_total=1.3211  L_gt=1.0927  L_obj=0.1719  L_sim=0.0565\n",
      "[Epoch 4 | step 5200/7098] L_total=1.0993  L_gt=0.8473  L_obj=0.2202  L_sim=0.0318\n",
      "[Epoch 4 | step 5250/7098] L_total=1.2040  L_gt=0.9043  L_obj=0.2159  L_sim=0.0838\n",
      "[Epoch 4 | step 5300/7098] L_total=1.3177  L_gt=1.0254  L_obj=0.2439  L_sim=0.0484\n",
      "[Epoch 4 | step 5350/7098] L_total=1.0764  L_gt=0.8931  L_obj=0.1555  L_sim=0.0279\n",
      "[Epoch 4 | step 5400/7098] L_total=1.3723  L_gt=1.0319  L_obj=0.3198  L_sim=0.0206\n",
      "[Epoch 4 | step 5450/7098] L_total=1.1269  L_gt=0.8873  L_obj=0.1579  L_sim=0.0817\n",
      "[Epoch 4 | step 5500/7098] L_total=0.9721  L_gt=0.6875  L_obj=0.2409  L_sim=0.0437\n",
      "[Epoch 4 | step 5550/7098] L_total=1.1350  L_gt=0.8259  L_obj=0.2668  L_sim=0.0423\n",
      "[Epoch 4 | step 5600/7098] L_total=1.3989  L_gt=1.1395  L_obj=0.2024  L_sim=0.0570\n",
      "[Epoch 4 | step 5650/7098] L_total=1.1635  L_gt=0.9091  L_obj=0.2235  L_sim=0.0310\n",
      "[Epoch 4 | step 5700/7098] L_total=1.3409  L_gt=1.0507  L_obj=0.2144  L_sim=0.0757\n",
      "[Epoch 4 | step 5750/7098] L_total=1.1910  L_gt=0.9419  L_obj=0.1715  L_sim=0.0776\n",
      "[Epoch 4 | step 5800/7098] L_total=1.1558  L_gt=0.8393  L_obj=0.2897  L_sim=0.0268\n",
      "[Epoch 4 | step 5850/7098] L_total=1.2903  L_gt=0.9561  L_obj=0.2950  L_sim=0.0392\n",
      "[Epoch 4 | step 5900/7098] L_total=1.1256  L_gt=0.8996  L_obj=0.1685  L_sim=0.0574\n",
      "[Epoch 4 | step 5950/7098] L_total=1.5085  L_gt=1.2224  L_obj=0.2512  L_sim=0.0350\n",
      "[Epoch 4 | step 6000/7098] L_total=1.2181  L_gt=0.8681  L_obj=0.3208  L_sim=0.0292\n",
      "[Epoch 4 | step 6050/7098] L_total=1.3429  L_gt=1.0652  L_obj=0.1956  L_sim=0.0820\n",
      "[Epoch 4 | step 6100/7098] L_total=1.2215  L_gt=1.0038  L_obj=0.1681  L_sim=0.0496\n",
      "[Epoch 4 | step 6150/7098] L_total=1.1760  L_gt=0.9259  L_obj=0.1851  L_sim=0.0650\n",
      "[Epoch 4 | step 6200/7098] L_total=1.1259  L_gt=0.9498  L_obj=0.0894  L_sim=0.0867\n",
      "[Epoch 4 | step 6250/7098] L_total=1.1348  L_gt=0.9471  L_obj=0.1123  L_sim=0.0754\n",
      "[Epoch 4 | step 6300/7098] L_total=1.2429  L_gt=0.9799  L_obj=0.2338  L_sim=0.0292\n",
      "[Epoch 4 | step 6350/7098] L_total=1.2967  L_gt=1.0947  L_obj=0.1453  L_sim=0.0566\n",
      "[Epoch 4 | step 6400/7098] L_total=1.1716  L_gt=0.8956  L_obj=0.2270  L_sim=0.0490\n",
      "[Epoch 4 | step 6450/7098] L_total=1.2841  L_gt=1.0163  L_obj=0.1914  L_sim=0.0764\n",
      "[Epoch 4 | step 6500/7098] L_total=1.2675  L_gt=1.0412  L_obj=0.1577  L_sim=0.0686\n",
      "[Epoch 4 | step 6550/7098] L_total=1.5369  L_gt=1.2713  L_obj=0.2309  L_sim=0.0347\n",
      "[Epoch 4 | step 6600/7098] L_total=0.9778  L_gt=0.7856  L_obj=0.1307  L_sim=0.0615\n",
      "[Epoch 4 | step 6650/7098] L_total=1.2332  L_gt=0.9696  L_obj=0.2307  L_sim=0.0330\n",
      "[Epoch 4 | step 6700/7098] L_total=1.2127  L_gt=0.9780  L_obj=0.1990  L_sim=0.0358\n",
      "[Epoch 4 | step 6750/7098] L_total=1.0593  L_gt=0.7811  L_obj=0.1957  L_sim=0.0825\n",
      "[Epoch 4 | step 6800/7098] L_total=1.3146  L_gt=1.0847  L_obj=0.1819  L_sim=0.0480\n",
      "[Epoch 4 | step 6850/7098] L_total=1.0873  L_gt=0.9235  L_obj=0.1081  L_sim=0.0557\n",
      "[Epoch 4 | step 6900/7098] L_total=1.2574  L_gt=0.9860  L_obj=0.1938  L_sim=0.0776\n",
      "[Epoch 4 | step 6950/7098] L_total=1.3184  L_gt=1.1083  L_obj=0.1749  L_sim=0.0352\n",
      "[Epoch 4 | step 7000/7098] L_total=1.1916  L_gt=0.8844  L_obj=0.2563  L_sim=0.0509\n",
      "[Epoch 4 | step 7050/7098] L_total=1.2317  L_gt=0.9142  L_obj=0.2872  L_sim=0.0302\n",
      "[Epoch 4] TRAIN: L_total=1.2341  L_gt=0.9820  L_obj=0.1956  L_sim=0.0565\n",
      "[Epoch 4] VAL  : L_total=0.2480  L_gt=0.0000  L_obj=0.2001  L_sim=0.0480\n",
      "\n",
      "[Epoch 5] -----------------------------\n",
      "[Epoch 5 | step 50/7098] L_total=0.9979  L_gt=0.8114  L_obj=0.0958  L_sim=0.0907\n",
      "[Epoch 5 | step 100/7098] L_total=1.0816  L_gt=0.7900  L_obj=0.2316  L_sim=0.0600\n",
      "[Epoch 5 | step 150/7098] L_total=1.0537  L_gt=0.8041  L_obj=0.2006  L_sim=0.0490\n",
      "[Epoch 5 | step 200/7098] L_total=1.0217  L_gt=0.8050  L_obj=0.1794  L_sim=0.0373\n",
      "[Epoch 5 | step 250/7098] L_total=1.1425  L_gt=0.8448  L_obj=0.2887  L_sim=0.0090\n",
      "[Epoch 5 | step 300/7098] L_total=1.0048  L_gt=1.0048  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 5 | step 350/7098] L_total=1.2866  L_gt=1.0158  L_obj=0.2337  L_sim=0.0371\n",
      "[Epoch 5 | step 400/7098] L_total=1.1903  L_gt=0.8035  L_obj=0.3674  L_sim=0.0194\n",
      "[Epoch 5 | step 450/7098] L_total=1.1353  L_gt=0.8402  L_obj=0.2551  L_sim=0.0401\n",
      "[Epoch 5 | step 500/7098] L_total=1.2976  L_gt=0.9376  L_obj=0.3095  L_sim=0.0505\n",
      "[Epoch 5 | step 550/7098] L_total=1.2415  L_gt=0.9915  L_obj=0.1980  L_sim=0.0520\n",
      "[Epoch 5 | step 600/7098] L_total=1.3716  L_gt=1.1975  L_obj=0.1151  L_sim=0.0591\n",
      "[Epoch 5 | step 650/7098] L_total=0.9861  L_gt=0.7039  L_obj=0.2543  L_sim=0.0279\n",
      "[Epoch 5 | step 700/7098] L_total=0.9422  L_gt=0.9422  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 5 | step 750/7098] L_total=1.2385  L_gt=0.9090  L_obj=0.3040  L_sim=0.0256\n",
      "[Epoch 5 | step 800/7098] L_total=1.1071  L_gt=0.8850  L_obj=0.2122  L_sim=0.0099\n",
      "[Epoch 5 | step 850/7098] L_total=1.1340  L_gt=0.8384  L_obj=0.2464  L_sim=0.0491\n",
      "[Epoch 5 | step 900/7098] L_total=1.5568  L_gt=1.3906  L_obj=0.1002  L_sim=0.0661\n",
      "[Epoch 5 | step 950/7098] L_total=1.1405  L_gt=0.9996  L_obj=0.0919  L_sim=0.0489\n",
      "[Epoch 5 | step 1000/7098] L_total=0.9522  L_gt=0.9522  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 5 | step 1050/7098] L_total=1.4025  L_gt=1.0572  L_obj=0.3083  L_sim=0.0370\n",
      "[Epoch 5 | step 1100/7098] L_total=1.2015  L_gt=0.9128  L_obj=0.2741  L_sim=0.0146\n",
      "[Epoch 5 | step 1150/7098] L_total=1.3631  L_gt=0.9949  L_obj=0.2316  L_sim=0.1367\n",
      "[Epoch 5 | step 1200/7098] L_total=1.4646  L_gt=1.2728  L_obj=0.1507  L_sim=0.0410\n",
      "[Epoch 5 | step 1250/7098] L_total=1.4174  L_gt=1.0952  L_obj=0.2494  L_sim=0.0728\n",
      "[Epoch 5 | step 1300/7098] L_total=0.9716  L_gt=0.9716  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 5 | step 1350/7098] L_total=1.2095  L_gt=1.0063  L_obj=0.1779  L_sim=0.0254\n",
      "[Epoch 5 | step 1400/7098] L_total=1.2652  L_gt=1.0684  L_obj=0.1968  L_sim=0.0000\n",
      "[Epoch 5 | step 1450/7098] L_total=1.1896  L_gt=0.9469  L_obj=0.2088  L_sim=0.0339\n",
      "[Epoch 5 | step 1500/7098] L_total=1.3173  L_gt=1.1018  L_obj=0.1588  L_sim=0.0567\n",
      "[Epoch 5 | step 1550/7098] L_total=1.4013  L_gt=1.0394  L_obj=0.2606  L_sim=0.1012\n",
      "[Epoch 5 | step 1600/7098] L_total=1.1691  L_gt=0.9532  L_obj=0.1855  L_sim=0.0304\n",
      "[Epoch 5 | step 1650/7098] L_total=1.0766  L_gt=0.8938  L_obj=0.1518  L_sim=0.0309\n",
      "[Epoch 5 | step 1700/7098] L_total=1.0556  L_gt=0.9116  L_obj=0.1201  L_sim=0.0238\n",
      "[Epoch 5 | step 1750/7098] L_total=1.1933  L_gt=0.9168  L_obj=0.2359  L_sim=0.0406\n",
      "[Epoch 5 | step 1800/7098] L_total=0.9182  L_gt=0.7492  L_obj=0.1268  L_sim=0.0422\n",
      "[Epoch 5 | step 1850/7098] L_total=1.1988  L_gt=0.9609  L_obj=0.1486  L_sim=0.0893\n",
      "[Epoch 5 | step 1900/7098] L_total=1.2698  L_gt=1.0633  L_obj=0.1954  L_sim=0.0111\n",
      "[Epoch 5 | step 1950/7098] L_total=1.2235  L_gt=0.8238  L_obj=0.3605  L_sim=0.0392\n",
      "[Epoch 5 | step 2000/7098] L_total=1.4073  L_gt=1.1258  L_obj=0.2491  L_sim=0.0325\n",
      "[Epoch 5 | step 2050/7098] L_total=0.9680  L_gt=0.9680  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 5 | step 2100/7098] L_total=1.0741  L_gt=0.9208  L_obj=0.0937  L_sim=0.0597\n",
      "[Epoch 5 | step 2150/7098] L_total=1.0519  L_gt=0.9378  L_obj=0.1043  L_sim=0.0099\n",
      "[Epoch 5 | step 2200/7098] L_total=0.9493  L_gt=0.9493  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 5 | step 2250/7098] L_total=1.3289  L_gt=1.0824  L_obj=0.1920  L_sim=0.0546\n",
      "[Epoch 5 | step 2300/7098] L_total=1.3778  L_gt=1.1830  L_obj=0.1562  L_sim=0.0386\n",
      "[Epoch 5 | step 2350/7098] L_total=1.4340  L_gt=1.0443  L_obj=0.3833  L_sim=0.0064\n",
      "[Epoch 5 | step 2400/7098] L_total=1.7466  L_gt=1.3877  L_obj=0.1303  L_sim=0.2286\n",
      "[Epoch 5 | step 2450/7098] L_total=1.3405  L_gt=1.0161  L_obj=0.2830  L_sim=0.0413\n",
      "[Epoch 5 | step 2500/7098] L_total=1.0168  L_gt=0.7574  L_obj=0.2266  L_sim=0.0329\n",
      "[Epoch 5 | step 2550/7098] L_total=1.3365  L_gt=1.1471  L_obj=0.1166  L_sim=0.0729\n",
      "[Epoch 5 | step 2600/7098] L_total=1.1108  L_gt=0.8768  L_obj=0.2019  L_sim=0.0320\n",
      "[Epoch 5 | step 2650/7098] L_total=1.4958  L_gt=1.1646  L_obj=0.2450  L_sim=0.0863\n",
      "[Epoch 5 | step 2700/7098] L_total=1.5980  L_gt=1.3114  L_obj=0.2282  L_sim=0.0585\n",
      "[Epoch 5 | step 2750/7098] L_total=1.1973  L_gt=1.1973  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 5 | step 2800/7098] L_total=1.3166  L_gt=1.1159  L_obj=0.1318  L_sim=0.0688\n",
      "[Epoch 5 | step 2850/7098] L_total=1.1714  L_gt=0.9032  L_obj=0.2206  L_sim=0.0477\n",
      "[Epoch 5 | step 2900/7098] L_total=1.0207  L_gt=0.8003  L_obj=0.1671  L_sim=0.0534\n",
      "[Epoch 5 | step 2950/7098] L_total=1.2082  L_gt=0.9918  L_obj=0.1732  L_sim=0.0432\n",
      "[Epoch 5 | step 3000/7098] L_total=1.1615  L_gt=0.8686  L_obj=0.2526  L_sim=0.0403\n",
      "[Epoch 5 | step 3050/7098] L_total=1.3722  L_gt=1.1217  L_obj=0.2258  L_sim=0.0247\n",
      "[Epoch 5 | step 3100/7098] L_total=0.9673  L_gt=0.8426  L_obj=0.1025  L_sim=0.0222\n",
      "[Epoch 5 | step 3150/7098] L_total=1.2749  L_gt=0.8679  L_obj=0.3386  L_sim=0.0684\n",
      "[Epoch 5 | step 3200/7098] L_total=1.0770  L_gt=0.9551  L_obj=0.0735  L_sim=0.0485\n",
      "[Epoch 5 | step 3250/7098] L_total=1.1584  L_gt=1.0763  L_obj=0.0821  L_sim=0.0000\n",
      "[Epoch 5 | step 3300/7098] L_total=1.5319  L_gt=1.1738  L_obj=0.3297  L_sim=0.0284\n",
      "[Epoch 5 | step 3350/7098] L_total=1.3816  L_gt=1.1584  L_obj=0.2060  L_sim=0.0171\n",
      "[Epoch 5 | step 3400/7098] L_total=1.3981  L_gt=1.0115  L_obj=0.3656  L_sim=0.0210\n",
      "[Epoch 5 | step 3450/7098] L_total=1.1920  L_gt=0.9238  L_obj=0.2216  L_sim=0.0466\n",
      "[Epoch 5 | step 3500/7098] L_total=1.3232  L_gt=1.0476  L_obj=0.2425  L_sim=0.0331\n",
      "[Epoch 5 | step 3550/7098] L_total=1.1278  L_gt=0.9007  L_obj=0.1582  L_sim=0.0690\n",
      "[Epoch 5 | step 3600/7098] L_total=1.0756  L_gt=0.9119  L_obj=0.1262  L_sim=0.0375\n",
      "[Epoch 5 | step 3650/7098] L_total=1.3372  L_gt=1.0682  L_obj=0.2599  L_sim=0.0091\n",
      "[Epoch 5 | step 3700/7098] L_total=1.2957  L_gt=1.1193  L_obj=0.1425  L_sim=0.0339\n",
      "[Epoch 5 | step 3750/7098] L_total=1.5314  L_gt=1.0041  L_obj=0.5112  L_sim=0.0161\n",
      "[Epoch 5 | step 3800/7098] L_total=1.0635  L_gt=1.0635  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 5 | step 3850/7098] L_total=1.0805  L_gt=0.8572  L_obj=0.1192  L_sim=0.1042\n",
      "[Epoch 5 | step 3900/7098] L_total=1.2660  L_gt=1.0005  L_obj=0.1966  L_sim=0.0689\n",
      "[Epoch 5 | step 3950/7098] L_total=1.2668  L_gt=1.0781  L_obj=0.1457  L_sim=0.0431\n",
      "[Epoch 5 | step 4000/7098] L_total=1.2502  L_gt=0.8372  L_obj=0.3845  L_sim=0.0285\n",
      "[Epoch 5 | step 4050/7098] L_total=1.1716  L_gt=1.0185  L_obj=0.0983  L_sim=0.0548\n",
      "[Epoch 5 | step 4100/7098] L_total=1.5260  L_gt=1.1973  L_obj=0.2931  L_sim=0.0356\n",
      "[Epoch 5 | step 4150/7098] L_total=1.0716  L_gt=0.7611  L_obj=0.2494  L_sim=0.0611\n",
      "[Epoch 5 | step 4200/7098] L_total=1.2700  L_gt=1.0811  L_obj=0.0148  L_sim=0.1740\n",
      "[Epoch 5 | step 4250/7098] L_total=1.0431  L_gt=0.7481  L_obj=0.2394  L_sim=0.0556\n",
      "[Epoch 5 | step 4300/7098] L_total=1.1991  L_gt=0.9415  L_obj=0.2179  L_sim=0.0397\n",
      "[Epoch 5 | step 4350/7098] L_total=1.1500  L_gt=0.9183  L_obj=0.1698  L_sim=0.0619\n",
      "[Epoch 5 | step 4400/7098] L_total=1.2530  L_gt=1.0586  L_obj=0.1209  L_sim=0.0735\n",
      "[Epoch 5 | step 4450/7098] L_total=1.2336  L_gt=0.9566  L_obj=0.1983  L_sim=0.0786\n",
      "[Epoch 5 | step 4500/7098] L_total=1.1314  L_gt=0.9255  L_obj=0.0871  L_sim=0.1189\n",
      "[Epoch 5 | step 4550/7098] L_total=1.0894  L_gt=0.8087  L_obj=0.2499  L_sim=0.0308\n",
      "[Epoch 5 | step 4600/7098] L_total=1.0724  L_gt=0.7517  L_obj=0.3167  L_sim=0.0040\n",
      "[Epoch 5 | step 4650/7098] L_total=1.2222  L_gt=0.9195  L_obj=0.2404  L_sim=0.0622\n",
      "[Epoch 5 | step 4700/7098] L_total=1.4648  L_gt=1.2778  L_obj=0.1470  L_sim=0.0400\n",
      "[Epoch 5 | step 4750/7098] L_total=1.2560  L_gt=0.9861  L_obj=0.2231  L_sim=0.0469\n",
      "[Epoch 5 | step 4800/7098] L_total=1.2691  L_gt=1.0567  L_obj=0.1568  L_sim=0.0556\n",
      "[Epoch 5 | step 4850/7098] L_total=1.1358  L_gt=0.8214  L_obj=0.3056  L_sim=0.0088\n",
      "[Epoch 5 | step 4900/7098] L_total=1.1224  L_gt=0.8802  L_obj=0.1923  L_sim=0.0500\n",
      "[Epoch 5 | step 4950/7098] L_total=1.3644  L_gt=1.0618  L_obj=0.2654  L_sim=0.0372\n",
      "[Epoch 5 | step 5000/7098] L_total=1.2913  L_gt=1.0174  L_obj=0.2567  L_sim=0.0173\n",
      "[Epoch 5 | step 5050/7098] L_total=1.1693  L_gt=0.9990  L_obj=0.1529  L_sim=0.0174\n",
      "[Epoch 5 | step 5100/7098] L_total=0.8893  L_gt=0.7916  L_obj=0.0715  L_sim=0.0262\n",
      "[Epoch 5 | step 5150/7098] L_total=1.4454  L_gt=1.0215  L_obj=0.4125  L_sim=0.0114\n",
      "[Epoch 5 | step 5200/7098] L_total=1.0481  L_gt=1.0481  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 5 | step 5250/7098] L_total=1.0850  L_gt=0.8056  L_obj=0.2586  L_sim=0.0208\n",
      "[Epoch 5 | step 5300/7098] L_total=1.1682  L_gt=1.1682  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 5 | step 5350/7098] L_total=1.3746  L_gt=0.9308  L_obj=0.3615  L_sim=0.0822\n",
      "[Epoch 5 | step 5400/7098] L_total=1.3183  L_gt=0.9769  L_obj=0.3223  L_sim=0.0191\n",
      "[Epoch 5 | step 5450/7098] L_total=0.8272  L_gt=0.8272  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 5 | step 5500/7098] L_total=1.2578  L_gt=1.0910  L_obj=0.1561  L_sim=0.0107\n",
      "[Epoch 5 | step 5550/7098] L_total=1.2880  L_gt=1.1389  L_obj=0.0642  L_sim=0.0849\n",
      "[Epoch 5 | step 5600/7098] L_total=0.9535  L_gt=0.8667  L_obj=0.0273  L_sim=0.0595\n",
      "[Epoch 5 | step 5650/7098] L_total=1.1681  L_gt=0.8936  L_obj=0.2396  L_sim=0.0349\n",
      "[Epoch 5 | step 5700/7098] L_total=1.2897  L_gt=1.0130  L_obj=0.2097  L_sim=0.0670\n",
      "[Epoch 5 | step 5750/7098] L_total=1.1061  L_gt=0.9050  L_obj=0.1696  L_sim=0.0315\n",
      "[Epoch 5 | step 5800/7098] L_total=1.1539  L_gt=0.8252  L_obj=0.2746  L_sim=0.0541\n",
      "[Epoch 5 | step 5850/7098] L_total=1.2373  L_gt=1.0433  L_obj=0.1533  L_sim=0.0407\n",
      "[Epoch 5 | step 5900/7098] L_total=1.1693  L_gt=0.9349  L_obj=0.1692  L_sim=0.0652\n",
      "[Epoch 5 | step 5950/7098] L_total=1.0009  L_gt=0.7588  L_obj=0.1507  L_sim=0.0914\n",
      "[Epoch 5 | step 6000/7098] L_total=1.0185  L_gt=0.7839  L_obj=0.1653  L_sim=0.0693\n",
      "[Epoch 5 | step 6050/7098] L_total=0.9385  L_gt=0.9385  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 5 | step 6100/7098] L_total=1.2323  L_gt=0.8134  L_obj=0.4059  L_sim=0.0130\n",
      "[Epoch 5 | step 6150/7098] L_total=1.0664  L_gt=0.8421  L_obj=0.1603  L_sim=0.0640\n",
      "[Epoch 5 | step 6200/7098] L_total=1.0129  L_gt=0.8094  L_obj=0.1280  L_sim=0.0755\n",
      "[Epoch 5 | step 6250/7098] L_total=1.1761  L_gt=0.9370  L_obj=0.2140  L_sim=0.0250\n",
      "[Epoch 5 | step 6300/7098] L_total=0.9821  L_gt=0.8252  L_obj=0.1326  L_sim=0.0243\n",
      "[Epoch 5 | step 6350/7098] L_total=1.1716  L_gt=0.8560  L_obj=0.2566  L_sim=0.0591\n",
      "[Epoch 5 | step 6400/7098] L_total=1.0059  L_gt=0.8258  L_obj=0.1387  L_sim=0.0414\n",
      "[Epoch 5 | step 6450/7098] L_total=1.1502  L_gt=0.9727  L_obj=0.1154  L_sim=0.0621\n",
      "[Epoch 5 | step 6500/7098] L_total=1.4200  L_gt=1.1544  L_obj=0.2312  L_sim=0.0344\n",
      "[Epoch 5 | step 6550/7098] L_total=1.2005  L_gt=0.9984  L_obj=0.1867  L_sim=0.0154\n",
      "[Epoch 5 | step 6600/7098] L_total=1.4079  L_gt=1.0301  L_obj=0.2247  L_sim=0.1531\n",
      "[Epoch 5 | step 6650/7098] L_total=0.9478  L_gt=0.7542  L_obj=0.1631  L_sim=0.0304\n",
      "[Epoch 5 | step 6700/7098] L_total=1.5399  L_gt=1.0758  L_obj=0.3988  L_sim=0.0653\n",
      "[Epoch 5 | step 6750/7098] L_total=1.2646  L_gt=0.9511  L_obj=0.2455  L_sim=0.0680\n",
      "[Epoch 5 | step 6800/7098] L_total=1.2435  L_gt=0.8217  L_obj=0.3548  L_sim=0.0670\n",
      "[Epoch 5 | step 6850/7098] L_total=1.4225  L_gt=1.1110  L_obj=0.2491  L_sim=0.0624\n",
      "[Epoch 5 | step 6900/7098] L_total=1.3307  L_gt=1.1545  L_obj=0.0948  L_sim=0.0813\n",
      "[Epoch 5 | step 6950/7098] L_total=1.3544  L_gt=1.1395  L_obj=0.1551  L_sim=0.0598\n",
      "[Epoch 5 | step 7000/7098] L_total=1.3525  L_gt=1.2407  L_obj=0.0896  L_sim=0.0222\n",
      "[Epoch 5 | step 7050/7098] L_total=0.9984  L_gt=0.7315  L_obj=0.2506  L_sim=0.0163\n",
      "[Epoch 5] TRAIN: L_total=1.1980  L_gt=0.9538  L_obj=0.1990  L_sim=0.0453\n",
      "[Epoch 5] VAL  : L_total=0.2293  L_gt=0.0000  L_obj=0.1894  L_sim=0.0399\n",
      "\n",
      "[Epoch 6] -----------------------------\n",
      "[Epoch 6 | step 50/7098] L_total=1.0500  L_gt=0.9195  L_obj=0.1006  L_sim=0.0299\n",
      "[Epoch 6 | step 100/7098] L_total=1.2092  L_gt=1.1061  L_obj=0.1030  L_sim=0.0000\n",
      "[Epoch 6 | step 150/7098] L_total=0.7541  L_gt=0.7541  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 6 | step 200/7098] L_total=1.4022  L_gt=1.1641  L_obj=0.1034  L_sim=0.1346\n",
      "[Epoch 6 | step 250/7098] L_total=1.2257  L_gt=0.8703  L_obj=0.3158  L_sim=0.0397\n",
      "[Epoch 6 | step 300/7098] L_total=0.6459  L_gt=0.6459  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 6 | step 350/7098] L_total=1.0966  L_gt=0.8033  L_obj=0.2553  L_sim=0.0380\n",
      "[Epoch 6 | step 400/7098] L_total=1.2278  L_gt=1.0657  L_obj=0.0982  L_sim=0.0639\n",
      "[Epoch 6 | step 450/7098] L_total=1.0201  L_gt=0.7654  L_obj=0.2369  L_sim=0.0178\n",
      "[Epoch 6 | step 500/7098] L_total=0.8557  L_gt=0.8557  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 6 | step 550/7098] L_total=1.3835  L_gt=1.0858  L_obj=0.2977  L_sim=0.0000\n",
      "[Epoch 6 | step 600/7098] L_total=0.8348  L_gt=0.8348  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 6 | step 650/7098] L_total=1.2887  L_gt=0.9864  L_obj=0.2735  L_sim=0.0288\n",
      "[Epoch 6 | step 700/7098] L_total=1.1650  L_gt=1.0167  L_obj=0.0864  L_sim=0.0619\n",
      "[Epoch 6 | step 750/7098] L_total=1.0358  L_gt=0.9259  L_obj=0.0243  L_sim=0.0856\n",
      "[Epoch 6 | step 800/7098] L_total=0.7313  L_gt=0.7313  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 6 | step 850/7098] L_total=1.0380  L_gt=0.8191  L_obj=0.1630  L_sim=0.0558\n",
      "[Epoch 6 | step 900/7098] L_total=1.2530  L_gt=1.1030  L_obj=0.1138  L_sim=0.0362\n",
      "[Epoch 6 | step 950/7098] L_total=1.4427  L_gt=1.2564  L_obj=0.0916  L_sim=0.0947\n",
      "[Epoch 6 | step 1000/7098] L_total=1.1265  L_gt=1.1265  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 6 | step 1050/7098] L_total=0.9218  L_gt=0.9218  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 6 | step 1100/7098] L_total=1.1345  L_gt=0.9372  L_obj=0.1210  L_sim=0.0762\n",
      "[Epoch 6 | step 1150/7098] L_total=1.1737  L_gt=0.9877  L_obj=0.1568  L_sim=0.0292\n",
      "[Epoch 6 | step 1200/7098] L_total=0.9364  L_gt=0.9364  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 6 | step 1250/7098] L_total=1.3703  L_gt=1.0454  L_obj=0.2549  L_sim=0.0700\n",
      "[Epoch 6 | step 1300/7098] L_total=0.9458  L_gt=0.9458  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 6 | step 1350/7098] L_total=1.1165  L_gt=1.1165  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 6 | step 1400/7098] L_total=1.2725  L_gt=1.0293  L_obj=0.1851  L_sim=0.0581\n",
      "[Epoch 6 | step 1450/7098] L_total=1.2227  L_gt=0.9989  L_obj=0.1940  L_sim=0.0298\n",
      "[Epoch 6 | step 1500/7098] L_total=1.4488  L_gt=0.9189  L_obj=0.5298  L_sim=0.0000\n",
      "[Epoch 6 | step 1550/7098] L_total=1.4992  L_gt=1.1201  L_obj=0.3264  L_sim=0.0527\n",
      "[Epoch 6 | step 1600/7098] L_total=1.0148  L_gt=1.0148  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 6 | step 1650/7098] L_total=1.1187  L_gt=0.8033  L_obj=0.2670  L_sim=0.0483\n",
      "[Epoch 6 | step 1700/7098] L_total=1.1018  L_gt=1.1018  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 6 | step 1750/7098] L_total=1.1602  L_gt=1.0284  L_obj=0.0630  L_sim=0.0689\n",
      "[Epoch 6 | step 1800/7098] L_total=1.1695  L_gt=0.9062  L_obj=0.2171  L_sim=0.0463\n",
      "[Epoch 6 | step 1850/7098] L_total=1.1262  L_gt=0.8318  L_obj=0.2412  L_sim=0.0532\n",
      "[Epoch 6 | step 1900/7098] L_total=1.1050  L_gt=1.1050  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 6 | step 1950/7098] L_total=0.8973  L_gt=0.8973  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 6 | step 2000/7098] L_total=1.3844  L_gt=1.1953  L_obj=0.1593  L_sim=0.0297\n",
      "[Epoch 6 | step 2050/7098] L_total=1.2038  L_gt=1.0065  L_obj=0.1593  L_sim=0.0380\n",
      "[Epoch 6 | step 2100/7098] L_total=1.6344  L_gt=1.2908  L_obj=0.3181  L_sim=0.0255\n",
      "[Epoch 6 | step 2150/7098] L_total=0.9380  L_gt=0.8592  L_obj=0.0559  L_sim=0.0229\n",
      "[Epoch 6 | step 2200/7098] L_total=1.0267  L_gt=0.8942  L_obj=0.0921  L_sim=0.0404\n",
      "[Epoch 6 | step 2250/7098] L_total=0.6404  L_gt=0.6404  L_obj=0.0000  L_sim=0.0000\n",
      "[Epoch 6 | step 2300/7098] L_total=0.8128  L_gt=0.8128  L_obj=0.0000  L_sim=0.0000\n"
     ]
    }
   ],
   "source": [
    "# === Training loop (teacher–student v-CLR, AMP, loss prints, robust view handling) ===\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "is_ts = hasattr(model, \"student\") and hasattr(model, \"teacher\")\n",
    "student = model.student if is_ts else model\n",
    "teacher = model.teacher if is_ts else model\n",
    "\n",
    "best_val_L_total = float(\"inf\")\n",
    "start_epoch = 1\n",
    "\n",
    "print(\"[DEBUG] Starting training.\")\n",
    "print(f\"[DEBUG] len(train_dataset) = {len(train_dataset)}\")\n",
    "print(f\"[DEBUG] len(train_loader)  = {len(train_loader)}\")\n",
    "print(f\"[DEBUG] TRAIN_BATCH_SIZE = {TRAIN_BATCH_SIZE}, TRAIN_NUM_WORKERS = {TRAIN_NUM_WORKERS}\")\n",
    "\n",
    "for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
    "    model.train()\n",
    "    adjust_lr(optimizer, epoch)\n",
    "\n",
    "    sum_L_total = 0.0\n",
    "    sum_L_gt    = 0.0\n",
    "    sum_L_obj   = 0.0\n",
    "    sum_L_sim   = 0.0\n",
    "    batch_count = 0\n",
    "\n",
    "    print(f\"\\n[Epoch {epoch}] -----------------------------\")\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        images_nat   = batch[\"image_nat\"].to(device, non_blocking=True)\n",
    "        images_depth = _normalize_view_batch(batch[\"image_depth\"], device)\n",
    "        images_style = _normalize_view_batch(batch[\"image_style\"], device)\n",
    "        targets      = batch[\"targets\"]\n",
    "        proposals    = batch[\"proposals\"]\n",
    "\n",
    "        has_depth = images_depth is not None\n",
    "        has_style = images_style is not None\n",
    "\n",
    "        # v-CLR view sampling: randomly pick one extra view when both exist\n",
    "        if has_depth and has_style:\n",
    "            if random.random() < 0.5:\n",
    "                use_depth, use_style = True, False\n",
    "            else:\n",
    "                use_depth, use_style = False, True\n",
    "        else:\n",
    "            use_depth, use_style = has_depth, has_style\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            if is_ts:\n",
    "                # EMA teacher on natural images (no gradient)\n",
    "                with torch.no_grad():\n",
    "                    outputs_nat_teacher, _ = teacher(\n",
    "                        images_nat, targets=None, proposals=proposals, view_name=\"nat\"\n",
    "                    )\n",
    "                # Student gets GT detection loss on nat\n",
    "                outputs_nat_student, loss_nat = student(\n",
    "                    images_nat, targets=targets, proposals=proposals, view_name=\"nat\"\n",
    "                )\n",
    "                outputs_nat = outputs_nat_teacher\n",
    "            else:\n",
    "                outputs_nat, loss_nat = student(\n",
    "                    images_nat, targets=targets, proposals=proposals, view_name=\"nat\"\n",
    "                )\n",
    "\n",
    "            outputs_depth, loss_depth = outputs_nat, {}\n",
    "            outputs_style, loss_style = outputs_nat, {}\n",
    "\n",
    "            # NOTE: Depth/style views do NOT receive GT targets per v-CLR paper.\n",
    "            # These views are trained via view-consistency losses (L_obj, L_sim) only.\n",
    "            # GT supervision is applied only to the natural view.\n",
    "            if use_depth and images_depth is not None:\n",
    "                outputs_depth, loss_depth = student(\n",
    "                    images_depth, targets=None, proposals=proposals, view_name=\"depth\"\n",
    "                )\n",
    "            if use_style and images_style is not None:\n",
    "                outputs_style, loss_style = student(\n",
    "                    images_style, targets=None, proposals=proposals, view_name=\"style\"\n",
    "                )\n",
    "\n",
    "            vclr_loss = compute_vclr_losses(\n",
    "                outputs_nat,  loss_nat,\n",
    "                outputs_depth, loss_depth,\n",
    "                outputs_style, loss_style,\n",
    "                proposals,\n",
    "            )\n",
    "            loss = vclr_loss[\"L_total\"]\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if is_ts:\n",
    "            model.update_teacher()\n",
    "\n",
    "        L_total = float(vclr_loss[\"L_total\"])\n",
    "        L_gt    = float(vclr_loss[\"L_gt\"])\n",
    "        L_obj   = float(vclr_loss[\"L_obj\"])\n",
    "        L_sim   = float(vclr_loss[\"L_sim\"])\n",
    "\n",
    "        sum_L_total += L_total\n",
    "        sum_L_gt    += L_gt\n",
    "        sum_L_obj   += L_obj\n",
    "        sum_L_sim   += L_sim\n",
    "        batch_count += 1\n",
    "\n",
    "        if (step + 1) % 50 == 0:\n",
    "            print(\n",
    "                f\"[Epoch {epoch} | step {step+1}/{len(train_loader)}] \"\n",
    "                f\"L_total={L_total:.4f}  L_gt={L_gt:.4f}  \"\n",
    "                f\"L_obj={L_obj:.4f}  L_sim={L_sim:.4f}\"\n",
    "            )\n",
    "\n",
    "    avg_L_total = sum_L_total / max(1, batch_count)\n",
    "    avg_L_gt    = sum_L_gt    / max(1, batch_count)\n",
    "    avg_L_obj   = sum_L_obj   / max(1, batch_count)\n",
    "    avg_L_sim   = sum_L_sim   / max(1, batch_count)\n",
    "\n",
    "    train_history[\"L_total_train\"].append(avg_L_total)\n",
    "    train_history[\"L_gt_train\"].append(avg_L_gt)\n",
    "    train_history[\"L_obj_train\"].append(avg_L_obj)\n",
    "    train_history[\"L_sim_train\"].append(avg_L_sim)\n",
    "\n",
    "    # === Validation epoch (loss-only) ===\n",
    "    val_losses = compute_epoch_loss_on_loader(model, val_loader_loss)\n",
    "    train_history[\"L_total_val\"].append(val_losses[\"L_total\"])\n",
    "    train_history[\"L_gt_val\"].append(val_losses[\"L_gt\"])\n",
    "    train_history[\"L_obj_val\"].append(val_losses[\"L_obj\"])\n",
    "    train_history[\"L_sim_val\"].append(val_losses[\"L_sim\"])\n",
    "\n",
    "    print(\n",
    "        f\"[Epoch {epoch}] TRAIN: \"\n",
    "        f\"L_total={avg_L_total:.4f}  L_gt={avg_L_gt:.4f}  \"\n",
    "        f\"L_obj={avg_L_obj:.4f}  L_sim={avg_L_sim:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"[Epoch {epoch}] VAL  : \"\n",
    "        f\"L_total={val_losses['L_total']:.4f}  \"\n",
    "        f\"L_gt={val_losses['L_gt']:.4f}  \"\n",
    "        f\"L_obj={val_losses['L_obj']:.4f}  \"\n",
    "        f\"L_sim={val_losses['L_sim']:.4f}\"\n",
    "    )\n",
    "\n",
    "    if val_losses[\"L_total\"] < best_val_L_total:\n",
    "        best_val_L_total = val_losses[\"L_total\"]\n",
    "        print(f\"[Epoch {epoch}] New best val L_total={best_val_L_total:.4f}\")\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"train_history\": train_history,\n",
    "            },\n",
    "            \"vclr_convnext_teacher_student_best.pth\",\n",
    "        )\n",
    "        print(\"  -> Saved checkpoint: vclr_convnext_teacher_student_best.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54e2887c-763d-4fd8-a1cb-4fd3b158073a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to: checkpoints\\convnext_vclr_epoch4_rev2.pth\n"
     ]
    }
   ],
   "source": [
    "# === Save a checkpoint after training\n",
    "from pathlib import Path\n",
    "\n",
    "CHECKPOINT_DIR = Path(\"checkpoints\")\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ckpt_path = CHECKPOINT_DIR / f\"convnext_vclr_epoch{NUM_EPOCHS}_rev2.pth\"\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        \"epoch\": NUM_EPOCHS,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"config\": {\n",
    "            \"IMG_SIZE\": IMG_SIZE,\n",
    "            \"TRAIN_BATCH_SIZE\": TRAIN_BATCH_SIZE,\n",
    "            \"BASE_LR\": BASE_LR,\n",
    "            \"NUM_EPOCHS\": NUM_EPOCHS,\n",
    "        },\n",
    "    },\n",
    "    ckpt_path,\n",
    ")\n",
    "\n",
    "print(\"Saved checkpoint to:\", ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafafdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Validation dataset for Non-VOC AR (UVO, boxes + proxy masks) ===\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "\n",
    "# COCO-format Non-VOC annotations (UVO subset)\n",
    "# NONVOC_VAL_JSON and NONVOC_IMG_DIR should already be defined as:\n",
    "# NONVOC_VAL_JSON = DATA_ROOT / \"uvo_nonvoc_val_rle.json\"\n",
    "# NONVOC_IMG_DIR  = DATA_ROOT / \"uvo_videos_dense_frames\"\n",
    "coco_nonvoc = COCO(str(NONVOC_VAL_JSON))\n",
    "val_img_ids = sorted(coco_nonvoc.getImgIds())\n",
    "print(\"Non-VOC val images:\", len(val_img_ids))\n",
    "\n",
    "\n",
    "class VCLRValDataset(Dataset):\n",
    "    def __init__(self, coco: COCO, img_root: Path, img_size: int = IMG_SIZE):\n",
    "        \"\"\"\n",
    "        img_root: directory that 'file_name' in JSON is relative to.\n",
    "        Example: file_name = '--33Lscn6sk/180.png'\n",
    "        and actual disk path:\n",
    "            NONVOC_IMG_DIR/--33Lscn6sk/180.png\n",
    "        \"\"\"\n",
    "        self.coco = coco\n",
    "        self.img_root = Path(img_root)\n",
    "        self.img_ids = sorted(coco.getImgIds())\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.resize_img = T.Resize((img_size, img_size))\n",
    "        self.to_tensor = T.ToTensor()\n",
    "        self.normalize = T.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco.loadImgs([img_id])[0]\n",
    "\n",
    "        # e.g. '--33Lscn6sk/180.png'\n",
    "        rel_path = Path(img_info[\"file_name\"])\n",
    "        path = self.img_root / rel_path\n",
    "\n",
    "        # Fallback: .png <-> .jpg if needed\n",
    "        if not path.is_file():\n",
    "            alt = None\n",
    "            if path.suffix.lower() == \".png\":\n",
    "                alt = path.with_suffix(\".jpg\")\n",
    "            elif path.suffix.lower() == \".jpg\":\n",
    "                alt = path.with_suffix(\".png\")\n",
    "            if alt is not None and alt.is_file():\n",
    "                path = alt\n",
    "            else:\n",
    "                raise FileNotFoundError(\n",
    "                    f\"Image not found for img_id={img_id}. Tried {path}\"\n",
    "                    + (f\" and {alt}\" if alt is not None else \"\")\n",
    "                )\n",
    "\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        w0, h0 = img.size\n",
    "        orig_size = torch.tensor([h0, w0], dtype=torch.float32)\n",
    "\n",
    "        img = self.resize_img(img)\n",
    "        img = self.to_tensor(img)\n",
    "        img = self.normalize(img)\n",
    "\n",
    "        # Returned:\n",
    "        #   img        : resized + normalized tensor (C, H, W)\n",
    "        #   img_id     : COCO/UVO image id (int)\n",
    "        #   orig_size  : original (H, W) for rescaling boxes/masks back\n",
    "        return img, img_id, orig_size\n",
    "\n",
    "\n",
    "def val_collate(batch):\n",
    "    images = torch.stack([b[0] for b in batch], dim=0)\n",
    "    img_ids = [b[1] for b in batch]\n",
    "    orig_sizes = torch.stack([b[2] for b in batch], dim=0)\n",
    "    return images, img_ids, orig_sizes\n",
    "\n",
    "\n",
    "val_dataset = VCLRValDataset(coco_nonvoc, NONVOC_IMG_DIR, img_size=IMG_SIZE)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=VAL_NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    collate_fn=val_collate,\n",
    ")\n",
    "\n",
    "print(\"Val (Non-VOC UVO) batches:\", len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4316a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COCO AR evaluation + Table-1-style row (bbox + segm) ===\n",
    "\n",
    "import gc\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools import mask as maskUtils\n",
    "import torch\n",
    "\n",
    "def evaluate_vclr(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Evaluate AR/AP for bounding boxes and proxy masks on Non-VOC UVO.\n",
    "    If a teacher–student wrapper is used, the EMA teacher is used for inference.\n",
    "    \"\"\"\n",
    "    # Use EMA teacher if available; otherwise use the model directly\n",
    "    if hasattr(model, \"teacher\"):\n",
    "        net = model.teacher\n",
    "    else:\n",
    "        net = model\n",
    "\n",
    "    net.eval()\n",
    "    dets_bbox = []\n",
    "    dets_segm = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, img_ids, orig_sizes in val_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "\n",
    "            # Forward pass on natural images\n",
    "            outputs, _ = net(images, targets=None, proposals=None, view_name=\"nat\")\n",
    "\n",
    "            for i, img_id in enumerate(img_ids):\n",
    "                out = outputs[i]\n",
    "                boxes = out[\"boxes\"].detach().cpu().clone()\n",
    "                scores = out[\"scores\"].detach().cpu()\n",
    "\n",
    "                if boxes.numel() == 0:\n",
    "                    continue\n",
    "\n",
    "                # Scale boxes from network (IMG_SIZE) back to original H,W\n",
    "                h0, w0 = orig_sizes[i].tolist()   # orig_size was [H, W]\n",
    "                sx = w0 / IMG_SIZE\n",
    "                sy = h0 / IMG_SIZE\n",
    "\n",
    "                boxes[:, 0] *= sx\n",
    "                boxes[:, 2] *= sx\n",
    "                boxes[:, 1] *= sy\n",
    "                boxes[:, 3] *= sy\n",
    "\n",
    "                ws = boxes[:, 2] - boxes[:, 0]\n",
    "                hs = boxes[:, 3] - boxes[:, 1]\n",
    "\n",
    "                H = int(round(h0))\n",
    "                W = int(round(w0))\n",
    "                image_id_int = int(img_id)\n",
    "\n",
    "                for k in range(boxes.size(0)):\n",
    "                    x0 = float(boxes[k, 0])\n",
    "                    y0 = float(boxes[k, 1])\n",
    "                    w  = float(ws[k])\n",
    "                    h  = float(hs[k])\n",
    "                    score = float(scores[k])\n",
    "\n",
    "                    if w <= 0 or h <= 0:\n",
    "                        continue\n",
    "\n",
    "                    # Bounding box detection\n",
    "                    dets_bbox.append(\n",
    "                        {\n",
    "                            \"image_id\": image_id_int,\n",
    "                            \"category_id\": 1,\n",
    "                            \"bbox\": [x0, y0, w, h],\n",
    "                            \"score\": score,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    # Rectangular proxy mask encoded as RLE for COCO \"segm\" eval\n",
    "                    x1 = x0 + w\n",
    "                    y1 = y0 + h\n",
    "                    poly = [x0, y0, x1, y0, x1, y1, x0, y1]  # xyxy polygon\n",
    "\n",
    "                    # frPyObjects -> RLE; COCO expects RLE for detection segm\n",
    "                    rle = maskUtils.frPyObjects([poly], H, W)[0]\n",
    "                    if isinstance(rle[\"counts\"], bytes):\n",
    "                        rle[\"counts\"] = rle[\"counts\"].decode(\"ascii\")\n",
    "\n",
    "                    dets_segm.append(\n",
    "                        {\n",
    "                            \"image_id\": image_id_int,\n",
    "                            \"category_id\": 1,\n",
    "                            \"segmentation\": rle,\n",
    "                            \"score\": score,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    if len(dets_bbox) == 0:\n",
    "        print(\"[WARN] No detections produced on Non-VOC val set.\")\n",
    "        return None\n",
    "\n",
    "    # Bounding box AR / AP\n",
    "    coco_dt_box = coco_nonvoc.loadRes(dets_bbox)\n",
    "    coco_eval_box = COCOeval(coco_nonvoc, coco_dt_box, iouType=\"bbox\")\n",
    "    coco_eval_box.evaluate()\n",
    "    coco_eval_box.accumulate()\n",
    "    coco_eval_box.summarize()\n",
    "    stats_box = coco_eval_box.stats\n",
    "\n",
    "    # Segmentation AR / AP using rectangular proxy masks (RLE)\n",
    "    coco_dt_segm = coco_nonvoc.loadRes(dets_segm)\n",
    "    coco_eval_segm = COCOeval(coco_nonvoc, coco_dt_segm, iouType=\"segm\")\n",
    "    coco_eval_segm.evaluate()\n",
    "    coco_eval_segm.accumulate()\n",
    "    coco_eval_segm.summarize()\n",
    "    stats_segm = coco_eval_segm.stats\n",
    "\n",
    "    return {\n",
    "        \"ARb_1\":   stats_box[6],\n",
    "        \"ARb_10\":  stats_box[7],\n",
    "        \"ARb_100\": stats_box[8],\n",
    "        \"APb\":     stats_box[0],\n",
    "        \"ARm_1\":   stats_segm[6],\n",
    "        \"ARm_10\":  stats_segm[7],\n",
    "        \"ARm_100\": stats_segm[8],\n",
    "        \"APm\":     stats_segm[0],\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Run evaluation and print Table-1-style row ---\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "ar = evaluate_vclr(model)\n",
    "\n",
    "if ar is not None:\n",
    "    ARb10  = ar[\"ARb_10\"]  * 100.0\n",
    "    ARb100 = ar[\"ARb_100\"] * 100.0\n",
    "    ARm10  = ar[\"ARm_10\"]  * 100.0\n",
    "    ARm100 = ar[\"ARm_100\"] * 100.0\n",
    "\n",
    "    print(\"\\nTable-1-style row (Non-VOC UVO):\")\n",
    "    print(\n",
    "        f\"ConvNeXt v-CLR (CNN, EMA) | \"\n",
    "        f\"AR^b_10 = {ARb10:.1f}  \"\n",
    "        f\"AR^b_100 = {ARb100:.1f}  \"\n",
    "        f\"AR^m_10 = {ARm10:.1f}  \"\n",
    "        f\"AR^m_100 = {ARm100:.1f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa07915b-2aec-47e6-9496-ac50154b7039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'supercategory': 'person', 'id': 1, 'name': 'person'}}\n",
      "Category IDs: [1]\n"
     ]
    }
   ],
   "source": [
    "print(coco_nonvoc.cats)\n",
    "print(\"Category IDs:\", coco_nonvoc.getCatIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95ce7889-3e00-4204-97e1-9588fd7358c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN BATCH ===\n",
      "image_nat batch shape : torch.Size([10, 3, 800, 800])\n",
      "  sample[0] shape   : torch.Size([3, 800, 800])\n",
      "image_depth[0] shape: torch.Size([3, 800, 800])\n",
      "image_style[0] shape: torch.Size([3, 800, 800])\n",
      "\n",
      "=== VAL-AR BATCH (UVO Non-VOC) ===\n",
      "images batch shape  : torch.Size([10, 3, 800, 800])\n",
      "sample[0] shape     : torch.Size([3, 800, 800])\n",
      "orig_size[0] (H,W)  : [480.0, 854.0]\n",
      "orig_size[1] (H,W)  : [480.0, 854.0]\n"
     ]
    }
   ],
   "source": [
    "# === Debug: compare training vs validation image dimensions ===\n",
    "\n",
    "import torch\n",
    "\n",
    "# ----- 1) One batch from TRAIN loader (v-CLR training set) -----\n",
    "train_batch = next(iter(train_loader))\n",
    "\n",
    "images_nat   = train_batch[\"image_nat\"]\n",
    "images_depth = train_batch[\"image_depth\"]\n",
    "images_style = train_batch[\"image_style\"]\n",
    "\n",
    "print(\"=== TRAIN BATCH ===\")\n",
    "print(\"image_nat batch shape :\",\n",
    "      images_nat.shape if isinstance(images_nat, torch.Tensor) else type(images_nat))\n",
    "\n",
    "if isinstance(images_nat, torch.Tensor):\n",
    "    print(\"  sample[0] shape   :\", images_nat[0].shape)\n",
    "\n",
    "# Depth view\n",
    "if images_depth is None:\n",
    "    print(\"image_depth         : None\")\n",
    "elif isinstance(images_depth, torch.Tensor):\n",
    "    print(\"image_depth batch   :\", images_depth.shape)\n",
    "elif isinstance(images_depth, list) and len(images_depth) > 0 and isinstance(images_depth[0], torch.Tensor):\n",
    "    print(\"image_depth[0] shape:\", images_depth[0].shape)\n",
    "else:\n",
    "    print(\"image_depth type    :\", type(images_depth))\n",
    "\n",
    "# Style view\n",
    "if images_style is None:\n",
    "    print(\"image_style         : None\")\n",
    "elif isinstance(images_style, torch.Tensor):\n",
    "    print(\"image_style batch   :\", images_style.shape)\n",
    "elif isinstance(images_style, list) and len(images_style) > 0 and isinstance(images_style[0], torch.Tensor):\n",
    "    print(\"image_style[0] shape:\", images_style[0].shape)\n",
    "else:\n",
    "    print(\"image_style type    :\", type(images_style))\n",
    "\n",
    "# ----- 2) One batch from AR VAL loader (UVO Non-VOC) -----\n",
    "val_batch = next(iter(val_loader))\n",
    "val_imgs, val_ids, val_orig_sizes = val_batch\n",
    "\n",
    "print(\"\\n=== VAL-AR BATCH (UVO Non-VOC) ===\")\n",
    "print(\"images batch shape  :\", val_imgs.shape)         # [B, 3, H_val, W_val]\n",
    "print(\"sample[0] shape     :\", val_imgs[0].shape)      # [3, H_val, W_val]\n",
    "print(\"orig_size[0] (H,W)  :\", val_orig_sizes[0].tolist())\n",
    "print(\"orig_size[1] (H,W)  :\", val_orig_sizes[1].tolist() if len(val_orig_sizes) > 1 else \"(only one sample)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780d3cef-db1e-4a59-a618-f7c86d10ff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plot training / validation losses for v-CLR ===\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Quick sanity check on what's in train_history\n",
    "print(\"L_total_train len:\", len(train_history.get(\"L_total_train\", [])))\n",
    "print(\"L_total_val   len:\", len(train_history.get(\"L_total_val\", [])))\n",
    "print(\"L_gt_train    len:\", len(train_history.get(\"L_gt_train\", [])))\n",
    "print(\"L_gt_val      len:\", len(train_history.get(\"L_gt_val\", [])))\n",
    "print(\"L_obj_train   len:\", len(train_history.get(\"L_obj_train\", [])))\n",
    "print(\"L_obj_val     len:\", len(train_history.get(\"L_obj_val\", [])))\n",
    "print(\"L_sim_train   len:\", len(train_history.get(\"L_sim_train\", [])))\n",
    "print(\"L_sim_val     len:\", len(train_history.get(\"L_sim_val\", [])))\n",
    "\n",
    "# Use the minimum length across train/val so x and y always match\n",
    "n_total = min(len(train_history.get(\"L_total_train\", [])),\n",
    "              len(train_history.get(\"L_total_val\", [])))\n",
    "n_gt    = min(len(train_history.get(\"L_gt_train\", [])),\n",
    "              len(train_history.get(\"L_gt_val\", [])))\n",
    "n_obj   = min(len(train_history.get(\"L_obj_train\", [])),\n",
    "              len(train_history.get(\"L_obj_val\", [])))\n",
    "n_sim   = min(len(train_history.get(\"L_sim_train\", [])),\n",
    "              len(train_history.get(\"L_sim_val\", [])))\n",
    "\n",
    "if n_total == 0:\n",
    "    print(\"No logged epochs in train_history. Run training first, then re-run this cell.\")\n",
    "else:\n",
    "    epochs_total = np.arange(1, n_total + 1)\n",
    "\n",
    "    # 1) Total loss: train vs val\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(epochs_total,\n",
    "             train_history[\"L_total_train\"][:n_total],\n",
    "             label=\"train L_total\")\n",
    "    plt.plot(epochs_total,\n",
    "             train_history[\"L_total_val\"][:n_total],\n",
    "             label=\"val L_total\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"L_total\")\n",
    "    plt.title(\"v-CLR total loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2) L_gt\n",
    "    if n_gt > 0:\n",
    "        epochs_gt = np.arange(1, n_gt + 1)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(epochs_gt,\n",
    "                 train_history[\"L_gt_train\"][:n_gt],\n",
    "                 label=\"train L_gt\")\n",
    "        plt.plot(epochs_gt,\n",
    "                 train_history[\"L_gt_val\"][:n_gt],\n",
    "                 label=\"val L_gt\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"L_gt\")\n",
    "        plt.title(\"v-CLR GT loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 3) L_obj\n",
    "    if n_obj > 0:\n",
    "        epochs_obj = np.arange(1, n_obj + 1)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(epochs_obj,\n",
    "                 train_history[\"L_obj_train\"][:n_obj],\n",
    "                 label=\"train L_obj\")\n",
    "        plt.plot(epochs_obj,\n",
    "                 train_history[\"L_obj_val\"][:n_obj],\n",
    "                 label=\"val L_obj\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"L_obj\")\n",
    "        plt.title(\"v-CLR object loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 4) L_sim\n",
    "    if n_sim > 0:\n",
    "        epochs_sim = np.arange(1, n_sim + 1)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(epochs_sim,\n",
    "                 train_history[\"L_sim_train\"][:n_sim],\n",
    "                 label=\"train L_sim\")\n",
    "        plt.plot(epochs_sim,\n",
    "                 train_history[\"L_sim_val\"][:n_sim],\n",
    "                 label=\"val L_sim\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"L_sim\")\n",
    "        plt.title(\"v-CLR similarity loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a8a231-b8e1-43b7-bebb-7f5385c844d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
