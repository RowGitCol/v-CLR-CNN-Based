{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd32234a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "# === Install ultralytics (YOLOv11) if not already installed ===\n",
    "# Uncomment the following line if you need to install it:\n",
    "# !pip install ultralytics pycocotools\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools import mask as maskUtils\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Check for GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78e4998f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset root: C:\\Users\\nickg\\Workspace\\v-CLR-CNN-Based\\datasets\n",
      "UVO annotations: C:\\Users\\nickg\\Workspace\\v-CLR-CNN-Based\\datasets\\uvo_val_coco.json\n",
      "UVO frames: C:\\Users\\nickg\\Workspace\\v-CLR-CNN-Based\\datasets\\uvo_frames\n",
      "YOLO model: yolo11s-seg.pt\n",
      "\n",
      "IMPORTANT: Run 'python extract_uvo_frames.py' first if you haven't already!\n"
     ]
    }
   ],
   "source": [
    "# === Configuration ===\n",
    "\n",
    "# Dataset paths (after running extract_uvo_frames.py)\n",
    "DATA_ROOT = Path(r\"C:\\Users\\nickg\\Workspace\\v-CLR-CNN-Based\\datasets\")\n",
    "\n",
    "# UVO validation set (converted to COCO format by extract_uvo_frames.py)\n",
    "# Run `python extract_uvo_frames.py` first to generate these files!\n",
    "UVO_VAL_JSON = DATA_ROOT / \"uvo_val_coco.json\"\n",
    "UVO_FRAMES_DIR = DATA_ROOT / \"uvo_frames\"\n",
    "\n",
    "# YOLOv11 model variant (options: yolo11n, yolo11s, yolo11m, yolo11l, yolo11x)\n",
    "# For segmentation, use: yolo11n-seg, yolo11s-seg, yolo11m-seg, yolo11l-seg, yolo11x-seg\n",
    "YOLO_MODEL = \"yolo11s-seg.pt\"  # Small model with segmentation\n",
    "\n",
    "# Inference settings\n",
    "IMG_SIZE = 640  # YOLOv11 default input size\n",
    "CONF_THRESHOLD = 0.001  # Low threshold to get more detections for AR computation\n",
    "IOU_THRESHOLD = 0.7  # NMS IoU threshold\n",
    "MAX_DET = 300  # Maximum detections per image\n",
    "\n",
    "# Batch size for inference\n",
    "BATCH_SIZE = 1  # Process one image at a time for accurate latency measurement\n",
    "\n",
    "print(f\"Dataset root: {DATA_ROOT}\")\n",
    "print(f\"UVO annotations: {UVO_VAL_JSON}\")\n",
    "print(f\"UVO frames: {UVO_FRAMES_DIR}\")\n",
    "print(f\"YOLO model: {YOLO_MODEL}\")\n",
    "print(\"\\nIMPORTANT: Run 'python extract_uvo_frames.py' first if you haven't already!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f0f8996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.91s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of validation images: 22950\n",
      "Number of categories: 1\n",
      "\n",
      "Sample images:\n",
      "  - -3zcsBnDVzU/180.png (854x480)\n",
      "  - -3zcsBnDVzU/181.png (854x480)\n",
      "  - -3zcsBnDVzU/182.png (854x480)\n",
      "Done (t=0.91s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of validation images: 22950\n",
      "Number of categories: 1\n",
      "\n",
      "Sample images:\n",
      "  - -3zcsBnDVzU/180.png (854x480)\n",
      "  - -3zcsBnDVzU/181.png (854x480)\n",
      "  - -3zcsBnDVzU/182.png (854x480)\n"
     ]
    }
   ],
   "source": [
    "# === Verify dataset paths exist ===\n",
    "\n",
    "if not UVO_VAL_JSON.exists():\n",
    "    print(f\"ERROR: Annotation file not found: {UVO_VAL_JSON}\")\n",
    "    print(\"\\nPlease run the frame extraction script first:\")\n",
    "    print(\"    python extract_uvo_frames.py\")\n",
    "    raise FileNotFoundError(f\"Run 'python extract_uvo_frames.py' to generate {UVO_VAL_JSON}\")\n",
    "\n",
    "if not UVO_FRAMES_DIR.exists():\n",
    "    print(f\"ERROR: Frames directory not found: {UVO_FRAMES_DIR}\")\n",
    "    print(\"\\nPlease run the frame extraction script first:\")\n",
    "    print(\"    python extract_uvo_frames.py\")\n",
    "    raise FileNotFoundError(f\"Run 'python extract_uvo_frames.py' to extract frames to {UVO_FRAMES_DIR}\")\n",
    "\n",
    "# Load COCO-format annotations\n",
    "coco_uvo = COCO(str(UVO_VAL_JSON))\n",
    "val_img_ids = sorted(coco_uvo.getImgIds())\n",
    "\n",
    "print(f\"Number of validation images: {len(val_img_ids)}\")\n",
    "print(f\"Number of categories: {len(coco_uvo.getCatIds())}\")\n",
    "\n",
    "# Show some sample images\n",
    "sample_imgs = coco_uvo.loadImgs(val_img_ids[:3])\n",
    "print(\"\\nSample images:\")\n",
    "for img in sample_imgs:\n",
    "    print(f\"  - {img['file_name']} ({img['width']}x{img['height']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05aa5240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLOv11 model: yolo11s-seg.pt\n",
      "Model loaded successfully\n",
      "Model task: segment\n",
      "Number of classes: 80\n",
      "Class names (first 10): ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light']\n",
      "Model loaded successfully\n",
      "Model task: segment\n",
      "Number of classes: 80\n",
      "Class names (first 10): ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light']\n"
     ]
    }
   ],
   "source": [
    "# === Load YOLOv11 model ===\n",
    "\n",
    "print(f\"Loading YOLOv11 model: {YOLO_MODEL}\")\n",
    "model = YOLO(YOLO_MODEL)\n",
    "\n",
    "# Move to GPU if available\n",
    "if device == \"cuda\":\n",
    "    model.to(device)\n",
    "\n",
    "print(f\"Model loaded successfully\")\n",
    "print(f\"Model task: {model.task}\")\n",
    "print(f\"Number of classes: {len(model.names)}\")\n",
    "print(f\"Class names (first 10): {list(model.names.values())[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6a44e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helper function to get image path ===\n",
    "\n",
    "def get_image_path(coco: COCO, img_id: int, img_root: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Get the full path to an image given its COCO image ID.\n",
    "    Handles .png <-> .jpg fallback if needed.\n",
    "    \"\"\"\n",
    "    img_info = coco.loadImgs([img_id])[0]\n",
    "    rel_path = Path(img_info[\"file_name\"])\n",
    "    path = img_root / rel_path\n",
    "    \n",
    "    if path.is_file():\n",
    "        return path\n",
    "    \n",
    "    # Try alternative extension\n",
    "    if path.suffix.lower() == \".png\":\n",
    "        alt = path.with_suffix(\".jpg\")\n",
    "    elif path.suffix.lower() == \".jpg\":\n",
    "        alt = path.with_suffix(\".png\")\n",
    "    else:\n",
    "        alt = None\n",
    "    \n",
    "    if alt is not None and alt.is_file():\n",
    "        return alt\n",
    "    \n",
    "    raise FileNotFoundError(\n",
    "        f\"Image not found for img_id={img_id}. Tried {path}\"\n",
    "        + (f\" and {alt}\" if alt is not None else \"\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e5b7c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Run YOLOv11 inference on the validation set ===\n",
    "\n",
    "def run_yolo_inference(\n",
    "    model: YOLO,\n",
    "    coco: COCO,\n",
    "    img_root: Path,\n",
    "    img_ids: List[int],\n",
    "    conf_thresh: float = 0.001,\n",
    "    iou_thresh: float = 0.7,\n",
    "    max_det: int = 300,\n",
    "    img_size: int = 640,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Run YOLO inference on all images and collect detections.\n",
    "    Returns: (bbox_detections, segm_detections, latency_stats)\n",
    "    \"\"\"\n",
    "    dets_bbox = []\n",
    "    dets_segm = []\n",
    "    latencies = []\n",
    "    \n",
    "    print(f\"Running inference on {len(img_ids)} images...\")\n",
    "    \n",
    "    for idx, img_id in enumerate(img_ids):\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(img_ids)} images...\")\n",
    "        \n",
    "        try:\n",
    "            img_path = get_image_path(coco, img_id, img_root)\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"  Warning: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Load image to get original size\n",
    "        img = Image.open(img_path)\n",
    "        orig_w, orig_h = img.size\n",
    "        \n",
    "        # Run inference with timing\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        results = model.predict(\n",
    "            source=str(img_path),\n",
    "            conf=conf_thresh,\n",
    "            iou=iou_thresh,\n",
    "            max_det=max_det,\n",
    "            imgsz=img_size,\n",
    "            verbose=False,\n",
    "            device=device,\n",
    "        )\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        latencies.append((end_time - start_time) * 1000)  # Convert to ms\n",
    "        \n",
    "        # Process results\n",
    "        result = results[0]  # Single image\n",
    "        \n",
    "        if result.boxes is None or len(result.boxes) == 0:\n",
    "            continue\n",
    "        \n",
    "        boxes = result.boxes.xyxy.cpu().numpy()  # [N, 4] in xyxy format\n",
    "        scores = result.boxes.conf.cpu().numpy()  # [N]\n",
    "        \n",
    "        # Get masks if available (for segmentation models)\n",
    "        has_masks = hasattr(result, 'masks') and result.masks is not None\n",
    "        if has_masks:\n",
    "            masks = result.masks.data.cpu().numpy()  # [N, H, W]\n",
    "        \n",
    "        for k in range(len(boxes)):\n",
    "            x1, y1, x2, y2 = boxes[k]\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            score = float(scores[k])\n",
    "            \n",
    "            if w <= 0 or h <= 0:\n",
    "                continue\n",
    "            \n",
    "            # Bounding box detection (COCO format: [x, y, w, h])\n",
    "            dets_bbox.append({\n",
    "                \"image_id\": int(img_id),\n",
    "                \"category_id\": 1,  # All detections mapped to category 1 for class-agnostic AR\n",
    "                \"bbox\": [float(x1), float(y1), float(w), float(h)],\n",
    "                \"score\": score,\n",
    "            })\n",
    "            \n",
    "            # Segmentation detection\n",
    "            if has_masks:\n",
    "                # Convert mask to RLE\n",
    "                mask_k = masks[k]\n",
    "                # Resize mask to original image size if needed\n",
    "                if mask_k.shape != (orig_h, orig_w):\n",
    "                    from PIL import Image as PILImage\n",
    "                    mask_pil = PILImage.fromarray((mask_k * 255).astype(np.uint8))\n",
    "                    mask_pil = mask_pil.resize((orig_w, orig_h), PILImage.NEAREST)\n",
    "                    mask_k = np.array(mask_pil) / 255.0\n",
    "                \n",
    "                # Convert to binary and encode as RLE\n",
    "                mask_binary = (mask_k > 0.5).astype(np.uint8)\n",
    "                mask_fortran = np.asfortranarray(mask_binary)\n",
    "                rle = maskUtils.encode(mask_fortran)\n",
    "                if isinstance(rle[\"counts\"], bytes):\n",
    "                    rle[\"counts\"] = rle[\"counts\"].decode(\"ascii\")\n",
    "                \n",
    "                dets_segm.append({\n",
    "                    \"image_id\": int(img_id),\n",
    "                    \"category_id\": 1,\n",
    "                    \"segmentation\": rle,\n",
    "                    \"score\": score,\n",
    "                })\n",
    "            else:\n",
    "                # Use box as proxy mask (rectangular RLE)\n",
    "                poly = [float(x1), float(y1), float(x2), float(y1),\n",
    "                        float(x2), float(y2), float(x1), float(y2)]\n",
    "                rle = maskUtils.frPyObjects([poly], int(orig_h), int(orig_w))[0]\n",
    "                if isinstance(rle[\"counts\"], bytes):\n",
    "                    rle[\"counts\"] = rle[\"counts\"].decode(\"ascii\")\n",
    "                \n",
    "                dets_segm.append({\n",
    "                    \"image_id\": int(img_id),\n",
    "                    \"category_id\": 1,\n",
    "                    \"segmentation\": rle,\n",
    "                    \"score\": score,\n",
    "                })\n",
    "    \n",
    "    # Compute latency statistics\n",
    "    latency_stats = {\n",
    "        \"mean_ms\": np.mean(latencies),\n",
    "        \"std_ms\": np.std(latencies),\n",
    "        \"median_ms\": np.median(latencies),\n",
    "        \"min_ms\": np.min(latencies),\n",
    "        \"max_ms\": np.max(latencies),\n",
    "        \"total_images\": len(latencies),\n",
    "        \"fps\": 1000.0 / np.mean(latencies) if np.mean(latencies) > 0 else 0,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nInference complete!\")\n",
    "    print(f\"  Total detections (bbox): {len(dets_bbox)}\")\n",
    "    print(f\"  Total detections (segm): {len(dets_segm)}\")\n",
    "    print(f\"  Average latency: {latency_stats['mean_ms']:.2f} ms ({latency_stats['fps']:.1f} FPS)\")\n",
    "    \n",
    "    return dets_bbox, dets_segm, latency_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52796f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on 22950 images...\n",
      "  Processed 100/22950 images...\n",
      "  Processed 100/22950 images...\n",
      "  Processed 200/22950 images...\n",
      "  Processed 200/22950 images...\n",
      "  Processed 300/22950 images...\n",
      "  Processed 300/22950 images...\n",
      "  Processed 400/22950 images...\n",
      "  Processed 400/22950 images...\n",
      "  Processed 500/22950 images...\n",
      "  Processed 500/22950 images...\n",
      "  Processed 600/22950 images...\n",
      "  Processed 600/22950 images...\n",
      "  Processed 700/22950 images...\n",
      "  Processed 700/22950 images...\n",
      "  Processed 800/22950 images...\n",
      "  Processed 800/22950 images...\n",
      "  Processed 900/22950 images...\n",
      "  Processed 900/22950 images...\n",
      "  Processed 1000/22950 images...\n",
      "  Processed 1000/22950 images...\n",
      "  Processed 1100/22950 images...\n",
      "  Processed 1100/22950 images...\n",
      "  Processed 1200/22950 images...\n",
      "  Processed 1200/22950 images...\n",
      "  Processed 1300/22950 images...\n",
      "  Processed 1300/22950 images...\n",
      "  Processed 1400/22950 images...\n",
      "  Processed 1400/22950 images...\n",
      "  Processed 1500/22950 images...\n",
      "  Processed 1500/22950 images...\n",
      "  Processed 1600/22950 images...\n",
      "  Processed 1600/22950 images...\n",
      "  Processed 1700/22950 images...\n",
      "  Processed 1700/22950 images...\n",
      "  Processed 1800/22950 images...\n",
      "  Processed 1800/22950 images...\n",
      "  Processed 1900/22950 images...\n",
      "  Processed 1900/22950 images...\n",
      "  Processed 2000/22950 images...\n",
      "  Processed 2000/22950 images...\n",
      "  Processed 2100/22950 images...\n",
      "  Processed 2100/22950 images...\n",
      "  Processed 2200/22950 images...\n",
      "  Processed 2200/22950 images...\n",
      "  Processed 2300/22950 images...\n",
      "  Processed 2300/22950 images...\n",
      "  Processed 2400/22950 images...\n",
      "  Processed 2400/22950 images...\n",
      "  Processed 2500/22950 images...\n",
      "  Processed 2500/22950 images...\n",
      "  Processed 2600/22950 images...\n",
      "  Processed 2600/22950 images...\n",
      "  Processed 2700/22950 images...\n",
      "  Processed 2700/22950 images...\n",
      "  Processed 2800/22950 images...\n",
      "  Processed 2800/22950 images...\n",
      "  Processed 2900/22950 images...\n",
      "  Processed 2900/22950 images...\n",
      "  Processed 3000/22950 images...\n",
      "  Processed 3000/22950 images...\n",
      "  Processed 3100/22950 images...\n",
      "  Processed 3100/22950 images...\n",
      "  Processed 3200/22950 images...\n",
      "  Processed 3200/22950 images...\n",
      "  Processed 3300/22950 images...\n",
      "  Processed 3300/22950 images...\n",
      "  Processed 3400/22950 images...\n",
      "  Processed 3400/22950 images...\n",
      "  Processed 3500/22950 images...\n",
      "  Processed 3500/22950 images...\n",
      "  Processed 3600/22950 images...\n",
      "  Processed 3600/22950 images...\n",
      "  Processed 3700/22950 images...\n",
      "  Processed 3700/22950 images...\n",
      "  Processed 3800/22950 images...\n",
      "  Processed 3800/22950 images...\n",
      "  Processed 3900/22950 images...\n",
      "  Processed 3900/22950 images...\n",
      "  Processed 4000/22950 images...\n",
      "  Processed 4000/22950 images...\n",
      "  Processed 4100/22950 images...\n",
      "  Processed 4100/22950 images...\n",
      "  Processed 4200/22950 images...\n",
      "  Processed 4200/22950 images...\n",
      "  Processed 4300/22950 images...\n",
      "  Processed 4300/22950 images...\n",
      "  Processed 4400/22950 images...\n",
      "  Processed 4400/22950 images...\n",
      "  Processed 4500/22950 images...\n",
      "  Processed 4500/22950 images...\n",
      "  Processed 4600/22950 images...\n",
      "  Processed 4600/22950 images...\n",
      "  Processed 4700/22950 images...\n",
      "  Processed 4700/22950 images...\n",
      "  Processed 4800/22950 images...\n",
      "  Processed 4800/22950 images...\n",
      "  Processed 4900/22950 images...\n",
      "  Processed 4900/22950 images...\n",
      "  Processed 5000/22950 images...\n",
      "  Processed 5000/22950 images...\n",
      "  Processed 5100/22950 images...\n",
      "  Processed 5100/22950 images...\n",
      "  Processed 5200/22950 images...\n",
      "  Processed 5200/22950 images...\n",
      "  Processed 5300/22950 images...\n",
      "  Processed 5300/22950 images...\n",
      "  Processed 5400/22950 images...\n",
      "  Processed 5400/22950 images...\n",
      "  Processed 5500/22950 images...\n",
      "  Processed 5500/22950 images...\n",
      "  Processed 5600/22950 images...\n",
      "  Processed 5600/22950 images...\n",
      "  Processed 5700/22950 images...\n",
      "  Processed 5700/22950 images...\n",
      "  Processed 5800/22950 images...\n",
      "  Processed 5800/22950 images...\n",
      "  Processed 5900/22950 images...\n",
      "  Processed 5900/22950 images...\n",
      "  Processed 6000/22950 images...\n",
      "  Processed 6000/22950 images...\n",
      "  Processed 6100/22950 images...\n",
      "  Processed 6100/22950 images...\n",
      "  Processed 6200/22950 images...\n",
      "  Processed 6200/22950 images...\n",
      "  Processed 6300/22950 images...\n",
      "  Processed 6300/22950 images...\n",
      "  Processed 6400/22950 images...\n",
      "  Processed 6400/22950 images...\n",
      "  Processed 6500/22950 images...\n",
      "  Processed 6500/22950 images...\n",
      "  Processed 6600/22950 images...\n",
      "  Processed 6600/22950 images...\n",
      "  Processed 6700/22950 images...\n",
      "  Processed 6700/22950 images...\n",
      "  Processed 6800/22950 images...\n",
      "  Processed 6800/22950 images...\n",
      "  Processed 6900/22950 images...\n",
      "  Processed 6900/22950 images...\n",
      "  Processed 7000/22950 images...\n",
      "  Processed 7000/22950 images...\n",
      "  Processed 7100/22950 images...\n",
      "  Processed 7100/22950 images...\n",
      "  Processed 7200/22950 images...\n",
      "  Processed 7200/22950 images...\n",
      "  Processed 7300/22950 images...\n",
      "  Processed 7300/22950 images...\n",
      "  Processed 7400/22950 images...\n",
      "  Processed 7400/22950 images...\n",
      "  Processed 7500/22950 images...\n",
      "  Processed 7500/22950 images...\n",
      "  Processed 7600/22950 images...\n",
      "  Processed 7600/22950 images...\n",
      "  Processed 7700/22950 images...\n",
      "  Processed 7700/22950 images...\n",
      "  Processed 7800/22950 images...\n",
      "  Processed 7800/22950 images...\n",
      "  Processed 7900/22950 images...\n",
      "  Processed 7900/22950 images...\n",
      "  Processed 8000/22950 images...\n",
      "  Processed 8000/22950 images...\n",
      "  Processed 8100/22950 images...\n",
      "  Processed 8100/22950 images...\n",
      "  Processed 8200/22950 images...\n",
      "  Processed 8200/22950 images...\n",
      "  Processed 8300/22950 images...\n",
      "  Processed 8300/22950 images...\n",
      "  Processed 8400/22950 images...\n",
      "  Processed 8400/22950 images...\n",
      "  Processed 8500/22950 images...\n",
      "  Processed 8500/22950 images...\n",
      "  Processed 8600/22950 images...\n",
      "  Processed 8600/22950 images...\n",
      "  Processed 8700/22950 images...\n",
      "  Processed 8700/22950 images...\n",
      "  Processed 8800/22950 images...\n",
      "  Processed 8800/22950 images...\n",
      "  Processed 8900/22950 images...\n",
      "  Processed 8900/22950 images...\n",
      "  Processed 9000/22950 images...\n",
      "  Processed 9000/22950 images...\n",
      "  Processed 9100/22950 images...\n",
      "  Processed 9100/22950 images...\n",
      "  Processed 9200/22950 images...\n",
      "  Processed 9200/22950 images...\n",
      "  Processed 9300/22950 images...\n",
      "  Processed 9300/22950 images...\n",
      "  Processed 9400/22950 images...\n",
      "  Processed 9400/22950 images...\n",
      "  Processed 9500/22950 images...\n",
      "  Processed 9500/22950 images...\n",
      "  Processed 9600/22950 images...\n",
      "  Processed 9600/22950 images...\n",
      "  Processed 9700/22950 images...\n",
      "  Processed 9700/22950 images...\n",
      "  Processed 9800/22950 images...\n",
      "  Processed 9800/22950 images...\n",
      "  Processed 9900/22950 images...\n",
      "  Processed 9900/22950 images...\n",
      "  Processed 10000/22950 images...\n",
      "  Processed 10000/22950 images...\n",
      "  Processed 10100/22950 images...\n",
      "  Processed 10100/22950 images...\n",
      "  Processed 10200/22950 images...\n",
      "  Processed 10200/22950 images...\n",
      "  Processed 10300/22950 images...\n",
      "  Processed 10300/22950 images...\n",
      "  Processed 10400/22950 images...\n",
      "  Processed 10400/22950 images...\n",
      "  Processed 10500/22950 images...\n",
      "  Processed 10500/22950 images...\n",
      "  Processed 10600/22950 images...\n",
      "  Processed 10600/22950 images...\n",
      "  Processed 10700/22950 images...\n",
      "  Processed 10700/22950 images...\n",
      "  Processed 10800/22950 images...\n",
      "  Processed 10800/22950 images...\n",
      "  Processed 10900/22950 images...\n",
      "  Processed 10900/22950 images...\n",
      "  Processed 11000/22950 images...\n",
      "  Processed 11000/22950 images...\n",
      "  Processed 11100/22950 images...\n",
      "  Processed 11100/22950 images...\n",
      "  Processed 11200/22950 images...\n",
      "  Processed 11200/22950 images...\n",
      "  Processed 11300/22950 images...\n",
      "  Processed 11300/22950 images...\n",
      "  Processed 11400/22950 images...\n",
      "  Processed 11400/22950 images...\n",
      "  Processed 11500/22950 images...\n",
      "  Processed 11500/22950 images...\n",
      "  Processed 11600/22950 images...\n",
      "  Processed 11600/22950 images...\n",
      "  Processed 11700/22950 images...\n",
      "  Processed 11700/22950 images...\n",
      "  Processed 11800/22950 images...\n",
      "  Processed 11800/22950 images...\n",
      "  Processed 11900/22950 images...\n",
      "  Processed 11900/22950 images...\n",
      "  Processed 12000/22950 images...\n",
      "  Processed 12000/22950 images...\n",
      "  Processed 12100/22950 images...\n",
      "  Processed 12100/22950 images...\n",
      "  Processed 12200/22950 images...\n",
      "  Processed 12200/22950 images...\n",
      "  Processed 12300/22950 images...\n",
      "  Processed 12300/22950 images...\n",
      "  Processed 12400/22950 images...\n",
      "  Processed 12400/22950 images...\n",
      "  Processed 12500/22950 images...\n",
      "  Processed 12500/22950 images...\n",
      "  Processed 12600/22950 images...\n",
      "  Processed 12600/22950 images...\n",
      "  Processed 12700/22950 images...\n",
      "  Processed 12700/22950 images...\n",
      "  Processed 12800/22950 images...\n",
      "  Processed 12800/22950 images...\n",
      "  Processed 12900/22950 images...\n",
      "  Processed 12900/22950 images...\n",
      "  Processed 13000/22950 images...\n",
      "  Processed 13000/22950 images...\n",
      "  Processed 13100/22950 images...\n",
      "  Processed 13100/22950 images...\n",
      "  Processed 13200/22950 images...\n",
      "  Processed 13200/22950 images...\n",
      "  Processed 13300/22950 images...\n",
      "  Processed 13300/22950 images...\n",
      "  Processed 13400/22950 images...\n",
      "  Processed 13400/22950 images...\n",
      "  Processed 13500/22950 images...\n",
      "  Processed 13500/22950 images...\n",
      "  Processed 13600/22950 images...\n",
      "  Processed 13600/22950 images...\n",
      "  Processed 13700/22950 images...\n",
      "  Processed 13700/22950 images...\n",
      "  Processed 13800/22950 images...\n",
      "  Processed 13800/22950 images...\n",
      "  Processed 13900/22950 images...\n",
      "  Processed 13900/22950 images...\n",
      "  Processed 14000/22950 images...\n",
      "  Processed 14000/22950 images...\n",
      "  Processed 14100/22950 images...\n",
      "  Processed 14100/22950 images...\n",
      "  Processed 14200/22950 images...\n",
      "  Processed 14200/22950 images...\n",
      "  Processed 14300/22950 images...\n",
      "  Processed 14300/22950 images...\n",
      "  Processed 14400/22950 images...\n",
      "  Processed 14400/22950 images...\n",
      "  Processed 14500/22950 images...\n",
      "  Processed 14500/22950 images...\n",
      "  Processed 14600/22950 images...\n",
      "  Processed 14600/22950 images...\n",
      "  Processed 14700/22950 images...\n",
      "  Processed 14700/22950 images...\n",
      "  Processed 14800/22950 images...\n",
      "  Processed 14800/22950 images...\n",
      "  Processed 14900/22950 images...\n",
      "  Processed 14900/22950 images...\n",
      "  Processed 15000/22950 images...\n",
      "  Processed 15000/22950 images...\n",
      "  Processed 15100/22950 images...\n",
      "  Processed 15100/22950 images...\n",
      "  Processed 15200/22950 images...\n",
      "  Processed 15200/22950 images...\n",
      "  Processed 15300/22950 images...\n",
      "  Processed 15300/22950 images...\n",
      "  Processed 15400/22950 images...\n",
      "  Processed 15400/22950 images...\n",
      "  Processed 15500/22950 images...\n",
      "  Processed 15500/22950 images...\n",
      "  Processed 15600/22950 images...\n",
      "  Processed 15600/22950 images...\n",
      "  Processed 15700/22950 images...\n",
      "  Processed 15700/22950 images...\n",
      "  Processed 15800/22950 images...\n",
      "  Processed 15800/22950 images...\n",
      "  Processed 15900/22950 images...\n",
      "  Processed 15900/22950 images...\n",
      "  Processed 16000/22950 images...\n",
      "  Processed 16000/22950 images...\n",
      "  Processed 16100/22950 images...\n",
      "  Processed 16100/22950 images...\n",
      "  Processed 16200/22950 images...\n",
      "  Processed 16200/22950 images...\n",
      "  Processed 16300/22950 images...\n",
      "  Processed 16300/22950 images...\n",
      "  Processed 16400/22950 images...\n",
      "  Processed 16400/22950 images...\n",
      "  Processed 16500/22950 images...\n",
      "  Processed 16500/22950 images...\n",
      "  Processed 16600/22950 images...\n",
      "  Processed 16600/22950 images...\n",
      "  Processed 16700/22950 images...\n",
      "  Processed 16700/22950 images...\n",
      "  Processed 16800/22950 images...\n",
      "  Processed 16800/22950 images...\n",
      "  Processed 16900/22950 images...\n",
      "  Processed 16900/22950 images...\n",
      "  Processed 17000/22950 images...\n",
      "  Processed 17000/22950 images...\n",
      "  Processed 17100/22950 images...\n",
      "  Processed 17100/22950 images...\n",
      "  Processed 17200/22950 images...\n",
      "  Processed 17200/22950 images...\n",
      "  Processed 17300/22950 images...\n",
      "  Processed 17300/22950 images...\n",
      "  Processed 17400/22950 images...\n",
      "  Processed 17400/22950 images...\n",
      "  Processed 17500/22950 images...\n",
      "  Processed 17500/22950 images...\n",
      "  Processed 17600/22950 images...\n",
      "  Processed 17600/22950 images...\n",
      "  Processed 17700/22950 images...\n",
      "  Processed 17700/22950 images...\n",
      "  Processed 17800/22950 images...\n",
      "  Processed 17800/22950 images...\n",
      "  Processed 17900/22950 images...\n",
      "  Processed 17900/22950 images...\n",
      "  Processed 18000/22950 images...\n",
      "  Processed 18000/22950 images...\n",
      "  Processed 18100/22950 images...\n",
      "  Processed 18100/22950 images...\n",
      "  Processed 18200/22950 images...\n",
      "  Processed 18200/22950 images...\n",
      "  Processed 18300/22950 images...\n",
      "  Processed 18300/22950 images...\n",
      "  Processed 18400/22950 images...\n",
      "  Processed 18400/22950 images...\n",
      "  Processed 18500/22950 images...\n",
      "  Processed 18500/22950 images...\n",
      "  Processed 18600/22950 images...\n",
      "  Processed 18600/22950 images...\n",
      "  Processed 18700/22950 images...\n",
      "  Processed 18700/22950 images...\n",
      "  Processed 18800/22950 images...\n",
      "  Processed 18800/22950 images...\n",
      "  Processed 18900/22950 images...\n",
      "  Processed 18900/22950 images...\n",
      "  Processed 19000/22950 images...\n",
      "  Processed 19000/22950 images...\n",
      "  Processed 19100/22950 images...\n",
      "  Processed 19100/22950 images...\n",
      "  Processed 19200/22950 images...\n",
      "  Processed 19200/22950 images...\n",
      "  Processed 19300/22950 images...\n",
      "  Processed 19300/22950 images...\n",
      "  Processed 19400/22950 images...\n",
      "  Processed 19400/22950 images...\n",
      "  Processed 19500/22950 images...\n",
      "  Processed 19500/22950 images...\n",
      "  Processed 19600/22950 images...\n",
      "  Processed 19600/22950 images...\n",
      "  Processed 19700/22950 images...\n",
      "  Processed 19700/22950 images...\n",
      "  Processed 19800/22950 images...\n",
      "  Processed 19800/22950 images...\n",
      "  Processed 19900/22950 images...\n",
      "  Processed 19900/22950 images...\n",
      "  Processed 20000/22950 images...\n",
      "  Processed 20000/22950 images...\n",
      "  Processed 20100/22950 images...\n",
      "  Processed 20100/22950 images...\n",
      "  Processed 20200/22950 images...\n",
      "  Processed 20200/22950 images...\n",
      "  Processed 20300/22950 images...\n",
      "  Processed 20300/22950 images...\n",
      "  Processed 20400/22950 images...\n",
      "  Processed 20400/22950 images...\n",
      "  Processed 20500/22950 images...\n",
      "  Processed 20500/22950 images...\n",
      "  Processed 20600/22950 images...\n",
      "  Processed 20600/22950 images...\n",
      "  Processed 20700/22950 images...\n",
      "  Processed 20700/22950 images...\n",
      "  Processed 20800/22950 images...\n",
      "  Processed 20800/22950 images...\n",
      "  Processed 20900/22950 images...\n",
      "  Processed 20900/22950 images...\n",
      "  Processed 21000/22950 images...\n",
      "  Processed 21000/22950 images...\n",
      "  Processed 21100/22950 images...\n",
      "  Processed 21100/22950 images...\n",
      "  Processed 21200/22950 images...\n",
      "  Processed 21200/22950 images...\n",
      "  Processed 21300/22950 images...\n",
      "  Processed 21300/22950 images...\n",
      "  Processed 21400/22950 images...\n",
      "  Processed 21400/22950 images...\n",
      "  Processed 21500/22950 images...\n",
      "  Processed 21500/22950 images...\n",
      "  Processed 21600/22950 images...\n",
      "  Processed 21600/22950 images...\n",
      "  Processed 21700/22950 images...\n",
      "  Processed 21700/22950 images...\n",
      "  Processed 21800/22950 images...\n",
      "  Processed 21800/22950 images...\n",
      "  Processed 21900/22950 images...\n",
      "  Processed 21900/22950 images...\n",
      "  Processed 22000/22950 images...\n",
      "  Processed 22000/22950 images...\n",
      "  Processed 22100/22950 images...\n",
      "  Processed 22100/22950 images...\n",
      "  Processed 22200/22950 images...\n",
      "  Processed 22200/22950 images...\n",
      "  Processed 22300/22950 images...\n",
      "  Processed 22300/22950 images...\n",
      "  Processed 22400/22950 images...\n",
      "  Processed 22400/22950 images...\n",
      "  Processed 22500/22950 images...\n",
      "  Processed 22500/22950 images...\n",
      "  Processed 22600/22950 images...\n",
      "  Processed 22600/22950 images...\n",
      "  Processed 22700/22950 images...\n",
      "  Processed 22700/22950 images...\n",
      "  Processed 22800/22950 images...\n",
      "  Processed 22800/22950 images...\n",
      "  Processed 22900/22950 images...\n",
      "  Processed 22900/22950 images...\n",
      "\n",
      "Inference complete!\n",
      "  Total detections (bbox): 2388968\n",
      "  Total detections (segm): 2388968\n",
      "  Average latency: 146.83 ms (6.8 FPS)\n",
      "\n",
      "Inference complete!\n",
      "  Total detections (bbox): 2388968\n",
      "  Total detections (segm): 2388968\n",
      "  Average latency: 146.83 ms (6.8 FPS)\n"
     ]
    }
   ],
   "source": [
    "# === Run inference ===\n",
    "\n",
    "dets_bbox, dets_segm, latency_stats = run_yolo_inference(\n",
    "    model=model,\n",
    "    coco=coco_uvo,\n",
    "    img_root=UVO_FRAMES_DIR,\n",
    "    img_ids=val_img_ids,\n",
    "    conf_thresh=CONF_THRESHOLD,\n",
    "    iou_thresh=IOU_THRESHOLD,\n",
    "    max_det=MAX_DET,\n",
    "    img_size=IMG_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c5ab098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BOUNDING BOX EVALUATION\n",
      "============================================================\n",
      "Loading and preparing results...\n",
      "DONE (t=5.75s)\n",
      "creating index...\n",
      "DONE (t=5.75s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=333.68s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=333.68s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=21.57s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.265\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.382\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.276\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.189\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.432\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.083\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.510\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.224\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.480\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.680\n",
      "\n",
      "============================================================\n",
      "SEGMENTATION EVALUATION\n",
      "============================================================\n",
      "Loading and preparing results...\n",
      "DONE (t=21.57s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.265\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.382\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.276\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.189\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.432\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.083\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.510\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.224\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.480\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.680\n",
      "\n",
      "============================================================\n",
      "SEGMENTATION EVALUATION\n",
      "============================================================\n",
      "Loading and preparing results...\n",
      "DONE (t=23.20s)\n",
      "creating index...\n",
      "DONE (t=23.20s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=378.96s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=378.96s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=21.68s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.171\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.327\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.163\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.008\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.091\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.322\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.068\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.211\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.077\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.270\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.478\n",
      "DONE (t=21.68s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.171\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.327\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.163\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.008\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.091\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.322\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.068\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.211\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.077\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.270\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.478\n"
     ]
    }
   ],
   "source": [
    "# === COCO Evaluation ===\n",
    "\n",
    "def evaluate_detections(coco_gt: COCO, dets_bbox: List[Dict], dets_segm: List[Dict]):\n",
    "    \"\"\"\n",
    "    Evaluate detections using COCO metrics.\n",
    "    Returns: dictionary with AR/AP metrics for both bbox and segm.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    if len(dets_bbox) == 0:\n",
    "        print(\"[WARN] No bounding box detections to evaluate!\")\n",
    "        return None\n",
    "    \n",
    "    # Bounding box evaluation\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BOUNDING BOX EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    coco_dt_box = coco_gt.loadRes(dets_bbox)\n",
    "    coco_eval_box = COCOeval(coco_gt, coco_dt_box, iouType=\"bbox\")\n",
    "    coco_eval_box.evaluate()\n",
    "    coco_eval_box.accumulate()\n",
    "    coco_eval_box.summarize()\n",
    "    stats_box = coco_eval_box.stats\n",
    "    \n",
    "    results[\"APb\"] = stats_box[0]      # AP @ IoU=0.50:0.95\n",
    "    results[\"APb_50\"] = stats_box[1]   # AP @ IoU=0.50\n",
    "    results[\"APb_75\"] = stats_box[2]   # AP @ IoU=0.75\n",
    "    results[\"ARb_1\"] = stats_box[6]    # AR @ 1 det\n",
    "    results[\"ARb_10\"] = stats_box[7]   # AR @ 10 dets\n",
    "    results[\"ARb_100\"] = stats_box[8]  # AR @ 100 dets\n",
    "    \n",
    "    # Segmentation evaluation\n",
    "    if len(dets_segm) > 0:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SEGMENTATION EVALUATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        coco_dt_segm = coco_gt.loadRes(dets_segm)\n",
    "        coco_eval_segm = COCOeval(coco_gt, coco_dt_segm, iouType=\"segm\")\n",
    "        coco_eval_segm.evaluate()\n",
    "        coco_eval_segm.accumulate()\n",
    "        coco_eval_segm.summarize()\n",
    "        stats_segm = coco_eval_segm.stats\n",
    "        \n",
    "        results[\"APm\"] = stats_segm[0]      # AP @ IoU=0.50:0.95\n",
    "        results[\"APm_50\"] = stats_segm[1]   # AP @ IoU=0.50\n",
    "        results[\"APm_75\"] = stats_segm[2]   # AP @ IoU=0.75\n",
    "        results[\"ARm_1\"] = stats_segm[6]    # AR @ 1 det\n",
    "        results[\"ARm_10\"] = stats_segm[7]   # AR @ 10 dets\n",
    "        results[\"ARm_100\"] = stats_segm[8]  # AR @ 100 dets\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluate_detections(coco_uvo, dets_bbox, dets_segm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16c8aa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL RESULTS - Table-1-style (Non-VOC UVO Evaluation)\n",
      "================================================================================\n",
      "\n",
      "Model: yolo11s-seg.pt\n",
      "Image size: 640\n",
      "Confidence threshold: 0.001\n",
      "\n",
      "Metric               Value\n",
      "-------------------------\n",
      "AR^b_10               30.6\n",
      "AR^b_100              51.0\n",
      "AR^m_10               21.1\n",
      "AR^m_100              31.6\n",
      "-------------------------\n",
      "AP^b                  26.5\n",
      "AP^m                  17.1\n",
      "\n",
      "========================================\n",
      "LATENCY STATISTICS\n",
      "========================================\n",
      "Mean latency:            146.83 ms\n",
      "Std latency:             117.14 ms\n",
      "Median latency:          115.81 ms\n",
      "Min latency:              23.05 ms\n",
      "Max latency:            2142.77 ms\n",
      "Throughput:                 6.8 FPS\n",
      "Total images:             22950\n",
      "\n",
      "================================================================================\n",
      "One-liner summary:\n",
      "YOLOv11 (yolo11s-seg.pt) | AR^b_10 = 30.6  AR^b_100 = 51.0  AR^m_10 = 21.1  AR^m_100 = 31.6  Latency = 146.8ms\n"
     ]
    }
   ],
   "source": [
    "# === Print Table-1-style results (matching v-CLR paper format) ===\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS - Table-1-style (Non-VOC UVO Evaluation)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if eval_results is not None:\n",
    "    # Convert to percentages\n",
    "    ARb10 = eval_results.get(\"ARb_10\", 0) * 100.0\n",
    "    ARb100 = eval_results.get(\"ARb_100\", 0) * 100.0\n",
    "    ARm10 = eval_results.get(\"ARm_10\", 0) * 100.0\n",
    "    ARm100 = eval_results.get(\"ARm_100\", 0) * 100.0\n",
    "    \n",
    "    APb = eval_results.get(\"APb\", 0) * 100.0\n",
    "    APm = eval_results.get(\"APm\", 0) * 100.0\n",
    "    \n",
    "    print(f\"\\nModel: {YOLO_MODEL}\")\n",
    "    print(f\"Image size: {IMG_SIZE}\")\n",
    "    print(f\"Confidence threshold: {CONF_THRESHOLD}\")\n",
    "    print(f\"\\n{'Metric':<15} {'Value':>10}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"{'AR^b_10':<15} {ARb10:>10.1f}\")\n",
    "    print(f\"{'AR^b_100':<15} {ARb100:>10.1f}\")\n",
    "    print(f\"{'AR^m_10':<15} {ARm10:>10.1f}\")\n",
    "    print(f\"{'AR^m_100':<15} {ARm100:>10.1f}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"{'AP^b':<15} {APb:>10.1f}\")\n",
    "    print(f\"{'AP^m':<15} {APm:>10.1f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(\"LATENCY STATISTICS\")\n",
    "    print(f\"{'='*40}\")\n",
    "    print(f\"{'Mean latency:':<20} {latency_stats['mean_ms']:>10.2f} ms\")\n",
    "    print(f\"{'Std latency:':<20} {latency_stats['std_ms']:>10.2f} ms\")\n",
    "    print(f\"{'Median latency:':<20} {latency_stats['median_ms']:>10.2f} ms\")\n",
    "    print(f\"{'Min latency:':<20} {latency_stats['min_ms']:>10.2f} ms\")\n",
    "    print(f\"{'Max latency:':<20} {latency_stats['max_ms']:>10.2f} ms\")\n",
    "    print(f\"{'Throughput:':<20} {latency_stats['fps']:>10.1f} FPS\")\n",
    "    print(f\"{'Total images:':<20} {latency_stats['total_images']:>10d}\")\n",
    "    \n",
    "    # One-liner summary (matching v-CLR paper format)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"One-liner summary:\")\n",
    "    print(\n",
    "        f\"YOLOv11 ({YOLO_MODEL}) | \"\n",
    "        f\"AR^b_10 = {ARb10:.1f}  \"\n",
    "        f\"AR^b_100 = {ARb100:.1f}  \"\n",
    "        f\"AR^m_10 = {ARm10:.1f}  \"\n",
    "        f\"AR^m_100 = {ARm100:.1f}  \"\n",
    "        f\"Latency = {latency_stats['mean_ms']:.1f}ms\"\n",
    "    )\n",
    "else:\n",
    "    print(\"No evaluation results available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5845c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d7bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Optional: Visualize some detections ===\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_detections(model: YOLO, coco: COCO, img_root: Path, img_ids: List[int], \n",
    "                         num_images: int = 5, conf_thresh: float = 0.25):\n",
    "    \"\"\"\n",
    "    Visualize YOLO detections on a few sample images.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(4 * num_images, 4))\n",
    "    if num_images == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    sample_ids = img_ids[:num_images]\n",
    "    \n",
    "    for ax, img_id in zip(axes, sample_ids):\n",
    "        try:\n",
    "            img_path = get_image_path(coco, img_id, img_root)\n",
    "            \n",
    "            # Run inference\n",
    "            results = model.predict(\n",
    "                source=str(img_path),\n",
    "                conf=conf_thresh,\n",
    "                verbose=False,\n",
    "                device=device,\n",
    "            )\n",
    "            \n",
    "            # Plot using ultralytics built-in visualization\n",
    "            result = results[0]\n",
    "            img_with_boxes = result.plot()\n",
    "            \n",
    "            # Convert BGR to RGB for matplotlib\n",
    "            img_rgb = img_with_boxes[:, :, ::-1]\n",
    "            \n",
    "            ax.imshow(img_rgb)\n",
    "            ax.set_title(f\"Image ID: {img_id}\")\n",
    "            ax.axis(\"off\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, f\"Error: {e}\", ha=\"center\", va=\"center\")\n",
    "            ax.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize a few samples\n",
    "print(\"Visualizing sample detections...\")\n",
    "visualize_detections(model, coco_uvo, UVO_FRAMES_DIR, val_img_ids, num_images=5, conf_thresh=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f9b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save results to file ===\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "results_summary = {\n",
    "    \"model\": YOLO_MODEL,\n",
    "    \"dataset\": \"UVO Non-VOC Validation\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"config\": {\n",
    "        \"img_size\": IMG_SIZE,\n",
    "        \"conf_threshold\": CONF_THRESHOLD,\n",
    "        \"iou_threshold\": IOU_THRESHOLD,\n",
    "        \"max_det\": MAX_DET,\n",
    "        \"device\": device,\n",
    "    },\n",
    "    \"metrics\": eval_results,\n",
    "    \"latency\": latency_stats,\n",
    "    \"num_images\": len(val_img_ids),\n",
    "    \"num_detections_bbox\": len(dets_bbox),\n",
    "    \"num_detections_segm\": len(dets_segm),\n",
    "}\n",
    "\n",
    "output_file = f\"yolov11_baseline_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c611cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Benchmark different YOLO model variants (optional) ===\n",
    "\n",
    "# Uncomment to run benchmarks across different model sizes\n",
    "\n",
    "# YOLO_VARIANTS = [\n",
    "#     \"yolo11n-seg.pt\",  # Nano\n",
    "#     \"yolo11s-seg.pt\",  # Small\n",
    "#     \"yolo11m-seg.pt\",  # Medium\n",
    "#     \"yolo11l-seg.pt\",  # Large\n",
    "#     \"yolo11x-seg.pt\",  # Extra-large\n",
    "# ]\n",
    "# \n",
    "# all_results = []\n",
    "# \n",
    "# for variant in YOLO_VARIANTS:\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"Evaluating: {variant}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "#     \n",
    "#     model_variant = YOLO(variant)\n",
    "#     if device == \"cuda\":\n",
    "#         model_variant.to(device)\n",
    "#     \n",
    "#     dets_b, dets_s, lat = run_yolo_inference(\n",
    "#         model=model_variant,\n",
    "#         coco=coco_nonvoc,\n",
    "#         img_root=NONVOC_IMG_DIR,\n",
    "#         img_ids=val_img_ids,\n",
    "#         conf_thresh=CONF_THRESHOLD,\n",
    "#         iou_thresh=IOU_THRESHOLD,\n",
    "#         max_det=MAX_DET,\n",
    "#         img_size=IMG_SIZE,\n",
    "#     )\n",
    "#     \n",
    "#     results_v = evaluate_detections(coco_nonvoc, dets_b, dets_s)\n",
    "#     \n",
    "#     all_results.append({\n",
    "#         \"model\": variant,\n",
    "#         \"metrics\": results_v,\n",
    "#         \"latency\": lat,\n",
    "#     })\n",
    "# \n",
    "# # Print comparison table\n",
    "# print(\"\\n\" + \"=\"*100)\n",
    "# print(\"MODEL COMPARISON\")\n",
    "# print(\"=\"*100)\n",
    "# print(f\"{'Model':<20} {'AR^b_10':>10} {'AR^b_100':>10} {'AR^m_10':>10} {'AR^m_100':>10} {'Latency (ms)':>12} {'FPS':>8}\")\n",
    "# print(\"-\"*100)\n",
    "# for r in all_results:\n",
    "#     if r[\"metrics\"] is not None:\n",
    "#         print(\n",
    "#             f\"{r['model']:<20} \"\n",
    "#             f\"{r['metrics']['ARb_10']*100:>10.1f} \"\n",
    "#             f\"{r['metrics']['ARb_100']*100:>10.1f} \"\n",
    "#             f\"{r['metrics']['ARm_10']*100:>10.1f} \"\n",
    "#             f\"{r['metrics']['ARm_100']*100:>10.1f} \"\n",
    "#             f\"{r['latency']['mean_ms']:>12.1f} \"\n",
    "#             f\"{r['latency']['fps']:>8.1f}\"\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
